#!/usr/bin/env python3
"""
ê·¹í•œ GPU ìµœì í™” ëª¨ë“ˆ
RTF < 0.05x ë‹¬ì„±ì„ ìœ„í•œ ê³ ê¸‰ ìµœì í™” ê¸°ë²•ë“¤
"""

import torch
import torch.nn as nn
import torch.cuda.profiler as profiler
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
import time
import logging
import gc
from concurrent.futures import ThreadPoolExecutor
import asyncio
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class BatchRequest:
    """ë°°ì¹˜ ì²˜ë¦¬ìš© ìš”ì²­ ë°ì´í„°"""
    request_id: str
    audio_data: np.ndarray
    language: str
    priority: str = "medium"

class ExtremeGPUOptimizer:
    """ê·¹í•œ GPU ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, device: str = "cuda"):
        self.device = device
        self.batch_size = 8  # ë™ì  ì¡°ì • ê°€ëŠ¥
        self.max_batch_wait_ms = 10  # ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„
        self.optimization_level = "extreme"
        
        # ê³ ê¸‰ GPU ì„¤ì •
        self._apply_extreme_gpu_settings()
        
    def _apply_extreme_gpu_settings(self):
        """ê·¹í•œ GPU ì„¤ì • ì ìš©"""
        if not torch.cuda.is_available():
            return
            
        logger.info("ğŸ”¥ ê·¹í•œ GPU ìµœì í™” ì„¤ì • ì ìš© ì¤‘...")
        
        # 1. ë©”ëª¨ë¦¬ ìµœì í™”
        torch.cuda.empty_cache()
        gc.collect()
        
        # 2. ì»´í“¨íŒ… ëª¨ë“œ ìµœì í™”
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
        
        # 3. ë©”ëª¨ë¦¬ í’€ ìµœì í™”
        torch.cuda.set_per_process_memory_fraction(0.98)  # ìµœëŒ€ ì‚¬ìš©
        
        # 4. ìŠ¤íŠ¸ë¦¼ ìµœì í™”
        torch.cuda.current_stream().synchronize()
        
        # 5. JIT ì»´íŒŒì¼ í™œì„±í™”
        torch.jit.set_fusion_strategy([("STATIC", 2), ("DYNAMIC", 2)])
        
        logger.info("âœ… ê·¹í•œ GPU ì„¤ì • ì™„ë£Œ")

class BatchProcessor:
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, stt_model, batch_size: int = 8, max_wait_ms: int = 10):
        self.stt_model = stt_model
        self.batch_size = batch_size
        self.max_wait_ms = max_wait_ms
        self.pending_requests: List[BatchRequest] = []
        self.processing = False
        
    async def add_request(self, request: BatchRequest) -> Dict[str, Any]:
        """ìš”ì²­ì„ ë°°ì¹˜ì— ì¶”ê°€í•˜ê³  ê²°ê³¼ ë°˜í™˜"""
        self.pending_requests.append(request)
        
        # ë°°ì¹˜ê°€ ê°€ë“ ì°¨ê±°ë‚˜ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ì‹œ ì²˜ë¦¬
        if len(self.pending_requests) >= self.batch_size or self._should_process_batch():
            return await self._process_batch()
        
        # ì§§ì€ ëŒ€ê¸° í›„ ë‹¤ì‹œ í™•ì¸
        await asyncio.sleep(self.max_wait_ms / 1000.0)
        if self.pending_requests and not self.processing:
            return await self._process_batch()
    
    def _should_process_batch(self) -> bool:
        """ë°°ì¹˜ ì²˜ë¦¬ ì—¬ë¶€ ê²°ì •"""
        if not self.pending_requests:
            return False
        
        # ë†’ì€ ìš°ì„ ìˆœìœ„ ìš”ì²­ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ì²˜ë¦¬
        high_priority = any(req.priority == "high" for req in self.pending_requests)
        return high_priority or len(self.pending_requests) >= self.batch_size
    
    async def _process_batch(self) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        if self.processing or not self.pending_requests:
            return []
        
        self.processing = True
        batch = self.pending_requests[:self.batch_size]
        self.pending_requests = self.pending_requests[self.batch_size:]
        
        try:
            # ë°°ì¹˜ ì²˜ë¦¬
            results = await self._process_audio_batch(batch)
            return results
        finally:
            self.processing = False
    
    async def _process_audio_batch(self, batch: List[BatchRequest]) -> List[Dict[str, Any]]:
        """ì‹¤ì œ ë°°ì¹˜ ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
        start_time = time.time()
        
        # GPUì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
        audio_arrays = [req.audio_data for req in batch]
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í…ì„œ ì¤€ë¹„
        max_length = max(len(audio) for audio in audio_arrays)
        
        # íŒ¨ë”© ë° ìŠ¤íƒ
        padded_audios = []
        for audio in audio_arrays:
            if len(audio) < max_length:
                padded = np.pad(audio, (0, max_length - len(audio)))
            else:
                padded = audio[:max_length]
            padded_audios.append(padded)
        
        # GPU í…ì„œë¡œ ë³€í™˜
        batch_tensor = torch.from_numpy(np.stack(padded_audios)).to(self.stt_model.device)
        
        # ë³‘ë ¬ ì¶”ë¡ 
        with torch.no_grad():
            batch_results = await self._parallel_inference(batch_tensor, batch)
        
        processing_time = time.time() - start_time
        
        results = []
        for i, (req, result) in enumerate(zip(batch, batch_results)):
            audio_duration = len(req.audio_data) / 16000.0
            rtf = processing_time / audio_duration if audio_duration > 0 else 0
            
            results.append({
                "request_id": req.request_id,
                "text": result.get("text", ""),
                "language": result.get("language", req.language),
                "rtf": rtf,
                "processing_time": processing_time / len(batch),
                "audio_duration": audio_duration,
                "batch_processed": True,
                "batch_size": len(batch)
            })
        
        return results
    
    async def _parallel_inference(self, batch_tensor: torch.Tensor, batch: List[BatchRequest]) -> List[Dict[str, Any]]:
        """ë³‘ë ¬ ì¶”ë¡  ì‹¤í–‰"""
        # ì—¬ê¸°ì„œ ì‹¤ì œ ëª¨ë¸ ì¶”ë¡ ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        # í˜„ì¬ëŠ” ê°œë³„ ì²˜ë¦¬ë¥¼ ì‹œë®¬ë ˆì´ì…˜
        results = []
        
        for i, req in enumerate(batch):
            audio_slice = batch_tensor[i].cpu().numpy()
            
            # ê°œë³„ ì¶”ë¡  (ì‹¤ì œë¡œëŠ” ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ ëŒ€ì²´í•´ì•¼ í•¨)
            result = await self._single_inference(audio_slice, req)
            results.append(result)
        
        return results
    
    async def _single_inference(self, audio: np.ndarray, req: BatchRequest) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¶”ë¡  (ë°°ì¹˜ ì¶”ë¡  êµ¬í˜„ê¹Œì§€ì˜ ì„ì‹œ ì²˜ë¦¬)"""
        # ì‹¤ì œ STT ëª¨ë¸ í˜¸ì¶œ
        try:
            # ì˜¤ë””ì˜¤ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
            audio_bytes = (audio * 32767).astype(np.int16).tobytes()
            result = await self.stt_model.transcribe_file_bytes(audio_bytes, language=req.language)
            
            return {
                "text": result.text,
                "language": result.language,
                "confidence": getattr(result, 'confidence', 0.0)
            }
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì¶”ë¡  ì˜¤ë¥˜: {e}")
            return {"text": "", "language": req.language, "confidence": 0.0}

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ìµœì í™” í´ë˜ìŠ¤"""
    
    @staticmethod
    def optimize_memory():
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
        if not torch.cuda.is_available():
            return
        
        # 1. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        torch.cuda.empty_cache()
        
        # 2. ë©”ëª¨ë¦¬ í’€ ìµœì í™”
        try:
            torch.cuda.memory.set_per_process_memory_fraction(0.98)
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ í’€ ì„¤ì • ì‹¤íŒ¨: {e}")
        
        # 3. ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ìµœì í™” (PyTorch ë²„ì „ í™•ì¸)
        try:
            if hasattr(torch.cuda.memory, 'set_allocator_settings'):
                # PyTorch 2.5+ í˜¸í™˜ì„±ì„ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬
                # torch.cuda.memory.set_allocator_settings("backend:cudaMallocAsync")
                logger.info("âš ï¸ set_allocator_settingsëŠ” PyTorch 2.5+ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            else:
                # ëŒ€ì•ˆ: ê¸°ë³¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
                torch.cuda.empty_cache()
                logger.info("âœ… CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ í• ë‹¹ì ì„¤ì • ì‹¤íŒ¨: {e}")
        
        logger.info("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")

class ModelOptimizer:
    """ëª¨ë¸ ìµœì í™” í´ë˜ìŠ¤"""
    
    @staticmethod
    def optimize_model_for_speed(model):
        """ì†ë„ë¥¼ ìœ„í•œ ëª¨ë¸ ìµœì í™”"""
        if not hasattr(model, 'model'):
            return model
        
        try:
            # 1. JIT ì»´íŒŒì¼
            if hasattr(model.model, 'encoder'):
                model.model.encoder = torch.jit.script(model.model.encoder)
            
            # 2. í˜¼í•© ì •ë°€ë„ í™œì„±í™”
            if hasattr(model.model, 'half'):
                model.model = model.model.half()
            
            # 3. í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
            model.model.eval()
            
            logger.info("âœ… ëª¨ë¸ ì†ë„ ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"ëª¨ë¸ ìµœì í™” ë¶€ë¶„ ì‹¤íŒ¨: {e}")
        
        return model

class StreamProcessor:
    """ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ìµœì í™”"""
    
    def __init__(self, num_streams: int = 4):
        self.num_streams = num_streams
        self.streams = [torch.cuda.Stream() for _ in range(num_streams)]
        self.current_stream = 0
        
    def get_next_stream(self) -> torch.cuda.Stream:
        """ë‹¤ìŒ ìŠ¤íŠ¸ë¦¼ ë°˜í™˜"""
        stream = self.streams[self.current_stream]
        self.current_stream = (self.current_stream + 1) % self.num_streams
        return stream
    
    async def process_with_stream(self, func, *args, **kwargs):
        """ìŠ¤íŠ¸ë¦¼ì„ ì‚¬ìš©í•œ ë¹„ë™ê¸° ì²˜ë¦¬"""
        stream = self.get_next_stream()
        
        with torch.cuda.stream(stream):
            result = await func(*args, **kwargs)
            
        # ìŠ¤íŠ¸ë¦¼ ë™ê¸°í™”
        stream.synchronize()
        return result

class ExtremeOptimizedSTTService:
    """ê·¹í•œ ìµœì í™”ëœ STT ì„œë¹„ìŠ¤"""
    
    def __init__(self, base_stt_service):
        self.base_service = base_stt_service
        self.gpu_optimizer = ExtremeGPUOptimizer()
        self.batch_processor = BatchProcessor(base_stt_service, batch_size=8)
        self.memory_optimizer = MemoryOptimizer()
        self.stream_processor = StreamProcessor(num_streams=4)
        
        # ëª¨ë¸ ìµœì í™”
        self.base_service = ModelOptimizer.optimize_model_for_speed(self.base_service)
        
        logger.info("ğŸš€ ê·¹í•œ ìµœì í™” STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def transcribe_file_bytes(self, audio_bytes: bytes, language: str = "ko") -> Dict[str, Any]:
        """ê·¹í•œ ìµœì í™”ëœ ìŒì„± ì¸ì‹"""
        start_time = time.time()
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        self.memory_optimizer.optimize_memory()
        
        # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
        audio_array = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
        
        # ë°°ì¹˜ ìš”ì²­ ìƒì„±
        request = BatchRequest(
            request_id=f"extreme_{int(time.time() * 1000000)}",
            audio_data=audio_array,
            language=language,
            priority="high"  # ìµœìš°ì„  ì²˜ë¦¬
        )
        
        # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
        result = await self.stream_processor.process_with_stream(
            self._process_single_request, request
        )
        
        processing_time = time.time() - start_time
        audio_duration = len(audio_array) / 16000.0
        rtf = processing_time / audio_duration if audio_duration > 0 else 0
        
        # ê²°ê³¼ í¬ë§·íŒ…
        return type('STTResult', (), {
            'text': result.get('text', ''),
            'language': result.get('language', language),
            'rtf': rtf,
            'processing_time': processing_time,
            'audio_duration': audio_duration,
            'optimization_applied': 'extreme'
        })()
    
    async def _process_single_request(self, request: BatchRequest) -> Dict[str, Any]:
        """ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬ (ê·¹í•œ ìµœì í™”)"""
        # ê¸°ì¡´ ì„œë¹„ìŠ¤ í˜¸ì¶œ (ì¶”í›„ ë°°ì¹˜ë¡œ ëŒ€ì²´)
        audio_bytes = (request.audio_data * 32767).astype(np.int16).tobytes()
        
        # GPU ìµœì í™”ëœ ì²˜ë¦¬
        with torch.cuda.amp.autocast():
            result = await self.base_service.transcribe_file_bytes(audio_bytes, request.language)
        
        return {
            'text': result.text,
            'language': result.language,
            'confidence': getattr(result, 'confidence', 0.0)
        }

def create_extreme_optimized_service(base_stt_service):
    """ê·¹í•œ ìµœì í™” STT ì„œë¹„ìŠ¤ ìƒì„±"""
    return ExtremeOptimizedSTTService(base_stt_service) 
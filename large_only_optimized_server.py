#!/usr/bin/env python3
"""
Large Only Optimized STT Server
Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ ìµœì í™” STT ì„œë²„
"""

import asyncio
import logging
import time
import torch
import numpy as np
import base64
from typing import Dict, Any, Optional, List
from datetime import datetime
import os
import argparse

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from src.api.stt_service import FasterWhisperSTTService
from src.utils.audio_utils import AudioUtils

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== Data Models ====================

class STTRequest(BaseModel):
    audio_data: str  # base64 encoded
    language: str = "ko"
    audio_format: str = "pcm_16khz"  # ì§€ì› í¬ë§·: pcm_16khz ë§Œ ì§€ì›
    vad_enabled: bool = True  # VAD(Voice Activity Detection) ì‚¬ìš© ì—¬ë¶€

class STTWithKeywordsRequest(BaseModel):
    audio_data: str  # base64 encoded
    language: str = "ko"
    audio_format: str = "pcm_16khz"
    keywords: List[str] = []  # ë¶€ìŠ¤íŒ…í•  í‚¤ì›Œë“œ ëª©ë¡
    keyword_boost: float = 2.0  # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ê°•ë„ (1.0-5.0)
    vad_enabled: bool = True  # VAD(Voice Activity Detection) ì‚¬ìš© ì—¬ë¶€

class STTResponse(BaseModel):
    text: str
    language: str
    rtf: float
    processing_time: float
    confidence: float = 0.0
    audio_duration: float = 0.0
    audio_format: str = "pcm_16khz"
    vad_enabled: bool = True  # VAD ì‚¬ìš© ì—¬ë¶€

class KeywordBoostResponse(BaseModel):
    text: str
    language: str
    rtf: float
    processing_time: float
    confidence: float = 0.0
    keywords_detected: List[str] = []  # ê°ì§€ëœ í‚¤ì›Œë“œ ëª©ë¡
    boost_applied: bool = False  # ë¶€ìŠ¤íŒ… ì ìš© ì—¬ë¶€
    audio_duration: float = 0.0
    audio_format: str = "pcm_16khz"
    vad_enabled: bool = True  # VAD ì‚¬ìš© ì—¬ë¶€

class HealthResponse(BaseModel):
    status: str
    gpu_info: Dict[str, Any]
    model_info: Dict[str, Any]
    optimization_status: Dict[str, Any]

# ==================== Main STT Service ====================

class LargeOnlyOptimizedSTTService:
    """Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ ìµœì í™” STT ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.model = None
        self.gpu_info = None
        self._setup_gpu_optimizations()
        
    def _setup_gpu_optimizations(self):
        """ê·¹í•œ GPU ìµœì í™” ì„¤ì •"""
        if not torch.cuda.is_available():
            raise RuntimeError("CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            
        logger.info("ğŸš€ Large ëª¨ë¸ ì „ìš© ê·¹í•œ GPU ìµœì í™” ì ìš© ì¤‘...")
        
        # GPU ë©”ëª¨ë¦¬ ì„¤ì • (PyTorch 2.5+ í˜¸í™˜)
        try:
            if hasattr(torch.cuda, 'set_memory_fraction'):
                torch.cuda.set_memory_fraction(0.95)  # GPU ë©”ëª¨ë¦¬ 95% ì‚¬ìš©
                logger.info("âœ… CUDA ë©”ëª¨ë¦¬ fraction ì„¤ì • ì™„ë£Œ")
            else:
                # PyTorch 2.5+ í˜¸í™˜ ë©”ëª¨ë¦¬ ì„¤ì •
                torch.cuda.memory.set_per_process_memory_fraction(0.95)
                logger.info("âœ… CUDA í”„ë¡œì„¸ìŠ¤ë³„ ë©”ëª¨ë¦¬ fraction ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ CUDA ë©”ëª¨ë¦¬ fraction ì„¤ì • ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬ë§Œ ìˆ˜í–‰
            torch.cuda.empty_cache()
            
        torch.backends.cudnn.benchmark = True  # cuDNN ë²¤ì¹˜ë§ˆí¬ í™œì„±í™”
        torch.backends.cuda.matmul.allow_tf32 = True  # TF32 í™œì„±í™”
        torch.backends.cudnn.allow_tf32 = True
        
        # CUDA ë©”ëª¨ë¦¬ í’€ ìµœì í™” (PyTorch 2.5+ í˜¸í™˜)
        try:
            if hasattr(torch.cuda, 'memory') and hasattr(torch.cuda.memory, 'set_memory_pool_limit'):
                torch.cuda.memory.set_memory_pool_limit(0.95)
                logger.info("âœ… CUDA ë©”ëª¨ë¦¬ í’€ ì œí•œ ì„¤ì • ì™„ë£Œ")
            else:
                # PyTorch 2.5+ í˜¸í™˜ ë©”ëª¨ë¦¬ ìµœì í™”
                torch.cuda.empty_cache()
                logger.info("âœ… CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ CUDA ë©”ëª¨ë¦¬ ì„¤ì • ì‹¤íŒ¨: {e}")
            torch.cuda.empty_cache()  # ìµœì†Œí•œ ë©”ëª¨ë¦¬ ì •ë¦¬ëŠ” ìˆ˜í–‰
                
        # Mixed precision í™œì„±í™”
        torch.backends.cuda.enable_flash_sdp(True)
        
        # GPU ì •ë³´ ìˆ˜ì§‘
        self.gpu_info = {
            "device": torch.cuda.get_device_name(),
            "memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB",
            "cuda_version": torch.version.cuda,
            "pytorch_version": torch.__version__
        }
        
        logger.info("âœ… ê·¹í•œ GPU ìµœì í™” ì™„ë£Œ")
        
    async def _load_large_model(self):
        """Large-v3 ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info("ğŸ“¦ Large-v3 ëª¨ë¸ ë¡œë”© ì¤‘ (float16 ìµœì í™”)...")
            
            self.model = FasterWhisperSTTService(
                model_size="large-v3",
                device="cuda",
                compute_type="float16"
            )
            
            # STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            logger.info("ğŸ”§ STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
            initialized = await self.model.initialize()
            if not initialized:
                raise RuntimeError("STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            logger.info("âœ… Large-v3 ëª¨ë¸ ë¡œë“œ ë° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ Large-v3 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
            
    async def _warmup_model(self):
        """ëª¨ë¸ ì›œì—…"""
        try:
            logger.info("ğŸ”¥ ëª¨ë¸ ì›œì—… ì‹œì‘...")
            
            # ë”ë¯¸ ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„± (1ì´ˆ, 16kHz)
            dummy_audio = np.random.randn(16000).astype(np.float32) * 0.1  # ì‘ì€ ë³¼ë¥¨
            
            # ëª¨ë¸ì˜ ë‚´ë¶€ transcribe ë©”ì„œë“œë¥¼ ì§ì ‘ ì‚¬ìš© (ê°€ì¥ ì•ˆì „í•¨)
            start_time = time.time()
            # STT ì„œë¹„ìŠ¤ì˜ ë‚´ë¶€ ëª¨ë¸ì— ì§ì ‘ ì ‘ê·¼
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'transcribe'):
                # FasterWhisper ëª¨ë¸ì˜ transcribe ë©”ì„œë“œ ì§ì ‘ í˜¸ì¶œ
                segments, info = self.model.model.transcribe(
                    dummy_audio,
                    beam_size=1,
                    best_of=1,
                    temperature=0.0,
                    vad_filter=False,
                    language="ko"
                )
                # ê²°ê³¼ ì†Œë¹„
                list(segments)
            else:
                logger.info("ì›œì—…ì„ ìœ„í•œ ì§ì ‘ ëª¨ë¸ ì ‘ê·¼ ë¶ˆê°€, ì›œì—… ê±´ë„ˆëœ€")
                
            warmup_time = time.time() - start_time
            
            logger.info(f"âœ… ëª¨ë¸ ì›œì—… ì™„ë£Œ ({warmup_time:.3f}ì´ˆ)")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ëª¨ë¸ ì›œì—… ì‹¤íŒ¨ (ë¹„ì¤‘ìš”): {e}")
            # ì›œì—… ì‹¤íŒ¨ëŠ” ì„œë²„ ì‹œì‘ì„ ë§‰ì§€ ì•ŠìŒ
            pass

    async def transcribe(self, audio_data: bytes, language: str = "ko", audio_format: str = "pcm_16khz", vad_enabled: bool = True) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ ì „ì‚¬ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            # PCM 16kHz ì „ìš© - ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚°
            if audio_format == "pcm_16khz":
                # 16kHz, 16bit (2 bytes per sample)
                audio_duration = len(audio_data) / (16000 * 2)
            else:
                # PCM 16kHzë§Œ ì§€ì›
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í¬ë§·: {audio_format}. PCM 16kHzë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
            
            # PCM 16kHz ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            audio_array = np.frombuffer(audio_data, dtype=np.int16)
            audio_array = audio_array.astype(np.float32) / 32768.0
            
            # FasterWhisper ëª¨ë¸ì— ì§ì ‘ ì „ì‚¬ ìš”ì²­
            if not self.model or not hasattr(self.model, 'model'):
                raise ValueError("STT ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
            segments, info = self.model.model.transcribe(
                audio_array,
                beam_size=5,
                best_of=5,
                temperature=0.0,
                language=language,
                vad_filter=vad_enabled
            )
            
            # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            result_text = " ".join([segment.text for segment in segments])
            
            # STTResult í˜•íƒœë¡œ ë³€í™˜
            class SimpleResult:
                def __init__(self, text):
                    self.text = text
                    
            result = SimpleResult(result_text)
            
            processing_time = time.time() - start_time
            rtf = processing_time / max(audio_duration, 0.001)
            
            # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            result_text = result.text if hasattr(result, 'text') else str(result)
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ í‘œì‹œ
            display_text = result_text[:50] + "..." if len(result_text) > 50 else result_text
            
            logger.info(
                f"âœ… ì „ì‚¬ ì™„ë£Œ - VAD: {vad_enabled}, ì˜¤ë””ì˜¤: {audio_duration:.3f}ì´ˆ, "
                f"ì²˜ë¦¬ì‹œê°„: {processing_time:.3f}ì´ˆ, RTF: {rtf:.4f}, í…ìŠ¤íŠ¸: \"{display_text}\""
            )
            
            return {
                "text": result.text if hasattr(result, 'text') else str(result),
                "language": language,
                "rtf": rtf,
                "processing_time": processing_time,
                "confidence": result.model_info.get("confidence", 0.0) if hasattr(result, 'model_info') else 0.0,
                "audio_duration": audio_duration,
                "audio_format": audio_format,
                "vad_enabled": vad_enabled
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ ì „ì‚¬ ì‹¤íŒ¨: {e}, ì²˜ë¦¬ì‹œê°„: {processing_time:.3f}ì´ˆ")
            raise HTTPException(status_code=500, detail=f"ì „ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    def get_status(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜"""
        gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
        gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3
        
        return {
            "status": "healthy" if self.model else "loading",
            "gpu_info": {
                **self.gpu_info,
                "memory_allocated": f"{gpu_memory_allocated:.2f}GB",
                "memory_reserved": f"{gpu_memory_reserved:.2f}GB"
            },
            "model_info": {
                "model": "large-v3",
                "compute_type": "float16",
                "device": "cuda",
                "loaded": self.model is not None
            },
            "optimization_status": {
                "cudnn_benchmark": torch.backends.cudnn.benchmark,
                "tf32_enabled": torch.backends.cuda.matmul.allow_tf32,
                "memory_fraction": 0.95
            }
        }

# ==================== FastAPI App ====================

def create_app() -> FastAPI:
    """FastAPI ì•± ìƒì„±"""
    app = FastAPI(
        title="Large Only Optimized STT Server",
        description="Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ ìµœì í™” STT ì„œë²„",
        version="2.0.0"
    )
    
    # CORS ì„¤ì •
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # STT ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
    stt_service = None
    
    @app.on_event("startup")
    async def startup_event():
        nonlocal stt_service
        try:
            logger.info("ğŸš€ Large Only ê·¹í•œ ìµœì í™” STT ì„œë²„ ì‹œì‘ ì¤‘...")
            
            # GPU ì •ë³´ ì¶œë ¥
            logger.info(f"GPU: {torch.cuda.get_device_name()}")
            logger.info(f"CUDA ë²„ì „: {torch.version.cuda}")
            logger.info(f"PyTorch ë²„ì „: {torch.__version__}")
            logger.info(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            
            # STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            stt_service = LargeOnlyOptimizedSTTService()
            await stt_service._load_large_model()
            await stt_service._warmup_model()
            
            logger.info("âœ… ì„œë²„ ì‹œì‘ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
            raise

    @app.get("/health", response_model=HealthResponse)
    async def health_check():
        """í—¬ìŠ¤ ì²´í¬"""
        if not stt_service:
            raise HTTPException(status_code=503, detail="ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return stt_service.get_status()

    @app.post("/transcribe", response_model=STTResponse)
    async def transcribe_audio(request: STTRequest):
        """ì˜¤ë””ì˜¤ ì „ì‚¬"""
        if not stt_service:
            raise HTTPException(status_code=503, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # Base64 ë””ì½”ë”©
            audio_bytes = base64.b64decode(request.audio_data)
            
            # ì „ì‚¬ ì²˜ë¦¬
            result = await stt_service.transcribe(
                audio_data=audio_bytes,
                language=request.language,
                audio_format=request.audio_format,
                vad_enabled=request.vad_enabled
            )
            
            return STTResponse(**result)
            
        except Exception as e:
            logger.error(f"âŒ ì „ì‚¬ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"ì „ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    @app.post("/transcribe/keywords", response_model=KeywordBoostResponse)
    async def transcribe_with_keywords(request: STTWithKeywordsRequest):
        """í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ (í˜„ì¬ëŠ” ê¸°ë³¸ ì „ì‚¬ì™€ ë™ì¼)"""
        if not stt_service:
            raise HTTPException(status_code=503, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # Base64 ë””ì½”ë”©
            audio_bytes = base64.b64decode(request.audio_data)
            
            # ì „ì‚¬ ì²˜ë¦¬
            result = await stt_service.transcribe(
                audio_data=audio_bytes,
                language=request.language,
                audio_format=request.audio_format,
                vad_enabled=request.vad_enabled
            )
            
            # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì‘ë‹µ í˜•íƒœë¡œ ë³€í™˜
            return KeywordBoostResponse(
                text=result["text"],
                language=result["language"],
                rtf=result["rtf"],
                processing_time=result["processing_time"],
                confidence=result["confidence"],
                keywords_detected=[],  # í˜„ì¬ëŠ” í‚¤ì›Œë“œ ê°ì§€ ë¯¸êµ¬í˜„
                boost_applied=False,  # í˜„ì¬ëŠ” ë¶€ìŠ¤íŒ… ë¯¸êµ¬í˜„
                audio_duration=result["audio_duration"],
                audio_format=result["audio_format"],
                vad_enabled=result["vad_enabled"]
            )
            
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"ì „ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    return app

# Gunicornì´ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ì•± ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì „ì—­ ìŠ¤ì½”í”„ì—ì„œ ìƒì„±í•©ë‹ˆë‹¤.
app = create_app()

# ==================== Main ====================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Large Only Optimized STT Server")
    parser.add_argument("--host", default="0.0.0.0", help="ì„œë²„ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8003, help="ì„œë²„ í¬íŠ¸")
    
    args = parser.parse_args()
    
    # ì„œë²„ ì‹¤í–‰
    import uvicorn
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        workers=1,  # ë‹¨ì¼ ì›Œì»¤ë¡œ ì‹¤í–‰ (GPU ê³µìœ  ë¬¸ì œ ë°©ì§€)
        loop="asyncio"
    )

if __name__ == "__main__":
    main() 
#!/bin/bash
# STT Decoder í™˜ê²½ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
# cuDNN ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ë° CUDA í™˜ê²½ ì„¤ì •

echo "ğŸš€ STT Decoder í™˜ê²½ ì„¤ì • ì¤‘..."

# cuDNN ìµœì í™” ë¹„í™œì„±í™” (ì•ˆì •ì„±ì„ ìœ„í•´)
export TORCH_CUDNN_ENABLED=0

# cuDNN ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì„¤ì • (í•„ìš”ì‹œ)
export LD_LIBRARY_PATH="/home/jonghooy/miniconda3/envs/stt-decoder/lib/python3.9/site-packages/nvidia/cudnn/lib:$LD_LIBRARY_PATH"

# CUDA ìµœì í™” ì„¤ì •
export CUDA_VISIBLE_DEVICES=0

echo "âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ:"
echo "   TORCH_CUDNN_ENABLED: $TORCH_CUDNN_ENABLED (Safe Mode)"
echo "   LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
echo "   CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

# í™˜ê²½ í™•ì¸
echo ""
echo "ğŸ” í™˜ê²½ í™•ì¸ ì¤‘..."
python -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'GPU count: {torch.cuda.device_count()}')
if torch.cuda.is_available():
    print(f'GPU name: {torch.cuda.get_device_name(0)}')
    print(f'cuDNN enabled: {torch.backends.cudnn.enabled}')
    if torch.backends.cudnn.is_available():
        print(f'cuDNN version: {torch.backends.cudnn.version()}')
"

echo ""
echo "âœ… í™˜ê²½ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
echo ""
echo "ğŸ“– ì‚¬ìš©ë²•:"
echo "   ì„œë²„ ì‹¤í–‰ (ì•ˆì „ ëª¨ë“œ): TORCH_CUDNN_ENABLED=0 python src/api/server.py --host 0.0.0.0 --port 8000"
echo "   RTF í…ŒìŠ¤íŠ¸ (ì•ˆì „ ëª¨ë“œ): python test_current_rtf_performance_safe.py"
echo ""
echo "ğŸ¯ ì„±ëŠ¥ ê²°ê³¼:"
echo "   í˜„ì¬ RTF: 0.0028x (350ë°° ì‹¤ì‹œê°„ ì†ë„)"
echo "   ì„±ëŠ¥ ë“±ê¸‰: ğŸŒŸ EXCELLENT"
echo "   ì•ˆì •ì„±: âœ… cuDNN ì¶©ëŒ í•´ê²°ë¨"
echo ""
echo "ğŸ’¡ ì°¸ê³ :"
echo "   - Safe Modeì—ì„œë„ ë›°ì–´ë‚œ ì„±ëŠ¥ (350x ì‹¤ì‹œê°„ ì†ë„)"
echo "   - cuDNN ìµœì í™” ë¹„í™œì„±í™”ë¡œ ì•ˆì •ì„± í™•ë³´"
echo "   - ëª¨ë“  RTX 4090 ìµœì í™”ëŠ” ê·¸ëŒ€ë¡œ ì ìš©ë¨" 
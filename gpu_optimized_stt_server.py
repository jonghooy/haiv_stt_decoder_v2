#!/usr/bin/env python3
"""
GPU Optimized STT Server with Queueing System
cuDNNì„ ì™„ì „íˆ í™œì„±í™”í•˜ê³  RTX 4090 ìµœì í™”ë¥¼ ì ìš©í•œ STT API ì„œë²„
20ê°œ ì´í•˜ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìœ„í•œ ì§€ëŠ¥í˜• íì‰ ì‹œìŠ¤í…œ í¬í•¨
"""

import sys
import os
sys.path.append('/home/jonghooy/haiv_stt_decoder_v2')

import asyncio
import time
import torch
import torchaudio
import numpy as np
import logging
import base64
import uvicorn
import uuid
import json
import tempfile
import shutil
import zipfile
from collections import defaultdict, deque, Counter
from enum import Enum
from dataclasses import dataclass, field, asdict
from typing import Optional, Dict, Any, List, Callable, Set
from datetime import datetime, timedelta
from pathlib import Path
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks, Request, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from pydantic import BaseModel
from contextlib import asynccontextmanager
import re

# GPU ìµœì í™” ì„¤ì • - Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ ìµœì í™”
def setup_extreme_gpu_optimizations():
    """Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ GPU ìµœì í™”"""
    if not torch.cuda.is_available():
        raise RuntimeError("CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
        
    logger.info("ğŸš€ Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ GPU ìµœì í™” ì ìš© ì¤‘...")
    
    # GPU ë©”ëª¨ë¦¬ ì„¤ì • (PyTorch 2.5+ í˜¸í™˜)
    try:
        if hasattr(torch.cuda, 'set_memory_fraction'):
            torch.cuda.set_memory_fraction(0.95)  # GPU ë©”ëª¨ë¦¬ 95% ì‚¬ìš©
            logger.info("âœ… CUDA ë©”ëª¨ë¦¬ fraction ì„¤ì • ì™„ë£Œ")
        else:
            # PyTorch 2.5+ í˜¸í™˜ ë©”ëª¨ë¦¬ ì„¤ì •
            torch.cuda.memory.set_per_process_memory_fraction(0.95)
            logger.info("âœ… CUDA í”„ë¡œì„¸ìŠ¤ë³„ ë©”ëª¨ë¦¬ fraction ì„¤ì • ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ CUDA ë©”ëª¨ë¦¬ fraction ì„¤ì • ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬ë§Œ ìˆ˜í–‰
        torch.cuda.empty_cache()
        
    torch.backends.cudnn.enabled = True
    torch.backends.cudnn.benchmark = True  # cuDNN ë²¤ì¹˜ë§ˆí¬ í™œì„±í™”
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.matmul.allow_tf32 = True  # TF32 í™œì„±í™”
    torch.backends.cudnn.allow_tf32 = True
    
    # CUDA ë©”ëª¨ë¦¬ í’€ ìµœì í™” (PyTorch 2.5+ í˜¸í™˜)
    try:
        if hasattr(torch.cuda, 'memory') and hasattr(torch.cuda.memory, 'set_memory_pool_limit'):
            torch.cuda.memory.set_memory_pool_limit(0.95)
            logger.info("âœ… CUDA ë©”ëª¨ë¦¬ í’€ ì œí•œ ì„¤ì • ì™„ë£Œ")
        else:
            # PyTorch 2.5+ í˜¸í™˜ ë©”ëª¨ë¦¬ ìµœì í™”
            torch.cuda.empty_cache()
            logger.info("âœ… CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ CUDA ë©”ëª¨ë¦¬ ì„¤ì • ì‹¤íŒ¨: {e}")
        torch.cuda.empty_cache()  # ìµœì†Œí•œ ë©”ëª¨ë¦¬ ì •ë¦¬ëŠ” ìˆ˜í–‰
            
    # Mixed precision í™œì„±í™”
    try:
        torch.backends.cuda.enable_flash_sdp(True)
        logger.info("âœ… Flash Attention SDP í™œì„±í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ Flash Attention SDP í™œì„±í™” ì‹¤íŒ¨: {e}")
    
    logger.info("âœ… ê·¹í•œ GPU ìµœì í™” ì™„ë£Œ")

async def warmup_large_model(stt_service):
    """Large-v3 ëª¨ë¸ ì›œì—… (ì²« ìš”ì²­ ì§€ì—° ìµœì†Œí™”)"""
    try:
        logger.info("ğŸ”¥ Large-v3 ëª¨ë¸ ì›œì—… ì‹œì‘...")
        
        # ë”ë¯¸ ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„± (1ì´ˆ, 16kHz)
        dummy_audio = np.random.randn(16000).astype(np.float32) * 0.1  # ì‘ì€ ë³¼ë¥¨
        
        # ëª¨ë¸ì˜ ë‚´ë¶€ transcribe ë©”ì„œë“œë¥¼ ì§ì ‘ ì‚¬ìš© (ê°€ì¥ ì•ˆì „í•¨)
        start_time = time.time()
        # STT ì„œë¹„ìŠ¤ì˜ ë‚´ë¶€ ëª¨ë¸ì— ì•ˆì „í•˜ê²Œ ì ‘ê·¼
        model = None
        
        # 1. ì§ì ‘ model ì†ì„±ì´ ìˆëŠ” ê²½ìš° (NeMo ë“±)
        if hasattr(stt_service, 'model') and hasattr(stt_service.model, 'transcribe'):
            model = stt_service.model
        # 2. WhisperSTTServiceAdapterì˜ ê²½ìš°
        elif hasattr(stt_service, 'whisper_service') and hasattr(stt_service.whisper_service, 'model'):
            model = stt_service.whisper_service.model
        # 3. ê¸°íƒ€ ì–´ëŒ‘í„° íŒ¨í„´
        elif hasattr(stt_service, 'service') and hasattr(stt_service.service, 'model'):
            model = stt_service.service.model
        
        if model and hasattr(model, 'transcribe'):
            # FasterWhisper ëª¨ë¸ì˜ transcribe ë©”ì„œë“œ ì§ì ‘ í˜¸ì¶œ (Large-v3 ìµœì í™”)
            segments, info = model.transcribe(
                dummy_audio,
                beam_size=5,  # Large-v3ì— ìµœì í™”ëœ beam_size
                best_of=5,    # Large-v3ì— ìµœì í™”ëœ best_of
                temperature=0.0,
                vad_filter=False,
                language="ko"
            )
            # ê²°ê³¼ ì†Œë¹„
            list(segments)
        else:
            logger.info("ì›œì—…ì„ ìœ„í•œ ì§ì ‘ ëª¨ë¸ ì ‘ê·¼ ë¶ˆê°€, ì›œì—… ê±´ë„ˆëœ€")
            
        warmup_time = time.time() - start_time
        
        logger.info(f"âœ… Large-v3 ëª¨ë¸ ì›œì—… ì™„ë£Œ ({warmup_time:.3f}ì´ˆ)")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ëª¨ë¸ ì›œì—… ì‹¤íŒ¨ (ë¹„ì¤‘ìš”): {e}")
        # ì›œì—… ì‹¤íŒ¨ëŠ” ì„œë²„ ì‹œì‘ì„ ë§‰ì§€ ì•ŠìŒ
        pass

# STT ì„œë¹„ìŠ¤ ê´€ë ¨ imports
from src.api.stt_service import FasterWhisperSTTService
from src.api.base_stt_service import BaseSTTService
from src.api.stt_factory import STTServiceFactory

from src.api.post_processing_correction import (
    get_post_processing_corrector,
    apply_keyword_correction,
    CorrectionResult
)
from src.api.models import (
    KeywordRegistrationRequest,
    KeywordCorrectionRequest,
    KeywordCorrectionResponse,
    TranscriptionWithCorrection,
    KeywordStatsResponse,
    ProcessingMetrics
)

# ë¡œê¹… ì„¤ì • - ë””ë²„ê¹…ì„ ìœ„í•´ DEBUG ë ˆë²¨ë¡œ ì„¤ì •, íŒŒì¼ê³¼ ì½˜ì†” ë™ì‹œ ì¶œë ¥
import logging.handlers

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
log_file = log_dir / "stt_server_debug.log"

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# í¬ë§·í„° ì„¤ì •
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# íŒŒì¼ í•¸ë“¤ëŸ¬ (íšŒì „ ë¡œê·¸ íŒŒì¼)
file_handler = logging.handlers.RotatingFileHandler(
    log_file, 
    maxBytes=50*1024*1024,  # 50MB
    backupCount=5,
    encoding='utf-8'
)
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)

# ì½˜ì†” í•¸ë“¤ëŸ¬
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)  # ì½˜ì†”ì€ INFO ë ˆë²¨ë§Œ
console_handler.setFormatter(formatter)

# í•¸ë“¤ëŸ¬ ì¶”ê°€
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# ë£¨íŠ¸ ë¡œê±°ë„ ì„¤ì •
logging.getLogger().setLevel(logging.DEBUG)
logging.getLogger().addHandler(file_handler)

# ê·¹í•œ GPU ìµœì í™” ì‹¤í–‰ (logger ì •ì˜ í›„)
setup_extreme_gpu_optimizations()

# ============================================================================
# ëª¨ë¸ ì„ íƒì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ ë° ì„¤ì •
# ============================================================================

# ì„œë²„ ì„¤ì •ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ë“¤ (argparseë¡œ ì„¤ì •ë¨)
SERVER_MODEL_TYPE = "whisper"  # ê¸°ë³¸ê°’: whisper
SERVER_MODEL_NAME = "large-v3"  # ê¸°ë³¸ê°’: large-v3
SERVER_HOST = "0.0.0.0"
SERVER_PORT = 8004

# ============================================================================
# ì§€ëŠ¥í˜• íì‰ ì‹œìŠ¤í…œ êµ¬í˜„
# ============================================================================

class RequestPriority(Enum):
    """ìš”ì²­ ìš°ì„ ìˆœìœ„"""
    HIGH = 1
    MEDIUM = 2
    LOW = 3

class RequestStatus(Enum):
    """ìš”ì²­ ìƒíƒœ"""
    QUEUED = "queued"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"
    CANCELLED = "cancelled"

@dataclass
class QueuedRequest:
    """íì— ë“¤ì–´ê°„ ìš”ì²­"""
    request_id: str
    client_id: str
    audio_data: str
    language: str
    audio_format: str
    priority: RequestPriority
    created_at: datetime
    timeout_at: datetime
    status: RequestStatus = RequestStatus.QUEUED
    processing_started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    
    def __post_init__(self):
        """ìš”ì²­ IDê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±"""
        if not self.request_id:
            self.request_id = str(uuid.uuid4())
    
    def __lt__(self, other):
        """ìš°ì„ ìˆœìœ„ íì—ì„œ ë¹„êµë¥¼ ìœ„í•œ ë©”ì„œë“œ"""
        if not isinstance(other, QueuedRequest):
            return NotImplemented
        # ìš°ì„ ìˆœìœ„ê°€ ê°™ìœ¼ë©´ ìƒì„± ì‹œê°„ìœ¼ë¡œ ë¹„êµ (ë¨¼ì € ìƒì„±ëœ ê²ƒì´ ìš°ì„ )
        if self.priority.value == other.priority.value:
            return self.created_at < other.created_at
        return self.priority.value < other.priority.value

@dataclass
class QueueStats:
    """í í†µê³„"""
    total_requests: int = 0
    queued_requests: int = 0
    processing_requests: int = 0
    completed_requests: int = 0
    failed_requests: int = 0
    timeout_requests: int = 0
    average_wait_time: float = 0.0
    average_processing_time: float = 0.0
    current_throughput: float = 0.0

# Helper functions
def logprob_to_confidence(avg_logprob: float) -> float:
    """
    Whisperì˜ avg_logprobì„ ì‹ ë¢°ë„ ì ìˆ˜(0.0-1.0)ë¡œ ë³€í™˜ (ê°œì„ ëœ ë²„ì „)
    
    Args:
        avg_logprob: Whisper ì„¸ê·¸ë¨¼íŠ¸ì˜ í‰ê·  ë¡œê·¸ í™•ë¥ 
        
    Returns:
        0.0-1.0 ë²”ìœ„ì˜ ì‹ ë¢°ë„ ì ìˆ˜
    """
    if avg_logprob is None:
        return 0.0
    
    # avg_logprobì„ ë” ì„¸ë°€í•˜ê²Œ êµ¬ê°„ë³„ë¡œ ë§¤í•‘
    # ì‹¤ì œ Whisper ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¡°ì •ëœ êµ¬ê°„ë“¤
    if avg_logprob >= -0.1:
        # ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„: -0.1 ì´ìƒ
        return min(0.99, 0.92 + (avg_logprob + 0.1) * 0.7)
    elif avg_logprob >= -0.3:
        # ë†’ì€ ì‹ ë¢°ë„: -0.3 ~ -0.1
        return 0.85 + (avg_logprob + 0.3) * 0.35  # -0.3~-0.1 -> 0.85~0.92
    elif avg_logprob >= -0.6:
        # ì¤‘ìƒ ì‹ ë¢°ë„: -0.6 ~ -0.3
        return 0.75 + (avg_logprob + 0.6) * 0.33  # -0.6~-0.3 -> 0.75~0.85
    elif avg_logprob >= -1.0:
        # ì¤‘ê°„ ì‹ ë¢°ë„: -1.0 ~ -0.6
        return 0.60 + (avg_logprob + 1.0) * 0.375  # -1.0~-0.6 -> 0.60~0.75
    elif avg_logprob >= -1.5:
        # ì¤‘í•˜ ì‹ ë¢°ë„: -1.5 ~ -1.0
        return 0.45 + (avg_logprob + 1.5) * 0.30   # -1.5~-1.0 -> 0.45~0.60
    elif avg_logprob >= -2.5:
        # ë‚®ì€ ì‹ ë¢°ë„: -2.5 ~ -1.5
        return 0.25 + (avg_logprob + 2.5) * 0.20   # -2.5~-1.5 -> 0.25~0.45
    else:
        # ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„: -2.5 ì´í•˜
        return max(0.05, 0.25 + (avg_logprob + 2.5) * 0.08)


def calculate_segment_confidence_from_words(words: List[dict], avg_logprob: float) -> float:
    """
    ë‹¨ì–´ ë ˆë²¨ ì‹ ë¢°ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì‹ ë¢°ë„ ê³„ì‚°
    
    Args:
        words: ë‹¨ì–´ë³„ ì‹ ë¢°ë„ ì •ë³´ê°€ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸
        avg_logprob: ì„¸ê·¸ë¨¼íŠ¸ì˜ í‰ê·  ë¡œê·¸ í™•ë¥  (fallbackìš©)
        
    Returns:
        ê³„ì‚°ëœ ì„¸ê·¸ë¨¼íŠ¸ ì‹ ë¢°ë„
    """
    if not words:
        # ë‹¨ì–´ ì •ë³´ê°€ ì—†ìœ¼ë©´ avg_logprob ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
        return logprob_to_confidence(avg_logprob)
    
    # ë‹¨ì–´ë³„ ì‹ ë¢°ë„ ìˆ˜ì§‘
    word_confidences = []
    for word in words:
        if isinstance(word, dict) and 'confidence' in word:
            confidence = word['confidence']
            if confidence is not None and confidence > 0:
                word_confidences.append(confidence)
    
    if not word_confidences:
        # ìœ íš¨í•œ ë‹¨ì–´ ì‹ ë¢°ë„ê°€ ì—†ìœ¼ë©´ avg_logprob ì‚¬ìš©
        return logprob_to_confidence(avg_logprob)
    
    # ê°€ì¤‘ í‰ê·  ê³„ì‚° (ë‚®ì€ ì‹ ë¢°ë„ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
    # ì´ëŠ” ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì‹ ë¢°ë„ë¥¼ ë³´ìˆ˜ì ìœ¼ë¡œ í‰ê°€í•˜ê¸° ìœ„í•¨
    sorted_confidences = sorted(word_confidences)
    
    if len(sorted_confidences) == 1:
        return sorted_confidences[0]
    
    # í•˜ìœ„ 30%, ì¤‘ìœ„ 40%, ìƒìœ„ 30%ë¡œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    n = len(sorted_confidences)
    lower_30_idx = max(1, int(n * 0.3))
    upper_70_idx = max(lower_30_idx + 1, int(n * 0.7))
    
    lower_30 = sorted_confidences[:lower_30_idx]
    middle_40 = sorted_confidences[lower_30_idx:upper_70_idx]
    upper_30 = sorted_confidences[upper_70_idx:]
    
    # ê°€ì¤‘ í‰ê·  (ë‚®ì€ ì‹ ë¢°ë„ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
    weighted_sum = 0.0
    total_weight = 0.0
    
    # í•˜ìœ„ 30%: ê°€ì¤‘ì¹˜ 0.5
    if lower_30:
        weighted_sum += sum(lower_30) * 0.5
        total_weight += len(lower_30) * 0.5
    
    # ì¤‘ìœ„ 40%: ê°€ì¤‘ì¹˜ 0.3  
    if middle_40:
        weighted_sum += sum(middle_40) * 0.3
        total_weight += len(middle_40) * 0.3
        
    # ìƒìœ„ 30%: ê°€ì¤‘ì¹˜ 0.2
    if upper_30:
        weighted_sum += sum(upper_30) * 0.2
        total_weight += len(upper_30) * 0.2
    
    if total_weight > 0:
        final_confidence = weighted_sum / total_weight
        # avg_logprobì™€ì˜ ì¡°í•© (70% word-based, 30% logprob-based)
        logprob_confidence = logprob_to_confidence(avg_logprob)
        return final_confidence * 0.7 + logprob_confidence * 0.3
    else:
        return logprob_to_confidence(avg_logprob)


class IntelligentSTTQueue:
    """ì§€ëŠ¥í˜• STT ì²˜ë¦¬ í"""
    
    def __init__(self, 
                 max_concurrent: int = 8,
                 max_queue_size: int = 50,
                 default_timeout: int = 60,
                 priority_timeout: int = 30):
        self.max_concurrent = max_concurrent
        self.max_queue_size = max_queue_size
        self.default_timeout = default_timeout
        self.priority_timeout = priority_timeout
        
        # í ê´€ë¦¬
        self.priority_queue: asyncio.PriorityQueue = asyncio.PriorityQueue()
        self.processing_requests: Dict[str, QueuedRequest] = {}
        self.completed_requests: Dict[str, QueuedRequest] = {}
        self.request_futures: Dict[str, asyncio.Future] = {}
        
        # ë™ì‹œì„± ì œì–´
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.processing_lock = asyncio.Lock()
        
        # í†µê³„ ë° ëª¨ë‹ˆí„°ë§
        self.stats = QueueStats()
        self.start_time = datetime.now()
        self.last_throughput_calc = datetime.now()
        self.recent_completions: List[datetime] = []
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
        self._cleanup_task: Optional[asyncio.Task] = None
        self._monitor_task: Optional[asyncio.Task] = None
        self._running = False
    
    async def start(self):
        """í ì‹œìŠ¤í…œ ì‹œì‘"""
        if self._running:
            return
        
        self._running = True
        self._cleanup_task = asyncio.create_task(self._cleanup_expired_requests())
        self._monitor_task = asyncio.create_task(self._monitor_performance())
        logger.info(f"ğŸ”„ STT í ì‹œìŠ¤í…œ ì‹œì‘ - ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬: {self.max_concurrent}ê°œ")
    
    async def stop(self):
        """í ì‹œìŠ¤í…œ ì¤‘ì§€"""
        self._running = False
        
        if self._cleanup_task:
            self._cleanup_task.cancel()
        if self._monitor_task:
            self._monitor_task.cancel()
        
        # ëª¨ë“  ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ ì·¨ì†Œ
        while not self.priority_queue.empty():
            try:
                _, request = self.priority_queue.get_nowait()
                request.status = RequestStatus.CANCELLED
                if request.request_id in self.request_futures:
                    future = self.request_futures[request.request_id]
                    if not future.done():
                        future.cancel()
            except asyncio.QueueEmpty:
                break
        
        logger.info("ğŸ›‘ STT í ì‹œìŠ¤í…œ ì¤‘ì§€")
    
    async def submit_request(self, 
                           audio_data: str,
                           language: str = "ko",
                           audio_format: str = "pcm_16khz",
                           client_id: str = None,
                           priority: RequestPriority = RequestPriority.MEDIUM,
                           timeout: Optional[int] = None) -> str:
        """ìš”ì²­ì„ íì— ì œì¶œ"""
        
        # í í¬ê¸° í™•ì¸
        current_queue_size = self.priority_queue.qsize()
        if current_queue_size >= self.max_queue_size:
            raise HTTPException(status_code=503, detail=f"Queue is full ({current_queue_size}/{self.max_queue_size})")
        
        # ìš”ì²­ ìƒì„±
        request_id = str(uuid.uuid4())
        client_id = client_id or f"client_{uuid.uuid4().hex[:8]}"
        timeout_seconds = timeout or (self.priority_timeout if priority == RequestPriority.HIGH else self.default_timeout)
        
        request = QueuedRequest(
            request_id=request_id,
            client_id=client_id,
            audio_data=audio_data,
            language=language,
            audio_format=audio_format,
            priority=priority,
            created_at=datetime.now(),
            timeout_at=datetime.now() + timedelta(seconds=timeout_seconds)
        )
        
        # Future ìƒì„±
        future = asyncio.Future()
        self.request_futures[request_id] = future
        
        # ìš°ì„ ìˆœìœ„ íì— ì¶”ê°€ (ìš°ì„ ìˆœìœ„ê°€ ë†’ì„ìˆ˜ë¡ ë¨¼ì € ì²˜ë¦¬)
        await self.priority_queue.put((priority.value, request))
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        async with self.processing_lock:
            self.stats.total_requests += 1
            self.stats.queued_requests += 1
        
        logger.info(f"ğŸ“ ìš”ì²­ í ì¶”ê°€ - ID: {request_id}, í´ë¼ì´ì–¸íŠ¸: {client_id}, ìš°ì„ ìˆœìœ„: {priority.name}, í í¬ê¸°: {current_queue_size + 1}")
        
        return request_id
    
    async def get_request_result(self, request_id: str) -> Dict[str, Any]:
        """ìš”ì²­ ê²°ê³¼ ëŒ€ê¸° ë° ë°˜í™˜"""
        if request_id not in self.request_futures:
            raise HTTPException(status_code=404, detail="Request not found")
        
        future = self.request_futures[request_id]
        
        try:
            # ê²°ê³¼ ëŒ€ê¸°
            result = await future
            
            # ì™„ë£Œëœ ìš”ì²­ ì •ë¦¬
            if request_id in self.request_futures:
                del self.request_futures[request_id]
            
            return result
        
        except asyncio.CancelledError:
            raise HTTPException(status_code=408, detail="Request was cancelled")
        except asyncio.TimeoutError:
            raise HTTPException(status_code=408, detail="Request timed out")
    
    async def get_request_status(self, request_id: str) -> Dict[str, Any]:
        """ìš”ì²­ ìƒíƒœ ì¡°íšŒ"""
        # ì²˜ë¦¬ ì¤‘ì¸ ìš”ì²­ í™•ì¸
        if request_id in self.processing_requests:
            request = self.processing_requests[request_id]
            wait_time = (datetime.now() - request.created_at).total_seconds()
            processing_time = (datetime.now() - request.processing_started_at).total_seconds() if request.processing_started_at else 0
            
            return {
                "request_id": request_id,
                "status": request.status.value,
                "priority": request.priority.name,
                "wait_time": wait_time,
                "processing_time": processing_time,
                "estimated_remaining_time": max(0, request.timeout_at.timestamp() - datetime.now().timestamp())
            }
        
        # ì™„ë£Œëœ ìš”ì²­ í™•ì¸
        if request_id in self.completed_requests:
            request = self.completed_requests[request_id]
            total_time = (request.completed_at - request.created_at).total_seconds()
            
            return {
                "request_id": request_id,
                "status": request.status.value,
                "priority": request.priority.name,
                "total_time": total_time,
                "completed_at": request.completed_at.isoformat(),
                "result": request.result,
                "error_message": request.error_message
            }
        
        # íì—ì„œ ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ í™•ì¸
        queue_position = await self._get_queue_position(request_id)
        if queue_position is not None:
            return {
                "request_id": request_id,
                "status": RequestStatus.QUEUED.value,
                "queue_position": queue_position,
                "estimated_wait_time": self._estimate_wait_time(queue_position)
            }
        
        raise HTTPException(status_code=404, detail="Request not found")
    
    async def cancel_request(self, request_id: str) -> bool:
        """ìš”ì²­ ì·¨ì†Œ"""
        # Futureê°€ ìˆìœ¼ë©´ ì·¨ì†Œ
        if request_id in self.request_futures:
            future = self.request_futures[request_id]
            if not future.done():
                future.cancel()
                del self.request_futures[request_id]
                return True
        
        # ì²˜ë¦¬ ì¤‘ì¸ ìš”ì²­ì€ ì·¨ì†Œí•  ìˆ˜ ì—†ìŒ
        if request_id in self.processing_requests:
            return False
        
        return False
    
    async def get_queue_stats(self) -> QueueStats:
        """í í†µê³„ ë°˜í™˜"""
        async with self.processing_lock:
            # ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸
            self.stats.queued_requests = self.priority_queue.qsize()
            self.stats.processing_requests = len(self.processing_requests)
            
            # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
            now = datetime.now()
            time_diff = (now - self.last_throughput_calc).total_seconds()
            if time_diff >= 10:  # 10ì´ˆë§ˆë‹¤ ì²˜ë¦¬ëŸ‰ ì—…ë°ì´íŠ¸
                recent_count = len([t for t in self.recent_completions if (now - t).total_seconds() <= 60])
                self.stats.current_throughput = recent_count / 60.0  # ë¶„ë‹¹ ì²˜ë¦¬ëŸ‰
                self.last_throughput_calc = now
            
            return self.stats
    
    async def process_requests(self, stt_service):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìš”ì²­ ì²˜ë¦¬"""
        while self._running:
            try:
                # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì œí•œ
                async with self.semaphore:
                    # íì—ì„œ ìš”ì²­ ê°€ì ¸ì˜¤ê¸°
                    try:
                        _, request = await asyncio.wait_for(self.priority_queue.get(), timeout=1.0)
                    except asyncio.TimeoutError:
                        continue
                    
                    # íƒ€ì„ì•„ì›ƒ í™•ì¸
                    if datetime.now() > request.timeout_at:
                        request.status = RequestStatus.TIMEOUT
                        await self._complete_request(request, None, "Request timed out")
                        continue
                    
                    # ì²˜ë¦¬ ì‹œì‘
                    request.status = RequestStatus.PROCESSING
                    request.processing_started_at = datetime.now()
                    
                    async with self.processing_lock:
                        self.processing_requests[request.request_id] = request
                        self.stats.queued_requests = max(0, self.stats.queued_requests - 1)
                        self.stats.processing_requests += 1
                    
                    logger.info(f"ğŸ”„ ìš”ì²­ ì²˜ë¦¬ ì‹œì‘ - ID: {request.request_id}")
                    
                    # ì‹¤ì œ STT ì²˜ë¦¬
                    try:
                        result = await self._process_stt_request(request, stt_service)
                        await self._complete_request(request, result, None)
                        
                    except Exception as e:
                        error_msg = str(e)
                        logger.error(f"âŒ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨ - ID: {request.request_id}, ì˜¤ë¥˜: {error_msg}")
                        await self._complete_request(request, None, error_msg)
            
            except Exception as e:
                logger.error(f"âŒ í ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def _process_stt_request(self, request: QueuedRequest, stt_service) -> Dict[str, Any]:
        """ì‹¤ì œ STT ì²˜ë¦¬"""
        start_time = time.time()
        
        # Base64 ë””ì½”ë”©
        audio_bytes = base64.b64decode(request.audio_data)
        
        # STT ì²˜ë¦¬ - ì˜¬ë°”ë¥¸ ë©”ì„œë“œëª… ì‚¬ìš©
        result = await stt_service.transcribe_file_bytes(
            audio_bytes, 
            language=request.language
        )
        
        processing_time = time.time() - start_time
        
        # STTResult ê°ì²´ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        return {
            "text": result.text,
            "language": result.language,
            "rtf": result.rtf,
            "processing_time": processing_time,
            "audio_duration": result.audio_duration,
            "gpu_optimized": True,
            "queue_processed": True,
            "request_id": request.request_id,
            "client_id": request.client_id,
            "segments": result.segments
        }
    
    async def _complete_request(self, request: QueuedRequest, result: Optional[Dict[str, Any]], error_message: Optional[str]):
        """ìš”ì²­ ì™„ë£Œ ì²˜ë¦¬"""
        request.completed_at = datetime.now()
        request.result = result
        request.error_message = error_message
        request.status = RequestStatus.COMPLETED if result else RequestStatus.FAILED
        
        # ì²˜ë¦¬ ì¤‘ ëª©ë¡ì—ì„œ ì œê±°
        async with self.processing_lock:
            if request.request_id in self.processing_requests:
                del self.processing_requests[request.request_id]
                self.stats.processing_requests = max(0, self.stats.processing_requests - 1)
        
        # ì™„ë£Œ ëª©ë¡ì— ì¶”ê°€
        self.completed_requests[request.request_id] = request
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        if result:
            self.stats.completed_requests += 1
            self.recent_completions.append(datetime.now())
        else:
            self.stats.failed_requests += 1
        
        # Future ì™„ë£Œ
        if request.request_id in self.request_futures:
            future = self.request_futures[request.request_id]
            if not future.done():
                if result:
                    future.set_result(result)
                else:
                    future.set_exception(Exception(error_message or "Unknown error"))
        
        logger.info(f"âœ… ìš”ì²­ ì™„ë£Œ - ID: {request.request_id}, ìƒíƒœ: {request.status.value}")
    
    async def _cleanup_expired_requests(self):
        """ë§Œë£Œëœ ìš”ì²­ ì •ë¦¬"""
        while self._running:
            try:
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì •ë¦¬
                
                now = datetime.now()
                expired_ids = []
                
                # ì™„ë£Œëœ ìš”ì²­ ì¤‘ 1ì‹œê°„ ì´ìƒ ëœ ê²ƒ ì œê±°
                for request_id, request in list(self.completed_requests.items()):
                    if (now - request.completed_at).total_seconds() > 3600:  # 1ì‹œê°„
                        expired_ids.append(request_id)
                
                for request_id in expired_ids:
                    del self.completed_requests[request_id]
                
                # ìµœê·¼ ì™„ë£Œ ê¸°ë¡ ì •ë¦¬ (1ì‹œê°„ ì´ìƒ)
                self.recent_completions = [t for t in self.recent_completions if (now - t).total_seconds() <= 3600]
                
                if expired_ids:
                    logger.info(f"ğŸ§¹ ë§Œë£Œëœ ìš”ì²­ {len(expired_ids)}ê°œ ì •ë¦¬ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ ìš”ì²­ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _monitor_performance(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        while self._running:
            try:
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§
                
                stats = await self.get_queue_stats()
                logger.info(f"ğŸ“Š í ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§:")
                logger.info(f"   ëŒ€ê¸° ì¤‘: {stats.queued_requests}ê°œ")
                logger.info(f"   ì²˜ë¦¬ ì¤‘: {stats.processing_requests}ê°œ")
                logger.info(f"   ì™„ë£Œ: {stats.completed_requests}ê°œ")
                logger.info(f"   ì‹¤íŒ¨: {stats.failed_requests}ê°œ")
                logger.info(f"   ì²˜ë¦¬ëŸ‰: {stats.current_throughput:.2f} ìš”ì²­/ë¶„")
                
            except Exception as e:
                logger.error(f"âŒ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _get_queue_position(self, request_id: str) -> Optional[int]:
        """íì—ì„œì˜ ìœ„ì¹˜ í™•ì¸"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í ë‚´ìš©ì„ í™•ì¸í•´ì•¼ í•˜ì§€ë§Œ,
        # PriorityQueueëŠ” ë‚´ìš©ì„ ì§ì ‘ í™•ì¸í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì¶”ì •
        return self.priority_queue.qsize()
    
    def _estimate_wait_time(self, queue_position: int) -> float:
        """ëŒ€ê¸° ì‹œê°„ ì¶”ì •"""
        if self.stats.current_throughput > 0:
            return queue_position / (self.stats.current_throughput / 60.0)  # ì´ˆ ë‹¨ìœ„
        return queue_position * 30  # ê¸°ë³¸ 30ì´ˆ/ìš”ì²­ìœ¼ë¡œ ì¶”ì •

# ì „ì—­ í ì¸ìŠ¤í„´ìŠ¤
stt_queue: Optional[IntelligentSTTQueue] = None

# Pydantic ëª¨ë¸
class TranscriptionRequest(BaseModel):
    audio_data: str  # base64 ì¸ì½”ë”©ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
    language: Optional[str] = "ko"
    audio_format: Optional[str] = "pcm_16khz"

class WordSegment(BaseModel):
    word: str
    start: float
    end: float
    confidence: Optional[float] = None

class SegmentInfo(BaseModel):
    id: int
    text: str
    start: float
    end: float
    confidence: Optional[float] = None
    words: Optional[List[WordSegment]] = None

class TranscriptionResponse(BaseModel):
    text: str
    language: str
    rtf: float
    processing_time: float
    audio_duration: float
    gpu_optimized: bool
    model_load_time: Optional[float] = None
    segments: Optional[List[SegmentInfo]] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    model_loaded: bool
    cudnn_enabled: bool
    gpu_name: Optional[str] = None
    gpu_info: Optional[Dict[str, Any]] = None
    optimization_status: Optional[Dict[str, Any]] = None

# íì‰ ì‹œìŠ¤í…œ ê´€ë ¨ Pydantic ëª¨ë¸
class QueuedTranscriptionRequest(BaseModel):
    audio_data: str
    language: Optional[str] = "ko"
    audio_format: Optional[str] = "pcm_16khz"
    client_id: Optional[str] = None
    priority: Optional[str] = "medium"  # high, medium, low
    timeout: Optional[int] = None

class QueuedTranscriptionResponse(BaseModel):
    request_id: str
    status: str
    message: str
    estimated_wait_time: Optional[float] = None
    queue_position: Optional[int] = None

class QueueStatusResponse(BaseModel):
    request_id: str
    status: str
    priority: Optional[str] = None
    wait_time: Optional[float] = None
    processing_time: Optional[float] = None
    total_time: Optional[float] = None
    queue_position: Optional[int] = None
    estimated_wait_time: Optional[float] = None
    estimated_remaining_time: Optional[float] = None
    completed_at: Optional[str] = None
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None

class QueueStatsResponse(BaseModel):
    total_requests: int
    queued_requests: int
    processing_requests: int
    completed_requests: int
    failed_requests: int
    timeout_requests: int
    average_wait_time: float
    average_processing_time: float
    current_throughput: float
    queue_capacity: int
    max_concurrent: int

# ë°°ì¹˜ ì²˜ë¦¬ ê´€ë ¨ ëª¨ë¸ë“¤
class BatchTranscriptionRequest(BaseModel):
    """ë°°ì¹˜ STT ì²˜ë¦¬ ìš”ì²­"""
    language: Optional[str] = "ko"
    vad_filter: Optional[bool] = False
    enable_word_timestamps: Optional[bool] = True
    enable_confidence: Optional[bool] = True
    client_id: Optional[str] = None
    priority: Optional[str] = "medium"  # high, medium, low
    
    # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ê´€ë ¨ í•„ë“œ
    call_id: Optional[str] = None  # í‚¤ì›Œë“œê°€ ë“±ë¡ëœ call_id
    enable_keyword_boosting: Optional[bool] = False  # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… í™œì„±í™”
    keywords: Optional[List[str]] = None  # ì§ì ‘ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ì œê³µ
    keyword_boost_factor: Optional[float] = 2.0  # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ê°•ë„ (1.0-5.0)

class BatchFileInfo(BaseModel):
    """ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´"""
    filename: str
    size_bytes: int
    duration_seconds: float
    processing_time_seconds: float
    text: str
    language: str
    confidence: float
    segments: Optional[List[SegmentInfo]] = None

class BatchTranscriptionResponse(BaseModel):
    """ë°°ì¹˜ STT ì²˜ë¦¬ ì‘ë‹µ"""
    batch_id: str
    status: str
    message: str
    total_files: int
    processed_files: int
    failed_files: int
    total_duration: float
    total_processing_time: float
    created_at: str
    download_url: Optional[str] = None
    files: Optional[List[BatchFileInfo]] = None

class BatchStatusResponse(BaseModel):
    """ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœ ì‘ë‹µ"""
    batch_id: str
    status: str  # processing, completed, failed, cancelled
    progress: float  # 0.0 - 1.0
    total_files: int
    processed_files: int
    failed_files: int
    estimated_remaining_time: Optional[float] = None
    created_at: str
    completed_at: Optional[str] = None
    error_message: Optional[str] = None

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Large-v3 ê·¹í•œ ìµœì í™” STT API",
    description="Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ GPU ìµœì í™” ìŒì„± ì¸ì‹ API ì„œë²„ - float16, TF32, 95% GPU ë©”ëª¨ë¦¬ í™œìš©",
    version="3.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
# ============================================================================

import os
import json
import tempfile
import shutil
import io
from typing import Dict, List
from fastapi.responses import FileResponse
import zipfile

@dataclass
class BatchJob:
    """ë°°ì¹˜ ì‘ì—… ì •ë³´"""
    batch_id: str
    status: str = "processing"  # processing, completed, failed, cancelled
    total_files: int = 0
    processed_files: int = 0
    failed_files: int = 0
    total_duration: float = 0.0
    total_processing_time: float = 0.0
    created_at: datetime = field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    result_files: List[BatchFileInfo] = field(default_factory=list)
    temp_dir: Optional[str] = None
    download_path: Optional[str] = None

class BatchProcessor:
    """ë°°ì¹˜ STT ì²˜ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.batch_jobs: Dict[str, BatchJob] = {}
        self.processing_queue = asyncio.Queue()
        self.max_concurrent_batches = 2  # ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë°°ì¹˜ ìˆ˜
        self.current_processing = 0
        self.batch_temp_dir = "/tmp/stt_batch_processing"
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.batch_temp_dir, exist_ok=True)
    
    async def submit_batch(self, files: List[UploadFile], request: BatchTranscriptionRequest) -> str:
        """ë°°ì¹˜ ì‘ì—… ì œì¶œ"""
        batch_id = str(uuid.uuid4())
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_dir = os.path.join(self.batch_temp_dir, batch_id)
        os.makedirs(temp_dir, exist_ok=True)
        
        # ë°°ì¹˜ ì‘ì—… ìƒì„±
        batch_job = BatchJob(
            batch_id=batch_id,
            total_files=len(files),
            temp_dir=temp_dir
        )
        
        self.batch_jobs[batch_id] = batch_job
        
        # íŒŒì¼ë“¤ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
        saved_files = []
        for file in files:
            file_path = os.path.join(temp_dir, file.filename)
            content = await file.read()
            with open(file_path, "wb") as f:
                f.write(content)
            saved_files.append((file.filename, file_path, len(content)))
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
        asyncio.create_task(self._process_batch(batch_id, saved_files, request))
        
        return batch_id
    
    async def _process_batch(self, batch_id: str, files: List[tuple], request: BatchTranscriptionRequest):
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        batch_job = self.batch_jobs[batch_id]
        
        try:
            self.current_processing += 1
            logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì‹œì‘ (íŒŒì¼ ìˆ˜: {len(files)})")
            
            # ì‹œì‘ ì§„í–‰ ìƒí™© ì „ì†¡
            await progress_notifier.update_progress(batch_id, {
                "status": "processing",
                "progress": 0.0,
                "total_files": batch_job.total_files,
                "processed_files": 0,
                "failed_files": 0,
                "estimated_remaining_time": None,
                "current_file": None
            })
            
            start_time = time.time()
            
            # ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê°œì„ : RTX 4090 GPU ì„±ëŠ¥ í™œìš©
            # GPU ë©”ëª¨ë¦¬ì™€ ì²˜ë¦¬ ëŠ¥ë ¥ì— ë”°ë¼ ë™ì‹œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” íŒŒì¼ ìˆ˜ ê²°ì •
            max_concurrent = min(4, len(files))  # RTX 4090: ìµœëŒ€ 4ê°œ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬
            
            logger.info(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(files)}ê°œ íŒŒì¼ì„ ìµœëŒ€ {max_concurrent}ê°œì”© ë™ì‹œ ì²˜ë¦¬")
            
            # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ì œí•œ
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_single_file(filename: str, file_path: str, file_size: int, file_index: int):
                """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ì œí•œ)"""
                async with semaphore:
                    try:
                        logger.info(f"ğŸ”„ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {filename} ({file_index + 1}/{len(files)})")
                        
                        # ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
                        file_info = await self._process_audio_file(
                            filename, file_path, file_size, request
                        )
                        
                        # Thread-safe ì—…ë°ì´íŠ¸
                        batch_job.result_files.append(file_info)
                        batch_job.processed_files += 1
                        batch_job.total_duration += file_info.duration_seconds
                        batch_job.total_processing_time += file_info.processing_time_seconds
                        
                        logger.info(f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {filename} ({batch_job.processed_files}/{batch_job.total_files}) RTF: {file_info.processing_time_seconds/file_info.duration_seconds:.3f}")
                        
                        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                        await progress_notifier.update_progress(batch_id, {
                            "status": "processing",
                            "progress": batch_job.processed_files / batch_job.total_files,
                            "total_files": batch_job.total_files,
                            "processed_files": batch_job.processed_files,
                            "failed_files": batch_job.failed_files,
                            "current_file": f"ì²˜ë¦¬ ì™„ë£Œ: {filename}",
                            "total_duration": batch_job.total_duration,
                            "total_processing_time": batch_job.total_processing_time,
                            "estimated_remaining_time": self._estimate_remaining_time(
                                start_time, batch_job.processed_files, len(files)
                            )
                        })
                        
                        return file_info
                        
                    except Exception as e:
                        batch_job.failed_files += 1
                        logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {filename} - {str(e)}")
                        
                        # ì‹¤íŒ¨ ìƒí™© ì—…ë°ì´íŠ¸
                        await progress_notifier.update_progress(batch_id, {
                            "status": "processing",
                            "progress": (batch_job.processed_files + batch_job.failed_files) / batch_job.total_files,
                            "total_files": batch_job.total_files,
                            "processed_files": batch_job.processed_files,
                            "failed_files": batch_job.failed_files,
                            "current_file": f"{filename} (ì‹¤íŒ¨)",
                            "last_error": str(e),
                            "estimated_remaining_time": self._estimate_remaining_time(
                                start_time, batch_job.processed_files + batch_job.failed_files, len(files)
                            )
                        })
                        return None
            
            # ëª¨ë“  íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            tasks = [
                process_single_file(filename, file_path, file_size, i)
                for i, (filename, file_path, file_size) in enumerate(files)
            ]
            
            # asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì²˜ë¦¬ (ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨)
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {files[i][0]} - {result}")
                    batch_job.failed_files += 1
            
            # ê²°ê³¼ íŒŒì¼ ìƒì„± ì§„í–‰ ìƒí™©
            await progress_notifier.update_progress(batch_id, {
                "status": "creating_package",
                "progress": 0.95,
                "total_files": batch_job.total_files,
                "processed_files": batch_job.processed_files,
                "failed_files": batch_job.failed_files,
                "current_file": "ê²°ê³¼ íŒ¨í‚¤ì§€ ìƒì„± ì¤‘...",
                "total_duration": batch_job.total_duration,
                "total_processing_time": batch_job.total_processing_time
            })
            
            # ê²°ê³¼ íŒŒì¼ ìƒì„±
            result_zip_path = await self._create_result_package(batch_job)
            batch_job.download_path = result_zip_path
            batch_job.status = "completed"
            batch_job.completed_at = datetime.now()
            
            # ì™„ë£Œ ìƒí™© ì „ì†¡
            await progress_notifier.update_progress(batch_id, {
                "status": "completed",
                "progress": 1.0,
                "total_files": batch_job.total_files,
                "processed_files": batch_job.processed_files,
                "failed_files": batch_job.failed_files,
                "total_duration": batch_job.total_duration,
                "total_processing_time": batch_job.total_processing_time,
                "completed_at": batch_job.completed_at.isoformat(),
                "download_available": True
            })
            
            logger.info(f"âœ… ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            batch_job.status = "failed"
            batch_job.error_message = str(e)
            batch_job.completed_at = datetime.now()
            
            # ì‹¤íŒ¨ ìƒí™© ì „ì†¡
            await progress_notifier.update_progress(batch_id, {
                "status": "failed",
                "progress": batch_job.processed_files / batch_job.total_files if batch_job.total_files > 0 else 0.0,
                "total_files": batch_job.total_files,
                "processed_files": batch_job.processed_files,
                "failed_files": batch_job.failed_files,
                "error_message": str(e),
                "completed_at": batch_job.completed_at.isoformat()
            })
            
            logger.error(f"âŒ ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        finally:
            self.current_processing -= 1
            # ë°°ì¹˜ ì™„ë£Œ í›„ ì—°ê²° ì •ë¦¬ (ë¹„ë™ê¸°ë¡œ)
            asyncio.create_task(progress_notifier.cleanup_batch(batch_id))
    
    def _estimate_remaining_time(self, start_time: float, completed: int, total: int) -> Optional[float]:
        """ë‚¨ì€ ì‹œê°„ ì¶”ì •"""
        if completed == 0:
            return None
        
        elapsed_time = time.time() - start_time
        avg_time_per_file = elapsed_time / completed
        remaining_files = total - completed
        
        return avg_time_per_file * remaining_files
    
    async def _process_audio_file(self, filename: str, file_path: str, file_size: int, 
                                request: BatchTranscriptionRequest) -> BatchFileInfo:
        """ê°œë³„ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬"""
        start_time = time.time()
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì½ì–´ì„œ STT ì²˜ë¦¬
        with open(file_path, "rb") as f:
            audio_content = f.read()
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚°
        try:
            # librosa ë˜ëŠ” torchaudioë¡œ ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚°
            try:
                import librosa
                audio_data, sr = librosa.load(file_path, sr=16000)
                duration = len(audio_data) / sr
            except ImportError:
                # librosaê°€ ì—†ìœ¼ë©´ torchaudio ì‚¬ìš©
                import torchaudio
                waveform, sample_rate = torchaudio.load(file_path)
                duration = waveform.shape[1] / sample_rate
        except Exception:
            # ëŒ€ëµì ì¸ ì¶”ì • (í‰ê·  ë¹„íŠ¸ë ˆì´íŠ¸ ê¸°ì¤€)
            duration = file_size / (16000 * 2)  # 16kHz, 16bit ê¸°ì¤€
        
        # STT ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì „ì‚¬
        # íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ê¸°ì¡´ STT ì„œë¹„ìŠ¤ ì‚¬ìš©
        audio_base64 = base64.b64encode(audio_content).decode('utf-8')
        
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì˜¤ë””ì˜¤ í¬ë§· ê²°ì •
        file_ext = os.path.splitext(filename)[1].lower()
        audio_format = "wav"  # ê¸°ë³¸ê°’
        if file_ext in ['.mp3', '.m4a']:
            audio_format = "mp3"
        elif file_ext in ['.flac']:
            audio_format = "flac"
        elif file_ext in ['.ogg', '.webm']:
            audio_format = "ogg"
        
        # STT ì²˜ë¦¬ ìš”ì²­ ìƒì„±
        transcription_request = TranscriptionRequest(
            audio_data=audio_base64,
            language=request.language,
            audio_format=audio_format
        )
        
        # ì‹¤ì œ STT ì²˜ë¦¬ (ê¸°ì¡´ ì„œë¹„ìŠ¤ ì¬ì‚¬ìš©)
        result = await self._transcribe_for_batch(transcription_request, request)
        
        processing_time = time.time() - start_time
        
        # íŒŒì¼ ì •ë³´ ìƒì„±
        file_info = BatchFileInfo(
            filename=filename,
            size_bytes=file_size,
            duration_seconds=duration,
            processing_time_seconds=processing_time,
            text=result.get("text", ""),
            language=result.get("language", request.language),
            confidence=result.get("confidence", 0.0),
            segments=result.get("segments", [])
        )
        
        return file_info
    
    async def _transcribe_for_batch(self, request: TranscriptionRequest, batch_request: Optional[BatchTranscriptionRequest] = None) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ìš© STT (í™˜ê° í•„í„° ë° í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì ìš©)"""
        if not stt_service:
            raise HTTPException(status_code=503, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # í™˜ê° í•„í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        hallucination_filter = HallucinationFilter()
        
        # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ê´€ë ¨ ë³€ìˆ˜
        boosting_applied = False
        boosted_text = None
        keyword_stats = {}
        temp_call_id = None
        
        try:
            # base64 ë””ì½”ë”©
            audio_bytes = base64.b64decode(request.audio_data)
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            try:
                # librosaë¡œ ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í¬ë§· ì²˜ë¦¬
                import librosa
                audio_np, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)
            except ImportError:
                # librosaê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì²˜ë¦¬
                if request.audio_format == "wav":
                    # WAV íŒŒì¼ì€ ë‹¨ìˆœ PCMìœ¼ë¡œ ê°€ì •
                    audio_np = np.frombuffer(audio_bytes[44:], dtype=np.int16).astype(np.float32) / 32768.0
                else:
                    # PCM 16kHzë¡œ ê°€ì •
                    audio_np = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
            
            # STT ì²˜ë¦¬ - ì„œë¹„ìŠ¤ ì¢…ë¥˜ì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ ì‚¬ìš©
            start_time = time.time()
            
            # NeMo ì„œë¹„ìŠ¤ì¸ì§€ í™•ì¸
            from src.api.nemo_stt_service import NeMoSTTService
            if isinstance(stt_service, NeMoSTTService):
                # NeMo ì„œë¹„ìŠ¤ ì‚¬ìš©
                logger.info("ğŸ¤– NeMo ì„œë¹„ìŠ¤ë¡œ ë°°ì¹˜ ì „ì‚¬ ì²˜ë¦¬ ì¤‘...")
                logger.info(f"ğŸ” í˜¸ì¶œ íŒŒë¼ë¯¸í„° - audio_format: {request.audio_format}, language: {request.language}")
                logger.info(f"ğŸ” audio_data ê¸¸ì´: {len(request.audio_data)} chars")
                print(f"ğŸš¨ NeMo transcribe_audio í˜¸ì¶œ ì§ì „! audio_format={request.audio_format}")
                
                try:
                    result = await stt_service.transcribe_audio(
                        request.audio_data,
                        audio_format=request.audio_format,
                        language=request.language
                    )
                    print("ğŸš¨ NeMo transcribe_audio í˜¸ì¶œ ì„±ê³µ!")
                    logger.info("âœ… NeMo transcribe_audio í˜¸ì¶œ ì„±ê³µ!")
                except Exception as e:
                    print(f"ğŸš¨ NeMo transcribe_audio í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    logger.error(f"âŒ NeMo transcribe_audio í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    raise
                
                # NeMo ê²°ê³¼ë¥¼ Whisper í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                full_text = result.text
                processing_time = time.time() - start_time
                audio_duration = len(audio_np) / 16000
                
                # ê¸°ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± (NeMoëŠ” segmentsë¥¼ ë”°ë¡œ ì œê³µí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
                segments_list = [{
                    "text": full_text,
                    "start": 0.0,
                    "end": audio_duration,
                    "confidence": result.confidence if hasattr(result, 'confidence') else 0.8
                }]
                
                # Whisper ìŠ¤íƒ€ì¼ info ê°ì²´ ëª¨ë°©
                class NeMoInfo:
                    def __init__(self, language):
                        self.language = language
                
                info = NeMoInfo(request.language)
                
            else:
                # Whisper ì„œë¹„ìŠ¤ ì‚¬ìš©
                logger.info("ğŸ¤ Whisper ì„œë¹„ìŠ¤ë¡œ ë°°ì¹˜ ì „ì‚¬ ì²˜ë¦¬ ì¤‘...")
                
                # ì•ˆì „í•œ ëª¨ë¸ ì ‘ê·¼
                model = None
                if hasattr(stt_service, 'model') and hasattr(stt_service.model, 'transcribe'):
                    model = stt_service.model
                elif hasattr(stt_service, 'whisper_service') and hasattr(stt_service.whisper_service, 'model'):
                    model = stt_service.whisper_service.model
                elif hasattr(stt_service, 'service') and hasattr(stt_service.service, 'model'):
                    model = stt_service.service.model
                
                if model and hasattr(model, 'transcribe'):
                    segments, info = model.transcribe(
                        audio_np,
                        language=request.language,
                        word_timestamps=True,
                        beam_size=5,
                        best_of=5,
                        temperature=0.0
                    )
                    segments_list = list(segments)
                else:
                    # ëª¨ë¸ ì§ì ‘ ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ì„œë¹„ìŠ¤ ë©”ì„œë“œ ì‚¬ìš©
                    logger.info("ğŸ”„ ëª¨ë¸ ì§ì ‘ ì ‘ê·¼ ë¶ˆê°€, ì„œë¹„ìŠ¤ ë©”ì„œë“œ ì‚¬ìš©")
                    # STT ì„œë¹„ìŠ¤ì˜ transcribe ë©”ì„œë“œ ì‚¬ìš©
                    audio_bytes = (audio_np * 32768).astype(np.int16).tobytes()
                    audio_b64 = base64.b64encode(audio_bytes).decode('utf-8')
                    
                    result = await stt_service.transcribe_audio(
                        audio_data=audio_b64,
                        audio_format="pcm_16khz",
                        language=request.language
                    )
                    
                    # ê²°ê³¼ë¥¼ segments í˜•íƒœë¡œ ë³€í™˜
                    segments_list = []
                    if hasattr(result, 'segments') and result.segments:
                        segments_list = result.segments
                    else:
                        # ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì²˜ë¦¬
                        class MockSegment:
                            def __init__(self, text, start, end):
                                self.text = text
                                self.start = start
                                self.end = end
                                self.avg_logprob = -0.5
                                self.words = []
                        
                        segments_list = [MockSegment(result.text, 0.0, len(audio_np) / 16000.0)]
                    
                    # info ê°ì²´ ìƒì„±
                    class MockInfo:
                        def __init__(self, language):
                            self.language = language
                    
                    info = MockInfo(request.language)
            
            # ì‹ ë¢°ë„ ê³„ì‚° ë° í™˜ê° í•„í„° ì ìš©
            avg_confidence = 0.0
            segment_infos = []
            filtered_text_parts = []
            total_hallucination_count = 0
            
            for i, segment in enumerate(segments_list):
                # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ ê°ì²´ì¸ì§€ í™•ì¸
                if isinstance(segment, dict):
                    # NeMo ê²°ê³¼ (ë”•ì…”ë„ˆë¦¬)
                    segment_text = segment.get("text", "")
                    segment_start = segment.get("start", 0.0)
                    segment_end = segment.get("end", audio_duration)
                    segment_confidence = segment.get("confidence", 0.8)
                    words = segment.get("words", [])
                else:
                    # Whisper ê²°ê³¼ (ê°ì²´)
                    segment_text = segment.text
                    segment_start = segment.start
                    segment_end = segment.end
                    
                    # ë‹¨ì–´ ì •ë³´ ìˆ˜ì§‘
                    words = []
                    if hasattr(segment, 'words') and segment.words:
                        for word in segment.words:
                            # word.probabilityë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (0-1 ë²”ìœ„)
                            word_confidence = word.probability if hasattr(word, 'probability') and word.probability else 0.0
                            word_dict = {
                                "word": word.word,
                                "start": word.start,
                                "end": word.end,
                                "confidence": word_confidence
                            }
                            words.append(word_dict)
                    
                    # ê°œì„ ëœ ì‹ ë¢°ë„ ê³„ì‚°: word-level ì‹ ë¢°ë„ë¥¼ ìš°ì„  ì‚¬ìš©
                    segment_confidence = calculate_segment_confidence_from_words(
                        words, 
                        segment.avg_logprob if hasattr(segment, 'avg_logprob') else None
                    )
                
                # SegmentInfoë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ìƒì„± (JSON ì§ë ¬í™” ë¬¸ì œ ë°©ì§€)
                segment_dict = {
                    "id": i,
                    "text": segment_text,
                    "start": segment_start,
                    "end": segment_end,
                    "confidence": segment_confidence,
                    "words": words
                }
                
                # í™˜ê° ê°ì§€
                duration = segment_dict['end'] - segment_dict['start']
                hallucination_info = hallucination_filter.detect_hallucination(segment_dict['text'], duration)
                
                if hallucination_info['is_hallucination']:
                    total_hallucination_count += 1
                    repeated_word = hallucination_info['repeated_word']
                    repeat_count = hallucination_info['repeat_count']
                    repetition_ratio = hallucination_info['repetition_ratio']
                    logger.warning(f"í™˜ê° ê°ì§€: {segment_dict['start']:.1f}s-{segment_dict['end']:.1f}s, ë‹¨ì–´: '{repeated_word}' ({repeat_count}íšŒ, {repetition_ratio:.1%})")
                
                # í™˜ê° í•„í„° ì ìš©
                filtered_segment = hallucination_filter.filter_segment(segment_dict)
                
                # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë§Œ ì „ì²´ í…ìŠ¤íŠ¸ì— í¬í•¨
                if filtered_segment['text'].strip():
                    filtered_text_parts.append(filtered_segment['text'])
                
                # confidence ê°’ì´ Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                segment_confidence_value = filtered_segment.get('confidence')
                if segment_confidence_value is None:
                    segment_confidence_value = 0.8  # ê¸°ë³¸ ì‹ ë¢°ë„
                
                avg_confidence += segment_confidence_value
                segment_infos.append(filtered_segment)
            
            if segments_list:
                avg_confidence /= len(segments_list)
            
            # í•„í„°ë§ëœ ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„± (NeMoì—ì„œ ì´ë¯¸ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ)
            if 'full_text' not in locals():
                full_text = " ".join(filtered_text_parts)
            
            # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… í›„ì²˜ë¦¬ ì‹œë„ (ë°°ì¹˜ ìš”ì²­ì—ì„œ í™œì„±í™”ëœ ê²½ìš°)
            if batch_request and batch_request.enable_keyword_boosting and post_processing_corrector:
                logger.info(f"ğŸ¯ ë°°ì¹˜ í‚¤ì›Œë“œ êµì • ì‹œë„ ì¤‘... call_id: {batch_request.call_id}")
                
                try:
                    # í‚¤ì›Œë“œ ì¤€ë¹„
                    keywords_list = []
                    
                    # 1. call_idê°€ ìˆëŠ” ê²½ìš° ë“±ë¡ëœ í‚¤ì›Œë“œ ì‚¬ìš©
                    if batch_request.call_id:
                        try:
                            active_keywords = await post_processing_corrector.get_keywords(batch_request.call_id)
                            if active_keywords:
                                keywords_list = list(active_keywords.keys())
                                logger.info(f"ğŸ“‹ call_id {batch_request.call_id}ì—ì„œ {len(keywords_list)}ê°œ í‚¤ì›Œë“œ ë¡œë“œë¨")
                        except Exception as e:
                            logger.warning(f"âš ï¸ call_id {batch_request.call_id} í‚¤ì›Œë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    
                    # 2. ì§ì ‘ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ê°€ ì œê³µëœ ê²½ìš°
                    elif batch_request.keywords:
                        keywords_list = [kw.strip() for kw in batch_request.keywords if kw.strip()]
                        logger.info(f"ğŸ“ ì§ì ‘ ì œê³µëœ {len(keywords_list)}ê°œ í‚¤ì›Œë“œ ì‚¬ìš©")
                        
                        # ì„ì‹œ call_idë¡œ í‚¤ì›Œë“œ ë“±ë¡
                        temp_call_id = f"batch_temp_{uuid.uuid4().hex[:8]}"
                        await post_processing_corrector.register_keywords(temp_call_id, keywords_list)
                        logger.info(f"ğŸ”„ ì„ì‹œ call_id {temp_call_id}ì— í‚¤ì›Œë“œ ë“±ë¡ ì™„ë£Œ")
                    
                    # í‚¤ì›Œë“œ êµì • ì ìš©
                    if keywords_list and full_text:
                        call_id_to_use = batch_request.call_id or temp_call_id
                        correction_result = await post_processing_corrector.apply_correction(
                            call_id=call_id_to_use,
                            text=full_text,
                            enable_fuzzy_matching=True,
                            min_similarity=0.8
                        )
                        
                        if correction_result and correction_result.corrected_text != full_text:
                            full_text = correction_result.corrected_text
                            boosting_applied = True
                            keyword_stats = {
                                'registered_keywords': len(keywords_list),
                                'keyword_list': keywords_list,
                                'boosting_applied': True,
                                'boost_factor': batch_request.keyword_boost_factor,
                                'original_text': full_text,
                                'corrected_text': correction_result.corrected_text,
                                'corrections': correction_result.corrections,
                                'detected_keywords': correction_result.keywords_detected
                            }
                            logger.info(f"âœ… ë°°ì¹˜ í‚¤ì›Œë“œ êµì • ì ìš© ì™„ë£Œ")
                            
                            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                            if correction_result.keywords_detected:
                                logger.info(f"ğŸ¯ ê°ì§€ëœ í‚¤ì›Œë“œ: {correction_result.keywords_detected}")
                        else:
                            logger.info(f"â„¹ï¸ í‚¤ì›Œë“œ êµì • ë¶ˆí•„ìš”")
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ ë°°ì¹˜ í‚¤ì›Œë“œ êµì • ì‹¤íŒ¨: {e}")
                
                # ì„ì‹œ í‚¤ì›Œë“œ ì •ë¦¬
                if temp_call_id:
                    try:
                        await post_processing_corrector.delete_keywords(temp_call_id)
                        logger.info(f"ğŸ§¹ ì„ì‹œ í‚¤ì›Œë“œ ì •ë¦¬ ì™„ë£Œ: {temp_call_id}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ ì„ì‹œ í‚¤ì›Œë“œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            if total_hallucination_count > 0:
                logger.info(f"ğŸ” ì´ {total_hallucination_count}ê°œ í™˜ê° êµ¬ê°„ ê°ì§€ ë° ì²˜ë¦¬ë¨")
            
            # ì²˜ë¦¬ ì‹œê°„ê³¼ ì˜¤ë””ì˜¤ ê¸¸ì´ (NeMoì—ì„œ ì´ë¯¸ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ)
            if 'processing_time' not in locals():
                processing_time = time.time() - start_time
            if 'audio_duration' not in locals():
                audio_duration = len(audio_np) / 16000
            
            response_data = {
                "text": full_text,
                "language": info.language,
                "confidence": avg_confidence,
                "processing_time": processing_time,
                "audio_duration": audio_duration,
                "segments": segment_infos,
                "gpu_optimized": True,
                "boosting_applied": boosting_applied
            }
            
            if boosting_applied and keyword_stats:
                response_data['keyword_stats'] = keyword_stats
                
            return response_data
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì „ì‚¬ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"STT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    def _create_timestamped_text(self, file_info: BatchFileInfo) -> str:
        """íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            logger.info(f"íƒ€ì„ìŠ¤íƒ¬í”„ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘ - íŒŒì¼: {file_info.filename}")
            logger.info(f"segments ì •ë³´: {len(file_info.segments) if file_info.segments else 0}ê°œ")
            logger.info(f"segments íƒ€ì…: {type(file_info.segments)}")
            
            if not file_info.segments:
                logger.warning(f"segments ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤ - íŒŒì¼: {file_info.filename}")
                # segmentsê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
                return f"íŒŒì¼ëª…: {file_info.filename}\nì „ì²´ ê¸¸ì´: {file_info.duration_seconds:.2f}ì´ˆ\nì²˜ë¦¬ ì‹œê°„: {file_info.processing_time_seconds:.2f}ì´ˆ\n\nì „ì²´ í…ìŠ¤íŠ¸:\n{file_info.text}"
            
            timestamped_lines = []
            timestamped_lines.append(f"íŒŒì¼ëª…: {file_info.filename}")
            timestamped_lines.append(f"ì „ì²´ ê¸¸ì´: {file_info.duration_seconds:.2f}ì´ˆ")
            timestamped_lines.append(f"ì²˜ë¦¬ ì‹œê°„: {file_info.processing_time_seconds:.2f}ì´ˆ")
            timestamped_lines.append("=" * 60)
            timestamped_lines.append("")
            
            logger.info(f"ì²˜ë¦¬í•  segments ìˆ˜: {len(file_info.segments)}")
            
            for i, segment in enumerate(file_info.segments):
                # segmentê°€ dictì¸ì§€ í™•ì¸
                if isinstance(segment, dict):
                    start_time = segment.get('start', 0.0)
                    end_time = segment.get('end', 0.0)
                    text = segment.get('text', '').strip()
                    confidence = segment.get('confidence', 0.0)
                else:
                    # Pydantic ëª¨ë¸ì¸ ê²½ìš°
                    start_time = getattr(segment, 'start', 0.0)
                    end_time = getattr(segment, 'end', 0.0)
                    text = getattr(segment, 'text', '').strip()
                    confidence = getattr(segment, 'confidence', 0.0)
                
                # ë””ë²„ê¹…ìš© ë¡œê·¸ (ì²˜ìŒ ëª‡ ê°œë§Œ)
                if i < 3:
                    logger.info(f"Segment {i}: start={start_time}, end={end_time}, confidence={confidence}, text='{text[:50]}...'")
                
                # ì‹œê°„ì„ MM:SS.sss í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                start_minutes = int(start_time // 60)
                start_seconds = start_time % 60
                end_minutes = int(end_time // 60)
                end_seconds = end_time % 60
                
                timestamp_str = f"[{start_minutes:02d}:{start_seconds:06.3f} â†’ {end_minutes:02d}:{end_seconds:06.3f}]"
                
                # ì‹ ë¢°ë„ í‘œì‹œ (ì‹¤ì œ ê°’ìœ¼ë¡œ í‘œì‹œ, Noneì´ê±°ë‚˜ 0ì´ë©´ í‘œì‹œí•˜ì§€ ì•ŠìŒ)
                if confidence is not None and confidence > 0:
                    confidence_str = f"(ì‹ ë¢°ë„: {confidence:.3f})"
                else:
                    confidence_str = ""
                
                timestamped_lines.append(f"{timestamp_str} {text} {confidence_str}")
            
            timestamped_lines.append("")
            timestamped_lines.append("=" * 60)
            timestamped_lines.append("ì „ì²´ í…ìŠ¤íŠ¸:")
            timestamped_lines.append(file_info.text)
            
            result = "\n".join(timestamped_lines)
            logger.info(f"íƒ€ì„ìŠ¤íƒ¬í”„ í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ - íŒŒì¼: {file_info.filename}")
            return result
            
        except Exception as e:
            logger.error(f"íƒ€ì„ìŠ¤íƒ¬í”„ í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
            return f"íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± ì‹¤íŒ¨ (ì˜¤ë¥˜: {str(e)})\n\nì „ì²´ í…ìŠ¤íŠ¸:\n{file_info.text}"

    async def _create_result_package(self, batch_job: BatchJob) -> str:
        """ê²°ê³¼ íŒ¨í‚¤ì§€ ìƒì„± (ZIP íŒŒì¼)"""
        zip_filename = f"batch_stt_results_{batch_job.batch_id}.zip"
        zip_path = os.path.join(batch_job.temp_dir, zip_filename)
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # ê²°ê³¼ JSON íŒŒì¼ ìƒì„±
            results_json = {
                "batch_id": batch_job.batch_id,
                "total_files": batch_job.total_files,
                "processed_files": batch_job.processed_files,
                "failed_files": batch_job.failed_files,
                "total_duration": batch_job.total_duration,
                "total_processing_time": batch_job.total_processing_time,
                "created_at": batch_job.created_at.isoformat(),
                "completed_at": batch_job.completed_at.isoformat() if batch_job.completed_at else None,
                "files": [self._file_info_to_dict(file_info) for file_info in batch_job.result_files]
            }
            
            # JSON íŒŒì¼ì„ ZIPì— ì¶”ê°€
            json_content = json.dumps(results_json, ensure_ascii=False, indent=2)
            zipf.writestr("batch_results.json", json_content)
            
            # ê° íŒŒì¼ë³„ í…ìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
            for file_info in batch_job.result_files:
                base_filename = os.path.splitext(file_info.filename)[0]
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ìƒì„¸ í…ìŠ¤íŠ¸ íŒŒì¼
                timestamped_text = self._create_timestamped_text(file_info)
                zipf.writestr(f"transcripts/{base_filename}.txt", timestamped_text)
                
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ë§Œ í¬í•¨ëœ íŒŒì¼ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
                zipf.writestr(f"transcripts/{base_filename}_plain.txt", file_info.text)
        
        return zip_path
    
    def _file_info_to_dict(self, file_info: BatchFileInfo) -> dict:
        """BatchFileInfoë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        try:
            # ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ë³€í™˜
            result = {
                "filename": getattr(file_info, 'filename', 'unknown'),
                "size_bytes": getattr(file_info, 'size_bytes', 0),
                "duration_seconds": getattr(file_info, 'duration_seconds', 0.0),
                "processing_time_seconds": getattr(file_info, 'processing_time_seconds', 0.0),
                "text": getattr(file_info, 'text', ''),
                "language": getattr(file_info, 'language', 'ko'),
                "confidence": getattr(file_info, 'confidence', 0.0),
                "segments": []
            }
            
            # segments ì•ˆì „í•œ ë³€í™˜
            segments = getattr(file_info, 'segments', None)
            if segments:
                safe_segments = []
                for segment in segments:
                    if isinstance(segment, dict):
                        # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                        safe_segments.append(segment)
                    else:
                        # ê°ì²´ì¸ ê²½ìš° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                        segment_dict = {
                            "id": getattr(segment, 'id', 0),
                            "text": getattr(segment, 'text', ''),
                            "start": getattr(segment, 'start', 0.0),
                            "end": getattr(segment, 'end', 0.0),
                            "confidence": getattr(segment, 'confidence', 0.0),
                            "words": []
                        }
                        
                        # words ì•ˆì „í•œ ë³€í™˜
                        words = getattr(segment, 'words', None)
                        if words:
                            safe_words = []
                            for word in words:
                                if isinstance(word, dict):
                                    safe_words.append(word)
                                else:
                                    word_dict = {
                                        "word": getattr(word, 'word', ''),
                                        "start": getattr(word, 'start', 0.0),
                                        "end": getattr(word, 'end', 0.0),
                                        "confidence": getattr(word, 'confidence', 0.0)
                                    }
                                    safe_words.append(word_dict)
                            segment_dict["words"] = safe_words
                        
                        safe_segments.append(segment_dict)
                
                result["segments"] = safe_segments
            
            return result
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì •ë³´ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "filename": str(file_info) if file_info else 'unknown',
                "size_bytes": 0,
                "duration_seconds": 0.0,
                "processing_time_seconds": 0.0,
                "text": '',
                "language": 'ko',
                "confidence": 0.0,
                "segments": []
            }
    
    def get_batch_status(self, batch_id: str) -> Optional[BatchJob]:
        """ë°°ì¹˜ ìƒíƒœ ì¡°íšŒ"""
        return self.batch_jobs.get(batch_id)
    
    def get_batch_progress(self, batch_id: str) -> float:
        """ë°°ì¹˜ ì§„í–‰ë¥  ì¡°íšŒ"""
        batch_job = self.batch_jobs.get(batch_id)
        if not batch_job or batch_job.total_files == 0:
            return 0.0
        return batch_job.processed_files / batch_job.total_files
    
    async def cancel_batch(self, batch_id: str) -> bool:
        """ë°°ì¹˜ ì·¨ì†Œ"""
        batch_job = self.batch_jobs.get(batch_id)
        if not batch_job:
            return False
        
        if batch_job.status == "processing":
            batch_job.status = "cancelled"
            batch_job.completed_at = datetime.now()
            return True
        
        return False
    
    async def cleanup_old_batches(self, max_age_hours: int = 24):
        """ì˜¤ë˜ëœ ë°°ì¹˜ ì •ë¦¬"""
        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
        
        to_remove = []
        for batch_id, batch_job in self.batch_jobs.items():
            if batch_job.completed_at and batch_job.completed_at < cutoff_time:
                # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ
                if batch_job.temp_dir and os.path.exists(batch_job.temp_dir):
                    shutil.rmtree(batch_job.temp_dir, ignore_errors=True)
                to_remove.append(batch_id)
        
        for batch_id in to_remove:
            del self.batch_jobs[batch_id]
        
        logger.info(f"ì •ë¦¬ëœ ë°°ì¹˜ ìˆ˜: {len(to_remove)}")

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
stt_service: Optional[BaseSTTService] = None
post_processing_corrector = None
batch_processor: Optional[BatchProcessor] = None

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì„ íƒëœ ëª¨ë¸ë¡œ STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
    global stt_service, post_processing_corrector, stt_queue, batch_processor
    try:
        logger.info(f"ğŸš€ {SERVER_MODEL_TYPE.upper()} {SERVER_MODEL_NAME} ëª¨ë¸ë¡œ STT Server ì‹œì‘ ì¤‘...")
        logger.info(f"cuDNN í™œì„±í™” ìƒíƒœ: {torch.backends.cudnn.enabled}")
        logger.info(f"cuDNN ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ: {torch.backends.cudnn.benchmark}")
        logger.info(f"TF32 í™œì„±í™” ìƒíƒœ: {torch.backends.cuda.matmul.allow_tf32}")
        
        if torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            logger.info(f"ğŸ¯ GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"ğŸ¯ GPU ë©”ëª¨ë¦¬: {gpu_props.total_memory / 1024**3:.1f}GB")
            logger.info(f"ğŸ¯ CUDA ë²„ì „: {torch.version.cuda}")
            logger.info(f"ğŸ¯ PyTorch ë²„ì „: {torch.__version__}")
            logger.info(f"ğŸ¯ GPU ì•„í‚¤í…ì²˜: {gpu_props.major}.{gpu_props.minor}")
            logger.info(f"ğŸ¯ ë©€í‹°í”„ë¡œì„¸ì„œ ìˆ˜: {gpu_props.multi_processor_count}")
        
        # ì„ íƒëœ ëª¨ë¸ë¡œ STT ì„œë¹„ìŠ¤ ìƒì„±
        logger.info(f"ğŸ“¦ {SERVER_MODEL_TYPE.upper()} {SERVER_MODEL_NAME} ëª¨ë¸ ë¡œë”© ì¤‘...")
        stt_service = STTServiceFactory.create_service(
            model_type=SERVER_MODEL_TYPE,
            model_name=SERVER_MODEL_NAME,
            device="cuda",
            compute_type="float16"
        )
        
        # ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ì²« ë²ˆì§¸ ìš”ì²­ ì§€ì—° ì œê±°
        start_time = time.time()
        success = await stt_service.initialize()
        load_time = time.time() - start_time
        
        if success:
            logger.info(f"âœ… {SERVER_MODEL_TYPE.upper()} STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸ ë¡œë”© ì‹œê°„: {load_time:.2f}ì´ˆ")
            
            # Whisper ëª¨ë¸ì˜ ê²½ìš° ì›œì—… ìˆ˜í–‰
            if SERVER_MODEL_TYPE == "whisper" and hasattr(stt_service, 'whisper_service'):
                await warmup_large_model(stt_service.whisper_service)
        else:
            logger.error(f"âŒ {SERVER_MODEL_TYPE.upper()} STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise RuntimeError(f"STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {stt_service.initialization_error}")
        
        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
        if torch.cuda.is_available():
            gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
            gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3
            logger.info(f"ğŸ¯ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ - í• ë‹¹: {gpu_memory_allocated:.2f}GB, ì˜ˆì•½: {gpu_memory_reserved:.2f}GB")
        
        # í›„ì²˜ë¦¬ í‚¤ì›Œë“œ êµì • ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logger.info("ğŸ”§ í›„ì²˜ë¦¬ í‚¤ì›Œë“œ êµì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        try:
            post_processing_corrector = get_post_processing_corrector()
            logger.info("âœ… í›„ì²˜ë¦¬ í‚¤ì›Œë“œ êµì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info("   - ì •í™• ë§¤ì¹­ êµì • í™œì„±í™”")
            logger.info("   - í¼ì§€ ë§¤ì¹­ êµì • í™œì„±í™”")
            logger.info("   - í•œêµ­ì–´ íŠ¹í™” êµì • í™œì„±í™”")
        except Exception as e:
            logger.error(f"âŒ í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            post_processing_corrector = None
        
        # ğŸ”„ ì§€ëŠ¥í˜• íì‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logger.info("ğŸ”„ ì§€ëŠ¥í˜• STT íì‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        try:
            stt_queue = IntelligentSTTQueue(
                max_concurrent=8,  # ë™ì‹œ ì²˜ë¦¬ ìµœëŒ€ 8ê°œ (í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜)
                max_queue_size=50,  # ìµœëŒ€ í í¬ê¸° 50ê°œ
                default_timeout=60,  # ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ 60ì´ˆ
                priority_timeout=30  # ìš°ì„ ìˆœìœ„ ìš”ì²­ íƒ€ì„ì•„ì›ƒ 30ì´ˆ
            )
            
            await stt_queue.start()
            
            # ë°±ê·¸ë¼ìš´ë“œ ìš”ì²­ ì²˜ë¦¬ ì‹œì‘
            asyncio.create_task(stt_queue.process_requests(stt_service))
            
            logger.info("âœ… ì§€ëŠ¥í˜• íì‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"   ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬: {stt_queue.max_concurrent}ê°œ")
            logger.info(f"   ìµœëŒ€ í í¬ê¸°: {stt_queue.max_queue_size}ê°œ")
            
        except Exception as e:
            logger.error(f"âŒ íì‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            stt_queue = None
        
        # ğŸ—‚ï¸ ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        logger.info("ğŸ—‚ï¸ ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì¤‘...")
        try:
            batch_processor = BatchProcessor()
            logger.info("âœ… ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"   ë°°ì¹˜ ì„ì‹œ ë””ë ‰í† ë¦¬: {batch_processor.batch_temp_dir}")
            logger.info(f"   ìµœëŒ€ ë™ì‹œ ë°°ì¹˜ ì²˜ë¦¬: {batch_processor.max_concurrent_batches}ê°œ")
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            batch_processor = None
        
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ íì‰ ì‹œìŠ¤í…œ ì •ë¦¬"""
    global stt_queue
    try:
        if stt_queue:
            logger.info("ğŸ›‘ íì‰ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
            await stt_queue.stop()
            logger.info("âœ… íì‰ ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ íì‰ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì •ë³´ í¬í•¨"""
    gpu_features = {}
    if torch.cuda.is_available():
        gpu_props = torch.cuda.get_device_properties(0)
        gpu_features = {
            "gpu_name": torch.cuda.get_device_name(0),
            "total_memory_gb": f"{gpu_props.total_memory / 1024**3:.1f}",
            "architecture": f"{gpu_props.major}.{gpu_props.minor}"
        }
    
    return {
        "message": f"{SERVER_MODEL_TYPE.upper()} {SERVER_MODEL_NAME} STT API Server", 
        "model_type": SERVER_MODEL_TYPE,
        "model_name": SERVER_MODEL_NAME,
        "optimization": "extreme" if SERVER_MODEL_TYPE == "whisper" else "optimized",
        "status": "running",
        "supported_models": STTServiceFactory.get_supported_models(),
        "features": {
            "model_type": SERVER_MODEL_TYPE,
            "model_name": SERVER_MODEL_NAME,
            "compute_type": "float16" if SERVER_MODEL_TYPE == "whisper" else "auto",
            "memory_fraction": 0.95 if SERVER_MODEL_TYPE == "whisper" else 0.8,
            "cudnn_enabled": torch.backends.cudnn.enabled,
            "cudnn_benchmark": torch.backends.cudnn.benchmark,
            "tf32_enabled": torch.backends.cuda.matmul.allow_tf32,
            "gpu_available": torch.cuda.is_available(),
            "nemo_available": STTServiceFactory.is_nemo_available(),
            **gpu_features
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ (Large-v3 ìµœì í™” ì •ë³´ í¬í•¨)"""
    model_loaded = False
    gpu_info = {}
    optimization_status = {}
    
    if stt_service is not None:
        model_loaded = hasattr(stt_service, 'model') and stt_service.model is not None
    
    if torch.cuda.is_available():
        gpu_props = torch.cuda.get_device_properties(0)
        gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
        gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3
        
        gpu_info = {
            "device_name": torch.cuda.get_device_name(0),
            "total_memory_gb": f"{gpu_props.total_memory / 1024**3:.1f}",
            "allocated_memory_gb": f"{gpu_memory_allocated:.2f}",
            "reserved_memory_gb": f"{gpu_memory_reserved:.2f}",
            "memory_utilization": f"{gpu_memory_allocated / (gpu_props.total_memory / 1024**3) * 100:.1f}%",
            "architecture": f"{gpu_props.major}.{gpu_props.minor}",
            "multiprocessor_count": gpu_props.multi_processor_count
        }
        
        optimization_status = {
            "model_type": "large-v3",
            "compute_type": "float16",
            "memory_fraction": "0.95",
            "cudnn_benchmark": torch.backends.cudnn.benchmark,
            "tf32_enabled": torch.backends.cuda.matmul.allow_tf32,
            "flash_attention": "enabled"
        }
        
    return HealthResponse(
        status="healthy" if model_loaded else "loading",
        gpu_available=torch.cuda.is_available(),
        model_loaded=model_loaded,
        cudnn_enabled=torch.backends.cudnn.enabled,
        gpu_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
        gpu_info=gpu_info,
        optimization_status=optimization_status
    )

@app.get("/models/info")
async def get_model_info():
    """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì˜ ìƒì„¸ ì •ë³´"""
    if stt_service is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    return {
        "current_model": stt_service.get_model_info(),
        "supported_models": STTServiceFactory.get_supported_models(),
        "server_config": {
            "model_type": SERVER_MODEL_TYPE,
            "model_name": SERVER_MODEL_NAME,
            "nemo_available": STTServiceFactory.is_nemo_available(),
            "available_model_types": STTServiceFactory.get_available_model_types()
        },
        "stats": stt_service.get_stats()
    }

@app.post("/infer/utterance", response_model=TranscriptionResponse) 
async def transcribe_utterance(request: TranscriptionRequest):
    """
    ì‹ ë¢°ë„ ì •ë³´ í¬í•¨í•œ ìƒì„¸ ì „ì‚¬ ì—”ë“œí¬ì¸íŠ¸
    
    Large-v3 ì „ìš© ê·¹í•œ ìµœì í™”ì™€ í•¨ê»˜ ì‹ ë¢°ë„ ì ìˆ˜, ì„¸ê·¸ë¨¼íŠ¸, ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    ìë™ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…ì´ ì ìš©ë©ë‹ˆë‹¤.
    """
    if stt_service is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        request_id = str(uuid.uuid4())
        
        logger.info(f"ğŸ¯ Large-v3 ì‹ ë¢°ë„ ì „ì‚¬ ìš”ì²­ {request_id} ì‹œì‘ (ì–¸ì–´: {request.language})")
        
        # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì²´í¬ ë° ì ìš©
        boosting_applied = False
        boosted_text = None
        keyword_stats = {}
        
        try:
            # ì „ì²´ í‚¤ì›Œë“œ í†µê³„ í™•ì¸
            logger.info("ğŸ” í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ìƒíƒœ í™•ì¸ ì¤‘...")
            global_stats = await keyword_service.get_global_statistics()
            total_keywords = global_stats.get('total_keywords', 0)
            active_keywords = global_stats.get('active_keywords', 0)
            
            logger.info(f"ğŸ”‘ í‚¤ì›Œë“œ ìƒíƒœ: ì´ {total_keywords}ê°œ, í™œì„± {active_keywords}ê°œ")
            
            if active_keywords > 0:
                logger.info("ğŸš€ í™œì„± í‚¤ì›Œë“œ ë°œê²¬! í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ì‹œë„...")
                
                # ëª¨ë“  í™œì„± í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
                all_keywords = await keyword_service.get_all_active_keywords()
                if all_keywords:
                    keywords_list = list(all_keywords.keys())
                    logger.info(f"ğŸ¯ ë°œê²¬ëœ í‚¤ì›Œë“œ: {keywords_list}")
                    
                    # ì„ì‹œ call_id ìƒì„±
                    temp_call_id = f"auto_boost_{request_id}"
                    
                    # í‚¤ì›Œë“œë¥¼ ì„ì‹œë¡œ ë“±ë¡ (ê¸°ì¡´ í‚¤ì›Œë“œê°€ ìë™ìœ¼ë¡œ ì ìš©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
                    try:
                        from src.api.keyword_models import KeywordEntry, KeywordRegistrationRequest
                        
                        temp_keywords = []
                        for keyword, details in all_keywords.items():
                            temp_keywords.append(KeywordEntry(
                                keyword=keyword,
                                boost_factor=details.get('boost_factor', 2.0),
                                category=details.get('category', 'custom'),
                                confidence_threshold=details.get('confidence_threshold', 0.3),
                                aliases=details.get('aliases', []),
                                enabled=True
                            ))
                        
                        temp_request = KeywordRegistrationRequest(
                            call_id=temp_call_id,
                            keywords=temp_keywords,
                            global_boost_factor=2.0,
                            replace_existing=True
                        )
                        
                        # ì„ì‹œ í‚¤ì›Œë“œ ë“±ë¡
                        await keyword_service.register_keywords(temp_request)
                        logger.info(f"âœ… ì„ì‹œ í‚¤ì›Œë“œ ë“±ë¡ ì™„ë£Œ: {temp_call_id}")
                        
                        # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ì‹œë„ (initial_prompt ì‚¬ìš©)
                        logger.info("ğŸ™ï¸ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ìƒí™©ì—ì„œ ì „ì‚¬ ì‹¤í–‰...")
                        
                        # í‚¤ì›Œë“œë¥¼ initial_promptë¡œ ë³€í™˜
                        keywords_prompt = ", ".join(keywords_list)
                        initial_prompt = f"ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤: {keywords_prompt}"
                        logger.info(f"ğŸ¯ Initial prompt: '{initial_prompt}'")
                        
                        # Base64 ë””ì½”ë”©í•˜ì—¬ NumPy ë°°ì—´ë¡œ ë³€í™˜
                        try:
                            audio_bytes = base64.b64decode(request.audio_data)
                            audio_array = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
                            
                            # ì•ˆì „í•œ ëª¨ë¸ ì ‘ê·¼ìœ¼ë¡œ initial_prompt ì‚¬ìš©
                            model = None
                            if hasattr(stt_service, 'model') and hasattr(stt_service.model, 'transcribe'):
                                model = stt_service.model
                            elif hasattr(stt_service, 'whisper_service') and hasattr(stt_service.whisper_service, 'model'):
                                model = stt_service.whisper_service.model
                            elif hasattr(stt_service, 'service') and hasattr(stt_service.service, 'model'):
                                model = stt_service.service.model
                            
                            if model and hasattr(model, 'transcribe'):
                                segments, info = model.transcribe(
                                    audio_array,
                                    beam_size=5,
                                    best_of=5,
                                    temperature=0.0,
                                    vad_filter=False,
                                    language=request.language,
                                    word_timestamps=True,
                                    initial_prompt=initial_prompt  # í‚¤ì›Œë“œ íŒíŠ¸ ì œê³µ
                                )
                            else:
                                # ëª¨ë¸ ì§ì ‘ ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
                                raise Exception("í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…ì„ ìœ„í•œ ëª¨ë¸ ì§ì ‘ ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
                            
                            segments_list = list(segments)
                            if segments_list:
                                boosted_text = " ".join([segment.text.strip() for segment in segments_list if segment.text.strip()])
                                
                                # STTResult í˜•íƒœë¡œ ë³€í™˜
                                class MockResult:
                                    def __init__(self, text, language, segments, audio_duration):
                                        self.text = text
                                        self.language = language
                                        self.segments = segments
                                        self.audio_duration = len(audio_array) / 16000.0
                                        self.rtf = 0.05  # ì„ì‹œê°’
                                
                                result = MockResult(boosted_text, request.language, segments_list, len(audio_array) / 16000.0)
                                logger.info(f"âœ… í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ê²°ê³¼: '{boosted_text}'")
                            
                        except Exception as e:
                            logger.error(f"âŒ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            # fallback to normal transcription
                            result = await stt_service.transcribe_audio(
                                audio_data=request.audio_data,
                                audio_format=request.audio_format,
                                language=request.language
                            )
                            boosting_applied = False
                        
                        if result and result.text:
                            boosted_text = result.text
                            boosting_applied = True
                            keyword_stats = {
                                'registered_keywords': len(all_keywords),
                                'keyword_list': keywords_list,
                                'boosting_applied': True
                            }
                            logger.info(f"âœ… í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ì„±ê³µ: '{boosted_text}'")
                            
                            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                            matches = []
                            for keyword in keywords_list:
                                if keyword in boosted_text:
                                    matches.append(keyword)
                            
                            if matches:
                                keyword_stats['detected_keywords'] = matches
                                logger.info(f"ğŸ¯ ê°ì§€ëœ í‚¤ì›Œë“œ: {matches}")
                            
                        # ì„ì‹œ í‚¤ì›Œë“œ ì •ë¦¬
                        await keyword_service.delete_keywords(temp_call_id)
                        logger.info(f"ğŸ§¹ ì„ì‹œ í‚¤ì›Œë“œ ì •ë¦¬ ì™„ë£Œ: {temp_call_id}")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ì‹¤íŒ¨: {e}")
                        # ì„ì‹œ í‚¤ì›Œë“œ ì •ë¦¬ ì‹œë„
                        try:
                            await keyword_service.delete_keywords(temp_call_id)
                        except:
                            pass
                else:
                    logger.info("â„¹ï¸ í™œì„± í‚¤ì›Œë“œê°€ ì‹¤ì œë¡œëŠ” ë¹„ì–´ìˆìŒ")
            else:
                logger.info("â„¹ï¸ í™œì„± í‚¤ì›Œë“œ ì—†ìŒ, ì¼ë°˜ ì „ì‚¬ ì§„í–‰")
                
        except Exception as e:
            logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… í™•ì¸ ì‹¤íŒ¨: {e}")
            boosting_applied = False
        
        # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…ì´ ì„±ê³µí•˜ì§€ ì•Šì€ ê²½ìš° ì¼ë°˜ ì „ì‚¬ ìˆ˜í–‰
        if not boosting_applied:
            logger.info("ğŸ™ï¸ ì¼ë°˜ FasterWhisper ì „ì‚¬ ì‹¤í–‰")
            # ì˜¤ë””ì˜¤ ì „ì‚¬ ì‹¤í–‰ (Large-v3 ìµœì í™” íŒŒë¼ë¯¸í„°)
            result = await stt_service.transcribe_audio(
                audio_data=request.audio_data,
                audio_format=request.audio_format,
                language=request.language
            )
        else:
            # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…ëœ ê²°ê³¼ ì‚¬ìš©
            pass
        
        processing_time = time.time() - start_time
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ ê²°ì •
        final_text = boosted_text if boosting_applied else result.text
        
        # ê¸°ë³¸ ì‘ë‹µ ìƒì„± (ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ í¬í•¨)
        response = TranscriptionResponse(
            text=final_text,
            language=result.language,
            rtf=result.rtf,
            processing_time=processing_time,
            audio_duration=result.audio_duration,
            gpu_optimized=True,
            model_load_time=None
        )
        
        # ì„¸ê·¸ë¨¼íŠ¸ì™€ ì‹ ë¢°ë„ ì •ë³´ ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
        segments_list = []
        if hasattr(result, 'segments') and result.segments:
            for i, segment in enumerate(result.segments):
                segment_confidence = None
                words_list = []
                
                # ì‹ ë¢°ë„ ê³„ì‚° (avg_logprobì´ ìˆëŠ” ê²½ìš°)
                if hasattr(segment, 'avg_logprob') or 'avg_logprob' in segment:
                    avg_logprob = getattr(segment, 'avg_logprob', segment.get('avg_logprob', -1.0))
                    segment_confidence = logprob_to_confidence(avg_logprob)
                
                # ë‹¨ì–´ ë ˆë²¨ ì •ë³´ (ìˆëŠ” ê²½ìš°)
                if hasattr(segment, 'words') or 'words' in segment:
                    segment_words = getattr(segment, 'words', segment.get('words', []))
                    if segment_words:
                        for word in segment_words:
                            word_confidence = None
                            if hasattr(word, 'probability') or 'probability' in word:
                                prob = getattr(word, 'probability', word.get('probability'))
                                if prob is not None:
                                    word_confidence = normalize_word_probability(prob)
                            
                            words_list.append(WordSegment(
                                word=getattr(word, 'word', word.get('word', '')),
                                start=getattr(word, 'start', word.get('start', 0.0)),
                                end=getattr(word, 'end', word.get('end', 0.0)),
                                confidence=word_confidence
                            ))
                
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ì¶”ê°€
                segment_info = SegmentInfo(
                    id=i,
                    text=getattr(segment, 'text', segment.get('text', '')).strip(),
                    start=getattr(segment, 'start', segment.get('start', 0.0)),
                    end=getattr(segment, 'end', segment.get('end', result.audio_duration)),
                    confidence=segment_confidence,
                    words=words_list if words_list else None
                )
                
                segments_list.append(segment_info)
        
        response.segments = segments_list if segments_list else None
        
        # ë¡œê·¸ ì¶œë ¥
        boost_status = "í‚¤ì›Œë“œë¶€ìŠ¤íŒ…ì ìš©" if boosting_applied else "ì¼ë°˜ì „ì‚¬"
        logger.info(f"âœ… {boost_status} ì™„ë£Œ {request_id}: RTF={result.rtf:.3f}x, "
                   f"ì²˜ë¦¬ì‹œê°„={processing_time:.3f}ì´ˆ, í…ìŠ¤íŠ¸='{final_text}'")
        
        if boosting_applied and keyword_stats:
            logger.info(f"ğŸ¯ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… í†µê³„: {keyword_stats}")
        
        return response
        
    except Exception as e:
        logger.error(f"âŒ ì‹ ë¢°ë„ ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì „ì‚¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(request: TranscriptionRequest):
    """ì˜¤ë””ì˜¤ ì „ì‚¬ ì—”ë“œí¬ì¸íŠ¸ (JSON)"""
    if stt_service is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        
        # ì˜¤ë””ì˜¤ ì „ì‚¬ ì‹¤í–‰
        result = await stt_service.transcribe_audio(
            audio_data=request.audio_data,
            audio_format=request.audio_format,
            language=request.language
        )
        
        processing_time = time.time() - start_time
        
        return TranscriptionResponse(
            text=result.text,
            language=result.language,
            rtf=result.rtf,
            processing_time=processing_time,
            audio_duration=result.audio_duration,
            gpu_optimized=torch.cuda.is_available() and torch.backends.cudnn.enabled
        )
        
    except Exception as e:
        logger.error(f"ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì „ì‚¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/transcribe/file", response_model=TranscriptionResponse)
async def transcribe_file(
    audio: UploadFile = File(...),
    language: str = Form("ko"),
    vad_filter: bool = Form(False)
):
    """ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬ ì—”ë“œí¬ì¸íŠ¸"""
    if stt_service is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        
        # íŒŒì¼ ì½ê¸°
        audio_bytes = await audio.read()
        
        # ì „ì‚¬ ì‹¤í–‰
        result = await stt_service.transcribe_file_bytes(
            audio_bytes=audio_bytes,
            language=language,
            vad_filter=vad_filter
        )
        
        processing_time = time.time() - start_time
        
        return TranscriptionResponse(
            text=result.text,
            language=result.language,
            rtf=result.rtf,
            processing_time=processing_time,
            audio_duration=result.audio_duration,
            gpu_optimized=torch.cuda.is_available() and torch.backends.cudnn.enabled
        )
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì „ì‚¬ ì‹¤íŒ¨: {str(e)}")

# ============================================================================
# íì‰ ì‹œìŠ¤í…œ API ì—”ë“œí¬ì¸íŠ¸
# ============================================================================

@app.post("/queue/transcribe", response_model=QueuedTranscriptionResponse)
async def queue_transcribe_audio(request: QueuedTranscriptionRequest):
    """ìŒì„± ì¸ì‹ ìš”ì²­ì„ íì— ì œì¶œ"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    try:
        # ìš°ì„ ìˆœìœ„ ë³€í™˜
        priority_map = {
            "high": RequestPriority.HIGH,
            "medium": RequestPriority.MEDIUM,
            "low": RequestPriority.LOW
        }
        priority = priority_map.get(request.priority.lower(), RequestPriority.MEDIUM)
        
        # íì— ìš”ì²­ ì œì¶œ
        request_id = await stt_queue.submit_request(
            audio_data=request.audio_data,
            language=request.language,
            audio_format=request.audio_format,
            client_id=request.client_id,
            priority=priority,
            timeout=request.timeout
        )
        
        # í í†µê³„ ê°€ì ¸ì˜¤ê¸°
        stats = await stt_queue.get_queue_stats()
        estimated_wait_time = (stats.queued_requests / max(1, stats.current_throughput)) * 60 if stats.current_throughput > 0 else stats.queued_requests * 30
        
        return QueuedTranscriptionResponse(
            request_id=request_id,
            status="queued",
            message="Request successfully queued for processing",
            estimated_wait_time=estimated_wait_time,
            queue_position=stats.queued_requests
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ í ìš”ì²­ ì œì¶œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to queue request: {str(e)}")

@app.get("/queue/result/{request_id}")
async def get_queue_result(request_id: str):
    """í ì²˜ë¦¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ëŒ€ê¸°/ì™„ë£Œê¹Œì§€)"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    try:
        result = await stt_queue.get_request_result(request_id)
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ í ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get result: {str(e)}")

@app.get("/queue/status/{request_id}", response_model=QueueStatusResponse)
async def get_queue_status(request_id: str):
    """ìš”ì²­ ìƒíƒœ ì¡°íšŒ"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    try:
        status_info = await stt_queue.get_request_status(request_id)
        return QueueStatusResponse(**status_info)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ í ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")

@app.delete("/queue/cancel/{request_id}")
async def cancel_queue_request(request_id: str):
    """í ìš”ì²­ ì·¨ì†Œ"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    try:
        cancelled = await stt_queue.cancel_request(request_id)
        
        if cancelled:
            return {"message": f"Request {request_id} cancelled successfully", "cancelled": True}
        else:
            return {"message": f"Request {request_id} could not be cancelled (may be processing or completed)", "cancelled": False}
        
    except Exception as e:
        logger.error(f"âŒ í ìš”ì²­ ì·¨ì†Œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to cancel request: {str(e)}")

@app.get("/queue/stats", response_model=QueueStatsResponse)
async def get_queue_stats():
    """í í†µê³„ ì¡°íšŒ"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    try:
        stats = await stt_queue.get_queue_stats()
        
        return QueueStatsResponse(
            total_requests=stats.total_requests,
            queued_requests=stats.queued_requests,
            processing_requests=stats.processing_requests,
            completed_requests=stats.completed_requests,
            failed_requests=stats.failed_requests,
            timeout_requests=stats.timeout_requests,
            average_wait_time=stats.average_wait_time,
            average_processing_time=stats.average_processing_time,
            current_throughput=stats.current_throughput,
            queue_capacity=stt_queue.max_queue_size,
            max_concurrent=stt_queue.max_concurrent
        )
        
    except Exception as e:
        logger.error(f"âŒ í í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get queue stats: {str(e)}")

@app.post("/queue/priority/transcribe", response_model=QueuedTranscriptionResponse)
async def priority_transcribe_audio(request: QueuedTranscriptionRequest):
    """ìš°ì„ ìˆœìœ„ ìŒì„± ì¸ì‹ ìš”ì²­ (ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ íì— ì œì¶œ)"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    # ìš”ì²­ì„ ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ ì„¤ì •
    request.priority = "high"
    
    return await queue_transcribe_audio(request)


# í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/keywords/register")
async def register_keywords(request: KeywordRegistrationRequest):
    """í‚¤ì›Œë“œ ë“±ë¡ API (í›„ì²˜ë¦¬ êµì •ìš©)"""
    if post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        success = await post_processing_corrector.register_keywords(
            call_id=request.call_id,
            keywords=request.keywords
        )
        
        if not success:
            raise HTTPException(status_code=500, detail="í‚¤ì›Œë“œ ë“±ë¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        return {
            "message": "í‚¤ì›Œë“œê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤",
            "call_id": request.call_id,
            "keyword_count": len(request.keywords),
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ë“±ë¡ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ë“±ë¡ ì‹¤íŒ¨: {str(e)}")


@app.post("/keywords/correct", response_model=KeywordCorrectionResponse)
async def correct_keywords(request: KeywordCorrectionRequest):
    """í‚¤ì›Œë“œ êµì • API"""
    if post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        result = await post_processing_corrector.apply_correction(
            call_id=request.call_id,
            text=request.text,
            enable_fuzzy_matching=request.enable_fuzzy_matching,
            min_similarity=request.min_similarity
        )
        
        return KeywordCorrectionResponse(
            original_text=result.original_text,
            corrected_text=result.corrected_text,
            corrections=result.corrections,
            keywords_detected=result.keywords_detected,
            confidence_score=result.confidence_score,
            processing_time=result.processing_time
        )
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ êµì • ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ êµì • ì‹¤íŒ¨: {str(e)}")


@app.get("/keywords/{call_id}")
async def get_keywords(call_id: str):
    """í‚¤ì›Œë“œ ì¡°íšŒ API"""
    if post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        keywords = await post_processing_corrector.get_keywords(call_id)
        if keywords is None:
            raise HTTPException(status_code=404, detail=f"í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {call_id}")
        
        # KeywordEntry ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        keywords_dict = {}
        for keyword, entry in keywords.items():
            keywords_dict[keyword] = {
                "keyword": entry.keyword,
                "aliases": entry.aliases,
                "confidence_threshold": entry.confidence_threshold,
                "category": entry.category,
                "enabled": entry.enabled,
                "created_at": entry.created_at.isoformat()
            }
        
        return {
            "call_id": call_id,
            "keywords": keywords_dict,
            "keyword_count": len(keywords_dict)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.delete("/keywords/{call_id}")
async def delete_keywords(call_id: str):
    """í‚¤ì›Œë“œ ì‚­ì œ API"""
    if post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        success = await post_processing_corrector.delete_keywords(call_id)
        if not success:
            raise HTTPException(status_code=404, detail=f"í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {call_id}")
        
        return {"message": f"í‚¤ì›Œë“œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {call_id}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.get("/keywords/stats", response_model=KeywordStatsResponse)
async def get_keyword_statistics():
    """í‚¤ì›Œë“œ êµì • í†µê³„ API"""
    if post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        stats = post_processing_corrector.get_stats()
        
        return KeywordStatsResponse(
            total_keywords=len(post_processing_corrector.keyword_cache),
            total_corrections=stats.get('total_corrections', 0),
            successful_corrections=stats.get('successful_corrections', 0),
            success_rate=stats.get('success_rate', 0.0),
            avg_processing_time=stats.get('avg_processing_time', 0.0),
            categories={}  # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ëŠ” í•„ìš”ì‹œ ì¶”ê°€
        )
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.post("/transcribe/with-correction/{call_id}", response_model=TranscriptionWithCorrection)
async def transcribe_with_keyword_correction(
    call_id: str,
    audio: UploadFile = File(...),
    language: str = Form("ko"),
    vad_filter: bool = Form(False),
    enable_fuzzy_matching: bool = Form(True),
    min_similarity: float = Form(0.8)
):
    """í‚¤ì›Œë“œ êµì •ì´ ì ìš©ëœ íŒŒì¼ ì „ì‚¬ API"""
    if stt_service is None or post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ ë˜ëŠ” êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        
        # íŒŒì¼ ì½ê¸°
        audio_bytes = await audio.read()
        
        # ê¸°ë³¸ ì „ì‚¬ ì‹¤í–‰
        stt_result = await stt_service.transcribe_file_bytes(
            audio_bytes=audio_bytes,
            language=language,
            vad_filter=vad_filter
        )
        
        # í‚¤ì›Œë“œ êµì • ì ìš©
        correction_result = await post_processing_corrector.apply_correction(
            call_id=call_id,
            text=stt_result.text,
            enable_fuzzy_matching=enable_fuzzy_matching,
            min_similarity=min_similarity
        )
        
        processing_time = time.time() - start_time
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë³€í™˜
        segments = []
        for i, segment in enumerate(stt_result.segments):
            segment_dict = segment if isinstance(segment, dict) else asdict(segment)
            segments.append(SegmentInfo(
                id=segment_dict.get('id', i),
                text=segment_dict.get('text', ''),
                start=segment_dict.get('start', 0.0),
                end=segment_dict.get('end', 0.0),
                confidence=segment_dict.get('confidence')
            ))
        
        return TranscriptionWithCorrection(
            text=stt_result.text,
            corrected_text=correction_result.corrected_text,
            language=stt_result.language,
            segments=segments,
            keyword_correction=KeywordCorrectionResponse(
                original_text=correction_result.original_text,
                corrected_text=correction_result.corrected_text,
                corrections=correction_result.corrections,
                keywords_detected=correction_result.keywords_detected,
                confidence_score=correction_result.confidence_score,
                processing_time=correction_result.processing_time
            ),
            metrics=ProcessingMetrics(
                total_duration=processing_time,
                audio_duration=stt_result.audio_duration,
                rtf=stt_result.rtf,
                inference_time=stt_result.processing_time
            )
        )
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ êµì • ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ êµì • ì „ì‚¬ ì‹¤íŒ¨: {str(e)}")


# ============================================================================
# ë°°ì¹˜ ì²˜ë¦¬ API ì—”ë“œí¬ì¸íŠ¸ë“¤
# ============================================================================

@app.post("/batch/transcribe")
async def batch_transcribe(
    language: str = Form("ko"),
    vad_filter: bool = Form(False),
    enable_word_timestamps: bool = Form(True),
    enable_confidence: bool = Form(True),
    client_id: Optional[str] = Form(None),
    priority: str = Form("medium"),
    # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ê´€ë ¨ ë§¤ê°œë³€ìˆ˜
    call_id: Optional[str] = Form(None),  # í‚¤ì›Œë“œê°€ ë“±ë¡ëœ call_id
    enable_keyword_boosting: bool = Form(False),  # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… í™œì„±í™”
    keywords: Optional[str] = Form(None),  # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    keyword_boost_factor: float = Form(2.0),  # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ê°•ë„
    files: List[UploadFile] = File(...)
):
    """
    ë°°ì¹˜ STT ì²˜ë¦¬ (í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ë° í™˜ê° í•„í„° í¬í•¨)
    
    - files: ì—…ë¡œë“œí•  ì˜¤ë””ì˜¤ íŒŒì¼ë“¤
    - language: ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸ê°’: ko)
    - vad_filter: Voice Activity Detection í•„í„° ì‚¬ìš© ì—¬ë¶€
    - enable_word_timestamps: ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± ì—¬ë¶€
    - enable_confidence: ì‹ ë¢°ë„ ì •ë³´ í¬í•¨ ì—¬ë¶€
    - call_id: ë“±ë¡ëœ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•  call_id
    - enable_keyword_boosting: í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… í™œì„±í™” ì—¬ë¶€
    - keywords: ì§ì ‘ í‚¤ì›Œë“œ ì§€ì • (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”,ê°ì‚¬í•©ë‹ˆë‹¤")
    - keyword_boost_factor: í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ê°•ë„ (1.0-5.0)
    """
    
    if not files:
        raise HTTPException(status_code=400, detail="ìµœì†Œ 1ê°œì˜ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤")
    
    # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
    keywords_list = None
    if keywords:
        keywords_list = [kw.strip() for kw in keywords.split(',') if kw.strip()]
    
    # ë°°ì¹˜ ìš”ì²­ ê°ì²´ ìƒì„±
    batch_request = BatchTranscriptionRequest(
        language=language,
        vad_filter=vad_filter,
        enable_word_timestamps=enable_word_timestamps,
        enable_confidence=enable_confidence,
        client_id=client_id,
        priority=priority,
        call_id=call_id,
        enable_keyword_boosting=enable_keyword_boosting,
        keywords=keywords_list,
        keyword_boost_factor=keyword_boost_factor
    )
    
    logger.info(f"ğŸ¯ ë°°ì¹˜ STT ìš”ì²­ - íŒŒì¼ ìˆ˜: {len(files)}, í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…: {enable_keyword_boosting}, call_id: {call_id}")
    if keywords_list:
        logger.info(f"ğŸ“ ì œê³µëœ í‚¤ì›Œë“œ: {keywords_list}")
    
    # ë°°ì¹˜ ì‘ì—… ì œì¶œ (ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬ë¨)
    batch_id = await batch_processor.submit_batch(files, batch_request)
    
    # ì¦‰ì‹œ 200 ì‘ë‹µê³¼ í•¨ê»˜ batch_id ë°˜í™˜
    return {
        "batch_id": batch_id,
        "status": "processing",
        "message": f"ë°°ì¹˜ ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. {len(files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘",
        "total_files": len(files),
        "processed_files": 0,
        "failed_files": 0,
        "created_at": datetime.now().isoformat(),
        "progress_url": f"/batch/progress/{batch_id}",
        "status_url": f"/batch/status/{batch_id}",
        "result_url": f"/batch/result/{batch_id}"
    }

@app.get("/batch/status/{batch_id}", response_model=BatchStatusResponse)
async def get_batch_status(batch_id: str):
    """ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    if batch_processor is None:
        raise HTTPException(status_code=503, detail="ë°°ì¹˜ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        batch_job = batch_processor.get_batch_status(batch_id)
        if not batch_job:
            raise HTTPException(status_code=404, detail=f"ë°°ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {batch_id}")
        
        progress = batch_processor.get_batch_progress(batch_id)
        
        # ë‚¨ì€ ì‹œê°„ ì¶”ì •
        estimated_remaining_time = None
        if batch_job.status == "processing" and progress > 0:
            elapsed_time = (datetime.now() - batch_job.created_at).total_seconds()
            total_estimated_time = elapsed_time / progress
            estimated_remaining_time = total_estimated_time - elapsed_time
        
        return BatchStatusResponse(
            batch_id=batch_id,
            status=batch_job.status,
            progress=progress,
            total_files=batch_job.total_files,
            processed_files=batch_job.processed_files,
            failed_files=batch_job.failed_files,
            estimated_remaining_time=estimated_remaining_time,
            created_at=batch_job.created_at.isoformat(),
            completed_at=batch_job.completed_at.isoformat() if batch_job.completed_at else None,
            error_message=batch_job.error_message
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/batch/result/{batch_id}", response_model=BatchTranscriptionResponse)
async def get_batch_result(batch_id: str):
    """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ì¡°íšŒ"""
    if batch_processor is None:
        raise HTTPException(status_code=503, detail="ë°°ì¹˜ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        batch_job = batch_processor.get_batch_status(batch_id)
        if not batch_job:
            raise HTTPException(status_code=404, detail=f"ë°°ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {batch_id}")
        
        # ë‹¤ìš´ë¡œë“œ URL ìƒì„± (ë°°ì¹˜ê°€ ì™„ë£Œëœ ê²½ìš°)
        download_url = None
        if batch_job.status == "completed" and batch_job.download_path:
            download_url = f"/batch/download/{batch_id}"
        
        return BatchTranscriptionResponse(
            batch_id=batch_id,
            status=batch_job.status,
            message="ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼" if batch_job.status == "completed" else f"ë°°ì¹˜ ìƒíƒœ: {batch_job.status}",
            total_files=batch_job.total_files,
            processed_files=batch_job.processed_files,
            failed_files=batch_job.failed_files,
            total_duration=batch_job.total_duration,
            total_processing_time=batch_job.total_processing_time,
            created_at=batch_job.created_at.isoformat(),
            download_url=download_url,
            files=batch_job.result_files if batch_job.status == "completed" else None
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/batch/download/{batch_id}")
async def download_batch_result(batch_id: str):
    """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    if batch_processor is None:
        raise HTTPException(status_code=503, detail="ë°°ì¹˜ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        batch_job = batch_processor.get_batch_status(batch_id)
        if not batch_job:
            raise HTTPException(status_code=404, detail=f"ë°°ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {batch_id}")
        
        if batch_job.status != "completed":
            raise HTTPException(status_code=400, detail=f"ë°°ì¹˜ê°€ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ìƒíƒœ: {batch_job.status}")
        
        if not batch_job.download_path or not os.path.exists(batch_job.download_path):
            raise HTTPException(status_code=404, detail="ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‘ë‹µ
        filename = f"batch_stt_results_{batch_id}.zip"
        return FileResponse(
            path=batch_job.download_path,
            filename=filename,
            media_type="application/zip"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

@app.delete("/batch/cancel/{batch_id}")
async def cancel_batch_processing(batch_id: str):
    """ë°°ì¹˜ ì²˜ë¦¬ ì·¨ì†Œ"""
    if batch_processor is None:
        raise HTTPException(status_code=503, detail="ë°°ì¹˜ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        success = await batch_processor.cancel_batch(batch_id)
        
        if success:
            return {"message": f"ë°°ì¹˜ {batch_id}ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤", "cancelled": True}
        else:
            return {"message": f"ë°°ì¹˜ {batch_id}ë¥¼ ì·¨ì†Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì´ë¯¸ ì™„ë£Œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ)", "cancelled": False}
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì·¨ì†Œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ì·¨ì†Œ ì‹¤íŒ¨: {str(e)}")

@app.get("/batch/list")
async def list_batch_jobs():
    """ëª¨ë“  ë°°ì¹˜ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    if batch_processor is None:
        raise HTTPException(status_code=503, detail="ë°°ì¹˜ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        batch_list = []
        for batch_id, batch_job in batch_processor.batch_jobs.items():
            progress = batch_processor.get_batch_progress(batch_id)
            
            batch_info = {
                "batch_id": batch_id,
                "status": batch_job.status,
                "progress": progress,
                "total_files": batch_job.total_files,
                "processed_files": batch_job.processed_files,
                "failed_files": batch_job.failed_files,
                "created_at": batch_job.created_at.isoformat(),
                "completed_at": batch_job.completed_at.isoformat() if batch_job.completed_at else None
            }
            batch_list.append(batch_info)
        
        # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬
        batch_list.sort(key=lambda x: x["created_at"], reverse=True)
        
        return {
            "total_batches": len(batch_list),
            "batches": batch_list
        }
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/batch/cleanup")
async def cleanup_old_batches(max_age_hours: int = 24):
    """ì˜¤ë˜ëœ ë°°ì¹˜ ì‘ì—… ì •ë¦¬"""
    if batch_processor is None:
        raise HTTPException(status_code=503, detail="ë°°ì¹˜ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        await batch_processor.cleanup_old_batches(max_age_hours)
        return {"message": f"{max_age_hours}ì‹œê°„ ì´ìƒ ëœ ë°°ì¹˜ ì‘ì—…ë“¤ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤"}
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")


# ============================================================================
# ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì „ë‹¬ ì‹œìŠ¤í…œ
# ============================================================================

class ProgressNotifier:
    """ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬"""
    
    def __init__(self):
        # WebSocket ì—°ê²° ê´€ë¦¬
        self.websocket_connections: Dict[str, Set[WebSocket]] = defaultdict(set)
        # SSE ì—°ê²° ê´€ë¦¬  
        self.sse_connections: Dict[str, Set] = defaultdict(set)
        # ì§„í–‰ ìƒí™© ì €ì¥
        self.progress_data: Dict[str, Dict] = {}
        # ì—…ë°ì´íŠ¸ ê°„ê²© (ì´ˆ)
        self.update_interval = 5
        
    async def add_websocket_connection(self, batch_id: str, websocket: WebSocket):
        """WebSocket ì—°ê²° ì¶”ê°€"""
        await websocket.accept()
        self.websocket_connections[batch_id].add(websocket)
        logger.info(f"ğŸ“¡ WebSocket ì—°ê²° ì¶”ê°€ - ë°°ì¹˜: {batch_id}")
        
        # í˜„ì¬ ì§„í–‰ ìƒí™© ì¦‰ì‹œ ì „ì†¡
        if batch_id in self.progress_data:
            try:
                await websocket.send_json(self.progress_data[batch_id])
            except Exception as e:
                logger.error(f"âŒ WebSocket ì´ˆê¸° ë°ì´í„° ì „ì†¡ ì‹¤íŒ¨: {e}")
                await self.remove_websocket_connection(batch_id, websocket)
    
    async def remove_websocket_connection(self, batch_id: str, websocket: WebSocket):
        """WebSocket ì—°ê²° ì œê±°"""
        self.websocket_connections[batch_id].discard(websocket)
        if not self.websocket_connections[batch_id]:
            del self.websocket_connections[batch_id]
        logger.info(f"ğŸ“¡ WebSocket ì—°ê²° ì œê±° - ë°°ì¹˜: {batch_id}")
    
    async def add_sse_connection(self, batch_id: str, response_queue: asyncio.Queue):
        """SSE ì—°ê²° ì¶”ê°€"""
        self.sse_connections[batch_id].add(response_queue)
        logger.info(f"ğŸ“¡ SSE ì—°ê²° ì¶”ê°€ - ë°°ì¹˜: {batch_id}")
        
        # í˜„ì¬ ì§„í–‰ ìƒí™© ì¦‰ì‹œ ì „ì†¡
        if batch_id in self.progress_data:
            try:
                await response_queue.put(self.progress_data[batch_id])
            except Exception as e:
                logger.error(f"âŒ SSE ì´ˆê¸° ë°ì´í„° ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    async def remove_sse_connection(self, batch_id: str, response_queue: asyncio.Queue):
        """SSE ì—°ê²° ì œê±°"""
        self.sse_connections[batch_id].discard(response_queue)
        if not self.sse_connections[batch_id]:
            del self.sse_connections[batch_id]
        logger.info(f"ğŸ“¡ SSE ì—°ê²° ì œê±° - ë°°ì¹˜: {batch_id}")
    
    async def update_progress(self, batch_id: str, progress_data: Dict):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ë° ì „ì†¡"""
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        progress_data["timestamp"] = datetime.now().isoformat()
        progress_data["batch_id"] = batch_id
        
        # ì§„í–‰ ìƒí™© ì €ì¥
        self.progress_data[batch_id] = progress_data
        
        # WebSocketìœ¼ë¡œ ì „ì†¡
        await self._send_to_websockets(batch_id, progress_data)
        
        # SSEë¡œ ì „ì†¡
        await self._send_to_sse(batch_id, progress_data)
        
        logger.debug(f"ğŸ“Š ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ - ë°°ì¹˜: {batch_id}, ì§„í–‰ë¥ : {progress_data.get('progress', 0):.1%}")
    
    async def _send_to_websockets(self, batch_id: str, data: Dict):
        """WebSocket ì—°ê²°ë“¤ì— ë°ì´í„° ì „ì†¡"""
        if batch_id not in self.websocket_connections:
            return
            
        dead_connections = set()
        for websocket in self.websocket_connections[batch_id].copy():
            try:
                await websocket.send_json(data)
            except Exception as e:
                logger.error(f"âŒ WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")
                dead_connections.add(websocket)
        
        # ì£½ì€ ì—°ê²° ì œê±°
        for websocket in dead_connections:
            await self.remove_websocket_connection(batch_id, websocket)
    
    async def _send_to_sse(self, batch_id: str, data: Dict):
        """SSE ì—°ê²°ë“¤ì— ë°ì´í„° ì „ì†¡"""
        if batch_id not in self.sse_connections:
            return
            
        dead_connections = set()
        for response_queue in self.sse_connections[batch_id].copy():
            try:
                await response_queue.put(data)
            except Exception as e:
                logger.error(f"âŒ SSE ì „ì†¡ ì‹¤íŒ¨: {e}")
                dead_connections.add(response_queue)
        
        # ì£½ì€ ì—°ê²° ì œê±°
        for response_queue in dead_connections:
            await self.remove_sse_connection(batch_id, response_queue)
    
    async def cleanup_batch(self, batch_id: str):
        """ë°°ì¹˜ ì™„ë£Œ ì‹œ ì—°ê²° ì •ë¦¬"""
        # ìµœì¢… ìƒíƒœ ì „ì†¡
        if batch_id in self.progress_data:
            final_data = self.progress_data[batch_id].copy()
            final_data["status"] = "completed"
            final_data["completed_at"] = datetime.now().isoformat()
            
            await self._send_to_websockets(batch_id, final_data)
            await self._send_to_sse(batch_id, final_data)
        
        # ì—°ê²° ì •ë¦¬
        if batch_id in self.websocket_connections:
            for websocket in self.websocket_connections[batch_id].copy():
                try:
                    await websocket.close()
                except:
                    pass
            del self.websocket_connections[batch_id]
        
        if batch_id in self.sse_connections:
            del self.sse_connections[batch_id]
        
        # ì§„í–‰ ìƒí™© ë°ì´í„° ì •ë¦¬ (1ì‹œê°„ í›„)
        await asyncio.sleep(3600)
        if batch_id in self.progress_data:
            del self.progress_data[batch_id]
        
        logger.info(f"ğŸ§¹ ë°°ì¹˜ {batch_id} ì§„í–‰ ìƒí™© ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")

# ì „ì—­ ì§„í–‰ ìƒí™© ì•Œë¦¬ë¯¸
progress_notifier = ProgressNotifier()

# ============================================================================
# ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì „ë‹¬ ì—”ë“œí¬ì¸íŠ¸ (WebSocket & SSE)
# ============================================================================

@app.websocket("/batch/progress/{batch_id}")
async def websocket_batch_progress(websocket: WebSocket, batch_id: str):
    """WebSocketìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ì „ë‹¬"""
    try:
        await progress_notifier.add_websocket_connection(batch_id, websocket)
        
        # ì—°ê²° ìœ ì§€ ë° ë©”ì‹œì§€ ì²˜ë¦¬
        while True:
            try:
                # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ping ë©”ì‹œì§€ ìˆ˜ì‹  (ì—°ê²° ìœ ì§€ìš©)
                data = await websocket.receive_text()
                
                if data == "ping":
                    await websocket.send_text("pong")
                elif data == "status":
                    # í˜„ì¬ ìƒíƒœ ìš”ì²­ ì‹œ ì¦‰ì‹œ ì „ì†¡
                    if batch_id in progress_notifier.progress_data:
                        await websocket.send_json(progress_notifier.progress_data[batch_id])
                        
            except WebSocketDisconnect:
                logger.info(f"ğŸ“¡ WebSocket ì—°ê²° ì¢…ë£Œ - ë°°ì¹˜: {batch_id}")
                break
                
    except Exception as e:
        logger.error(f"âŒ WebSocket ì˜¤ë¥˜: {e}")
    finally:
        await progress_notifier.remove_websocket_connection(batch_id, websocket)

@app.get("/batch/progress/{batch_id}")
async def sse_batch_progress(batch_id: str):
    """Server-Sent Eventsë¡œ ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ì „ë‹¬"""
    
    async def event_stream():
        response_queue = asyncio.Queue()
        
        try:
            # SSE ì—°ê²° ë“±ë¡
            await progress_notifier.add_sse_connection(batch_id, response_queue)
            
            while True:
                try:
                    # ì§„í–‰ ìƒí™© ë°ì´í„° ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ 30ì´ˆ)
                    data = await asyncio.wait_for(response_queue.get(), timeout=30.0)
                    
                    # SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì „ì†¡
                    json_data = json.dumps(data, ensure_ascii=False)
                    yield f"data: {json_data}\n\n"
                    
                    # ì™„ë£Œë˜ë©´ ì—°ê²° ì¢…ë£Œ
                    if data.get("status") in ["completed", "failed", "cancelled"]:
                        break
                        
                except asyncio.TimeoutError:
                    # ì—°ê²° ìœ ì§€ë¥¼ ìœ„í•œ heartbeat
                    yield f"data: {json.dumps({'heartbeat': True, 'timestamp': datetime.now().isoformat()})}\n\n"
                    
                except Exception as e:
                    logger.error(f"âŒ SSE ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {e}")
                    break
                    
        except Exception as e:
            logger.error(f"âŒ SSE ì—°ê²° ì˜¤ë¥˜: {e}")
        finally:
            await progress_notifier.remove_sse_connection(batch_id, response_queue)
    
    return StreamingResponse(
        event_stream(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control"
        }
    )

# ============================================================================
# ê¸°ì¡´ ë°°ì¹˜ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸ë“¤ (ì´ë¯¸ êµ¬í˜„ë¨)
# ============================================================================

class HallucinationFilter:
    """Whisper í™˜ê° í˜„ìƒ ê°ì§€ ë° í•„í„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.repetition_threshold = 3  # 3íšŒ ì´ìƒ ë°˜ë³µ ì‹œ í™˜ê°ìœ¼ë¡œ íŒë‹¨
        self.min_segment_duration = 5.0  # 5ì´ˆ ì´ìƒ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ ì£¼ì˜
        self.max_repetition_ratio = 0.7  # ë°˜ë³µë¥  70% ì´ìƒ ì‹œ í™˜ê° ì˜ì‹¬
        
    def detect_hallucination(self, text: str, duration: float) -> Dict[str, any]:
        """í™˜ê° í˜„ìƒ ê°ì§€"""
        words = text.strip().split()
        
        if len(words) < 3:
            return {'is_hallucination': False, 'confidence': 1.0}
        
        # ë‹¨ì–´ ë°˜ë³µ ë¶„ì„
        word_counts = Counter(words)
        most_common = word_counts.most_common(1)[0] if word_counts else ('', 0)
        repeated_word, repeat_count = most_common
        
        repetition_ratio = repeat_count / len(words) if len(words) > 0 else 0
        
        # í™˜ê° íŒë‹¨ ê¸°ì¤€
        is_repetitive = repeat_count >= self.repetition_threshold
        is_high_ratio = repetition_ratio >= self.max_repetition_ratio
        is_long_duration = duration >= self.min_segment_duration
        
        # íŠ¹ë³„í•œ ë°˜ë³µ íŒ¨í„´ ê°ì§€ (ì•„ì´ì½˜ì´, ê·¸ëŸ°ë°, ê·¸ë˜ì„œ ë“±)
        suspicious_patterns = ['ì•„ì´ì½˜ì´', 'ê·¸ëŸ°ë°', 'ê·¸ë˜ì„œ', 'ê·¸ë¦¬ê³ ', 'ê·¸ëƒ¥', 'ê·¼ë°']
        is_suspicious_word = repeated_word in suspicious_patterns
        
        is_hallucination = (is_repetitive and is_high_ratio) or (is_suspicious_word and is_repetitive)
        
        confidence_penalty = 0.0
        if is_hallucination:
            confidence_penalty = min(0.8, repetition_ratio * 0.9)  # ìµœëŒ€ 80% ì‹ ë¢°ë„ ê°ì†Œ
        
        return {
            'is_hallucination': is_hallucination,
            'repeated_word': repeated_word,
            'repeat_count': repeat_count,
            'repetition_ratio': repetition_ratio,
            'confidence_penalty': confidence_penalty,
            'original_confidence': 1.0,
            'adjusted_confidence': max(0.1, 1.0 - confidence_penalty)
        }
    
    def filter_segment(self, segment: Dict) -> Dict:
        """ì„¸ê·¸ë¨¼íŠ¸ í•„í„°ë§ ë° ì‹ ë¢°ë„ ì¡°ì •"""
        text = segment.get('text', '').strip()
        start = segment.get('start', 0)
        end = segment.get('end', 0)
        duration = end - start
        
        hallucination_info = self.detect_hallucination(text, duration)
        
        if hallucination_info['is_hallucination']:
            logger.warning(f"í™˜ê° ê°ì§€: {start:.1f}s-{end:.1f}s, ë‹¨ì–´: '{hallucination_info['repeated_word']}' ({hallucination_info['repeat_count']}íšŒ, {hallucination_info['repetition_ratio']:.1%})")
            
            # í™˜ê° êµ¬ê°„ ì²˜ë¦¬ ì˜µì…˜
            # 1. ë¹ˆ í…ìŠ¤íŠ¸ë¡œ ë³€ê²½ (ì™„ì „ ì œê±°)
            # 2. ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë§Œ ë‚¨ê¸°ê¸°
            # 3. ì‹ ë¢°ë„ë§Œ í¬ê²Œ ë‚®ì¶”ê¸°
            
            # ì˜µì…˜ 2: ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë§Œ ë‚¨ê¸°ê¸°
            words = text.split()
            unique_words = []
            word_counts = {}
            
            for word in words:
                word_counts[word] = word_counts.get(word, 0) + 1
                if word_counts[word] <= 2:  # ìµœëŒ€ 2ë²ˆê¹Œì§€ë§Œ í—ˆìš©
                    unique_words.append(word)
            
            filtered_text = ' '.join(unique_words)
            
            # ë„ˆë¬´ ì§§ì•„ì§„ ê²½ìš° ì•„ì˜ˆ ì œê±°
            if len(filtered_text.strip()) < 3:
                filtered_text = ""
            
            segment['text'] = filtered_text
            segment['confidence'] = hallucination_info['adjusted_confidence']
            segment['hallucination_detected'] = True
            segment['original_text'] = text
        else:
            segment['hallucination_detected'] = False
            
        return segment

# GPU ìµœì í™”ëœ STT ì„œë²„ í´ë˜ìŠ¤ì— í™˜ê° í•„í„° ì¶”ê°€
class GPUOptimizedSTTServer:
    def __init__(self):
        # ... existing initialization ...
        self.hallucination_filter = HallucinationFilter()  # í™˜ê° í•„í„° ì¶”ê°€

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="GPU Optimized STT Server with Model Selection")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=8004, help="Port to bind to")
    parser.add_argument("--workers", type=int, default=1, help="Number of workers")
    
    # ëª¨ë¸ ì„ íƒ ì˜µì…˜
    parser.add_argument("--model", 
                       choices=STTServiceFactory.get_available_model_types(),
                       default="whisper",
                       help="STT ëª¨ë¸ íƒ€ì… ì„ íƒ (whisper ë˜ëŠ” nemo)")
    parser.add_argument("--list-models", 
                       action="store_true",
                       help="ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # ì§€ì› ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    if args.list_models:
        print("ğŸ¤– ì§€ì›ë˜ëŠ” STT ëª¨ë¸:")
        supported_models = STTServiceFactory.get_supported_models()
        for model_type, info in supported_models.items():
            if info.get("available", True):
                print(f"\nğŸ“¦ {model_type.upper()} ({info['description']}):")
                for model in info['models']:
                    marker = " (ê¸°ë³¸)" if model == info['default'] else ""
                    print(f"  - {model}{marker}")
            else:
                print(f"\nâŒ {model_type.upper()} - ì‚¬ìš© ë¶ˆê°€ëŠ¥")
        
        if not STTServiceFactory.is_nemo_available():
            print(f"\nâš ï¸  NeMo ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:")
            print(f"   pip install nemo-toolkit[asr] omegaconf hydra-core")
        
        sys.exit(0)
    
    # ëª¨ë¸ ì„¤ì •
    SERVER_MODEL_TYPE = args.model
    SERVER_MODEL_NAME = STTServiceFactory.get_default_model(SERVER_MODEL_TYPE)
    
    # ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦
    if not STTServiceFactory.validate_model(SERVER_MODEL_TYPE, SERVER_MODEL_NAME):
        print(f"âŒ ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜:")
        if SERVER_MODEL_TYPE == "nemo" and not STTServiceFactory.is_nemo_available():
            print("NeMo íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì—¬ ì„¤ì¹˜í•˜ì„¸ìš”:")
            print("pip install nemo-toolkit[asr] omegaconf hydra-core")
        else:
            print(f"ëª¨ë¸ íƒ€ì… '{SERVER_MODEL_TYPE}'ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    # ì„œë²„ ì„¤ì •
    SERVER_HOST = args.host
    SERVER_PORT = args.port
    
    print(f"ğŸš€ STT ì„œë²„ ì‹œì‘:")
    print(f"   ëª¨ë¸: {SERVER_MODEL_TYPE} ({SERVER_MODEL_NAME})")
    print(f"   ì£¼ì†Œ: {SERVER_HOST}:{SERVER_PORT}")
    print(f"   ì›Œì»¤ ìˆ˜: {args.workers}")
    
    # NeMo ê²½ê³  ë©”ì‹œì§€
    if SERVER_MODEL_TYPE == "nemo" and not STTServiceFactory.is_nemo_available():
        print("âŒ NeMo íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„œë²„ ì‹œì‘ì— ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    uvicorn.run(app, host=SERVER_HOST, port=SERVER_PORT, workers=args.workers)



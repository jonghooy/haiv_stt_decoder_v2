#!/usr/bin/env python3
"""
GPU Optimized STT Server with Queueing System
cuDNNì„ ì™„ì „íˆ í™œì„±í™”í•˜ê³  RTX 4090 ìµœì í™”ë¥¼ ì ìš©í•œ STT API ì„œë²„
20ê°œ ì´í•˜ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìœ„í•œ ì§€ëŠ¥í˜• íì‰ ì‹œìŠ¤í…œ í¬í•¨
"""

import sys
import os
sys.path.append('/home/jonghooy/haiv_stt_decoder_v2')

import asyncio
import time
import torch
import torchaudio
import numpy as np
import logging
import base64
import uvicorn
import uuid
from enum import Enum
from dataclasses import dataclass, field, asdict
from typing import Optional, Dict, Any, List, Callable
from datetime import datetime, timedelta
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from contextlib import asynccontextmanager

# GPU ìµœì í™” ì„¤ì • - Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ ìµœì í™”
def setup_extreme_gpu_optimizations():
    """Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ GPU ìµœì í™”"""
    if not torch.cuda.is_available():
        raise RuntimeError("CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
        
    logger.info("ğŸš€ Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ GPU ìµœì í™” ì ìš© ì¤‘...")
    
    # GPU ë©”ëª¨ë¦¬ ì„¤ì • (PyTorch 2.5+ í˜¸í™˜)
    try:
        if hasattr(torch.cuda, 'set_memory_fraction'):
            torch.cuda.set_memory_fraction(0.95)  # GPU ë©”ëª¨ë¦¬ 95% ì‚¬ìš©
            logger.info("âœ… CUDA ë©”ëª¨ë¦¬ fraction ì„¤ì • ì™„ë£Œ")
        else:
            # PyTorch 2.5+ í˜¸í™˜ ë©”ëª¨ë¦¬ ì„¤ì •
            torch.cuda.memory.set_per_process_memory_fraction(0.95)
            logger.info("âœ… CUDA í”„ë¡œì„¸ìŠ¤ë³„ ë©”ëª¨ë¦¬ fraction ì„¤ì • ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ CUDA ë©”ëª¨ë¦¬ fraction ì„¤ì • ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë¦¬ë§Œ ìˆ˜í–‰
        torch.cuda.empty_cache()
        
    torch.backends.cudnn.enabled = True
    torch.backends.cudnn.benchmark = True  # cuDNN ë²¤ì¹˜ë§ˆí¬ í™œì„±í™”
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.matmul.allow_tf32 = True  # TF32 í™œì„±í™”
    torch.backends.cudnn.allow_tf32 = True
    
    # CUDA ë©”ëª¨ë¦¬ í’€ ìµœì í™” (PyTorch 2.5+ í˜¸í™˜)
    try:
        if hasattr(torch.cuda, 'memory') and hasattr(torch.cuda.memory, 'set_memory_pool_limit'):
            torch.cuda.memory.set_memory_pool_limit(0.95)
            logger.info("âœ… CUDA ë©”ëª¨ë¦¬ í’€ ì œí•œ ì„¤ì • ì™„ë£Œ")
        else:
            # PyTorch 2.5+ í˜¸í™˜ ë©”ëª¨ë¦¬ ìµœì í™”
            torch.cuda.empty_cache()
            logger.info("âœ… CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ CUDA ë©”ëª¨ë¦¬ ì„¤ì • ì‹¤íŒ¨: {e}")
        torch.cuda.empty_cache()  # ìµœì†Œí•œ ë©”ëª¨ë¦¬ ì •ë¦¬ëŠ” ìˆ˜í–‰
            
    # Mixed precision í™œì„±í™”
    try:
        torch.backends.cuda.enable_flash_sdp(True)
        logger.info("âœ… Flash Attention SDP í™œì„±í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"âš ï¸ Flash Attention SDP í™œì„±í™” ì‹¤íŒ¨: {e}")
    
    logger.info("âœ… ê·¹í•œ GPU ìµœì í™” ì™„ë£Œ")

async def warmup_large_model(stt_service):
    """Large-v3 ëª¨ë¸ ì›œì—… (ì²« ìš”ì²­ ì§€ì—° ìµœì†Œí™”)"""
    try:
        logger.info("ğŸ”¥ Large-v3 ëª¨ë¸ ì›œì—… ì‹œì‘...")
        
        # ë”ë¯¸ ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„± (1ì´ˆ, 16kHz)
        dummy_audio = np.random.randn(16000).astype(np.float32) * 0.1  # ì‘ì€ ë³¼ë¥¨
        
        # ëª¨ë¸ì˜ ë‚´ë¶€ transcribe ë©”ì„œë“œë¥¼ ì§ì ‘ ì‚¬ìš© (ê°€ì¥ ì•ˆì „í•¨)
        start_time = time.time()
        # STT ì„œë¹„ìŠ¤ì˜ ë‚´ë¶€ ëª¨ë¸ì— ì§ì ‘ ì ‘ê·¼ (Large-v3 ìµœì í™” íŒŒë¼ë¯¸í„°)
        if hasattr(stt_service, 'model') and hasattr(stt_service.model, 'transcribe'):
            # FasterWhisper ëª¨ë¸ì˜ transcribe ë©”ì„œë“œ ì§ì ‘ í˜¸ì¶œ (Large-v3 ìµœì í™”)
            segments, info = stt_service.model.transcribe(
                dummy_audio,
                beam_size=5,  # Large-v3ì— ìµœì í™”ëœ beam_size
                best_of=5,    # Large-v3ì— ìµœì í™”ëœ best_of
                temperature=0.0,
                vad_filter=False,
                language="ko"
            )
            # ê²°ê³¼ ì†Œë¹„
            list(segments)
        else:
            logger.info("ì›œì—…ì„ ìœ„í•œ ì§ì ‘ ëª¨ë¸ ì ‘ê·¼ ë¶ˆê°€, ì›œì—… ê±´ë„ˆëœ€")
            
        warmup_time = time.time() - start_time
        
        logger.info(f"âœ… Large-v3 ëª¨ë¸ ì›œì—… ì™„ë£Œ ({warmup_time:.3f}ì´ˆ)")
        
    except Exception as e:
        logger.warning(f"âš ï¸ ëª¨ë¸ ì›œì—… ì‹¤íŒ¨ (ë¹„ì¤‘ìš”): {e}")
        # ì›œì—… ì‹¤íŒ¨ëŠ” ì„œë²„ ì‹œì‘ì„ ë§‰ì§€ ì•ŠìŒ
        pass

from src.api.stt_service import FasterWhisperSTTService
from src.api.post_processing_correction import (
    get_post_processing_corrector,
    apply_keyword_correction,
    CorrectionResult
)
from src.api.models import (
    KeywordRegistrationRequest,
    KeywordCorrectionRequest,
    KeywordCorrectionResponse,
    TranscriptionWithCorrection,
    KeywordStatsResponse,
    ProcessingMetrics
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ê·¹í•œ GPU ìµœì í™” ì‹¤í–‰ (logger ì •ì˜ í›„)
setup_extreme_gpu_optimizations()

# ============================================================================
# ì§€ëŠ¥í˜• íì‰ ì‹œìŠ¤í…œ êµ¬í˜„
# ============================================================================

class RequestPriority(Enum):
    """ìš”ì²­ ìš°ì„ ìˆœìœ„"""
    HIGH = 1
    MEDIUM = 2
    LOW = 3

class RequestStatus(Enum):
    """ìš”ì²­ ìƒíƒœ"""
    QUEUED = "queued"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"
    CANCELLED = "cancelled"

@dataclass
class QueuedRequest:
    """íì— ë“¤ì–´ê°„ ìš”ì²­"""
    request_id: str
    client_id: str
    audio_data: str
    language: str
    audio_format: str
    priority: RequestPriority
    created_at: datetime
    timeout_at: datetime
    status: RequestStatus = RequestStatus.QUEUED
    processing_started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    
    def __post_init__(self):
        """ìš”ì²­ IDê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±"""
        if not self.request_id:
            self.request_id = str(uuid.uuid4())
    
    def __lt__(self, other):
        """ìš°ì„ ìˆœìœ„ íì—ì„œ ë¹„êµë¥¼ ìœ„í•œ ë©”ì„œë“œ"""
        if not isinstance(other, QueuedRequest):
            return NotImplemented
        # ìš°ì„ ìˆœìœ„ê°€ ê°™ìœ¼ë©´ ìƒì„± ì‹œê°„ìœ¼ë¡œ ë¹„êµ (ë¨¼ì € ìƒì„±ëœ ê²ƒì´ ìš°ì„ )
        if self.priority.value == other.priority.value:
            return self.created_at < other.created_at
        return self.priority.value < other.priority.value

@dataclass
class QueueStats:
    """í í†µê³„"""
    total_requests: int = 0
    queued_requests: int = 0
    processing_requests: int = 0
    completed_requests: int = 0
    failed_requests: int = 0
    timeout_requests: int = 0
    average_wait_time: float = 0.0
    average_processing_time: float = 0.0
    current_throughput: float = 0.0

# Helper functions
def logprob_to_confidence(avg_logprob: float) -> float:
    """
    Whisperì˜ avg_logprobì„ ì‹ ë¢°ë„ ì ìˆ˜(0.0-1.0)ë¡œ ë³€í™˜
    
    Args:
        avg_logprob: Whisper ì„¸ê·¸ë¨¼íŠ¸ì˜ í‰ê·  ë¡œê·¸ í™•ë¥ 
        
    Returns:
        0.0-1.0 ë²”ìœ„ì˜ ì‹ ë¢°ë„ ì ìˆ˜
    """
    if avg_logprob is None:
        return 0.0
    
    # avg_logprobì€ ì¼ë°˜ì ìœ¼ë¡œ -inf ~ 0 ë²”ìœ„
    # -1.0 ì´ìƒì´ë©´ ë†’ì€ ì‹ ë¢°ë„, -3.0 ì´í•˜ë©´ ë‚®ì€ ì‹ ë¢°ë„ë¡œ ê°„ì£¼
    if avg_logprob >= -0.5:
        return 0.95
    elif avg_logprob >= -1.0:
        return 0.8 + (avg_logprob + 1.0) * 0.3  # -1.0~-0.5 -> 0.8~0.95
    elif avg_logprob >= -2.0:
        return 0.5 + (avg_logprob + 2.0) * 0.3  # -2.0~-1.0 -> 0.5~0.8
    elif avg_logprob >= -3.0:
        return 0.2 + (avg_logprob + 3.0) * 0.3  # -3.0~-2.0 -> 0.2~0.5
    else:
        return max(0.1, 0.2 + (avg_logprob + 3.0) * 0.1)  # -3.0 ì´í•˜ -> 0.1~0.2


def normalize_word_probability(probability: float) -> float:
    """
    Whisper ë‹¨ì–´ í™•ë¥ ì„ ì •ê·œí™”ëœ ì‹ ë¢°ë„ë¡œ ë³€í™˜
    
    Args:
        probability: Whisper word probability (ë³´í†µ 0.0-1.0 ë²”ìœ„)
        
    Returns:
        ì •ê·œí™”ëœ ì‹ ë¢°ë„ ì ìˆ˜ (0.0-1.0)
    """
    if probability is None:
        return 0.0
    
    # Whisperì˜ word probabilityëŠ” ì´ë¯¸ 0-1 ë²”ìœ„ì´ì§€ë§Œ,
    # ì‹¤ì œë¡œëŠ” 0.3-1.0 ë²”ìœ„ì—ì„œ ë” ì˜ë¯¸ìˆëŠ” ê°’ë“¤ì´ ë‚˜ì˜´
    if probability >= 0.8:
        return probability  # ë†’ì€ ì‹ ë¢°ë„ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    elif probability >= 0.5:
        return 0.6 + (probability - 0.5) * 0.6  # 0.5-0.8 -> 0.6-0.8
    elif probability >= 0.3:
        return 0.3 + (probability - 0.3) * 1.5  # 0.3-0.5 -> 0.3-0.6
    else:
        return max(0.1, probability * 1.0)  # 0.3 ì´í•˜ -> ìµœì†Œ 0.1


class IntelligentSTTQueue:
    """ì§€ëŠ¥í˜• STT ì²˜ë¦¬ í"""
    
    def __init__(self, 
                 max_concurrent: int = 8,
                 max_queue_size: int = 50,
                 default_timeout: int = 60,
                 priority_timeout: int = 30):
        self.max_concurrent = max_concurrent
        self.max_queue_size = max_queue_size
        self.default_timeout = default_timeout
        self.priority_timeout = priority_timeout
        
        # í ê´€ë¦¬
        self.priority_queue: asyncio.PriorityQueue = asyncio.PriorityQueue()
        self.processing_requests: Dict[str, QueuedRequest] = {}
        self.completed_requests: Dict[str, QueuedRequest] = {}
        self.request_futures: Dict[str, asyncio.Future] = {}
        
        # ë™ì‹œì„± ì œì–´
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.processing_lock = asyncio.Lock()
        
        # í†µê³„ ë° ëª¨ë‹ˆí„°ë§
        self.stats = QueueStats()
        self.start_time = datetime.now()
        self.last_throughput_calc = datetime.now()
        self.recent_completions: List[datetime] = []
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
        self._cleanup_task: Optional[asyncio.Task] = None
        self._monitor_task: Optional[asyncio.Task] = None
        self._running = False
    
    async def start(self):
        """í ì‹œìŠ¤í…œ ì‹œì‘"""
        if self._running:
            return
        
        self._running = True
        self._cleanup_task = asyncio.create_task(self._cleanup_expired_requests())
        self._monitor_task = asyncio.create_task(self._monitor_performance())
        logger.info(f"ğŸ”„ STT í ì‹œìŠ¤í…œ ì‹œì‘ - ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬: {self.max_concurrent}ê°œ")
    
    async def stop(self):
        """í ì‹œìŠ¤í…œ ì¤‘ì§€"""
        self._running = False
        
        if self._cleanup_task:
            self._cleanup_task.cancel()
        if self._monitor_task:
            self._monitor_task.cancel()
        
        # ëª¨ë“  ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ ì·¨ì†Œ
        while not self.priority_queue.empty():
            try:
                _, request = self.priority_queue.get_nowait()
                request.status = RequestStatus.CANCELLED
                if request.request_id in self.request_futures:
                    future = self.request_futures[request.request_id]
                    if not future.done():
                        future.cancel()
            except asyncio.QueueEmpty:
                break
        
        logger.info("ğŸ›‘ STT í ì‹œìŠ¤í…œ ì¤‘ì§€")
    
    async def submit_request(self, 
                           audio_data: str,
                           language: str = "ko",
                           audio_format: str = "pcm_16khz",
                           client_id: str = None,
                           priority: RequestPriority = RequestPriority.MEDIUM,
                           timeout: Optional[int] = None) -> str:
        """ìš”ì²­ì„ íì— ì œì¶œ"""
        
        # í í¬ê¸° í™•ì¸
        current_queue_size = self.priority_queue.qsize()
        if current_queue_size >= self.max_queue_size:
            raise HTTPException(status_code=503, detail=f"Queue is full ({current_queue_size}/{self.max_queue_size})")
        
        # ìš”ì²­ ìƒì„±
        request_id = str(uuid.uuid4())
        client_id = client_id or f"client_{uuid.uuid4().hex[:8]}"
        timeout_seconds = timeout or (self.priority_timeout if priority == RequestPriority.HIGH else self.default_timeout)
        
        request = QueuedRequest(
            request_id=request_id,
            client_id=client_id,
            audio_data=audio_data,
            language=language,
            audio_format=audio_format,
            priority=priority,
            created_at=datetime.now(),
            timeout_at=datetime.now() + timedelta(seconds=timeout_seconds)
        )
        
        # Future ìƒì„±
        future = asyncio.Future()
        self.request_futures[request_id] = future
        
        # ìš°ì„ ìˆœìœ„ íì— ì¶”ê°€ (ìš°ì„ ìˆœìœ„ê°€ ë†’ì„ìˆ˜ë¡ ë¨¼ì € ì²˜ë¦¬)
        await self.priority_queue.put((priority.value, request))
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        async with self.processing_lock:
            self.stats.total_requests += 1
            self.stats.queued_requests += 1
        
        logger.info(f"ğŸ“ ìš”ì²­ í ì¶”ê°€ - ID: {request_id}, í´ë¼ì´ì–¸íŠ¸: {client_id}, ìš°ì„ ìˆœìœ„: {priority.name}, í í¬ê¸°: {current_queue_size + 1}")
        
        return request_id
    
    async def get_request_result(self, request_id: str) -> Dict[str, Any]:
        """ìš”ì²­ ê²°ê³¼ ëŒ€ê¸° ë° ë°˜í™˜"""
        if request_id not in self.request_futures:
            raise HTTPException(status_code=404, detail="Request not found")
        
        future = self.request_futures[request_id]
        
        try:
            # ê²°ê³¼ ëŒ€ê¸°
            result = await future
            
            # ì™„ë£Œëœ ìš”ì²­ ì •ë¦¬
            if request_id in self.request_futures:
                del self.request_futures[request_id]
            
            return result
        
        except asyncio.CancelledError:
            raise HTTPException(status_code=408, detail="Request was cancelled")
        except asyncio.TimeoutError:
            raise HTTPException(status_code=408, detail="Request timed out")
    
    async def get_request_status(self, request_id: str) -> Dict[str, Any]:
        """ìš”ì²­ ìƒíƒœ ì¡°íšŒ"""
        # ì²˜ë¦¬ ì¤‘ì¸ ìš”ì²­ í™•ì¸
        if request_id in self.processing_requests:
            request = self.processing_requests[request_id]
            wait_time = (datetime.now() - request.created_at).total_seconds()
            processing_time = (datetime.now() - request.processing_started_at).total_seconds() if request.processing_started_at else 0
            
            return {
                "request_id": request_id,
                "status": request.status.value,
                "priority": request.priority.name,
                "wait_time": wait_time,
                "processing_time": processing_time,
                "estimated_remaining_time": max(0, request.timeout_at.timestamp() - datetime.now().timestamp())
            }
        
        # ì™„ë£Œëœ ìš”ì²­ í™•ì¸
        if request_id in self.completed_requests:
            request = self.completed_requests[request_id]
            total_time = (request.completed_at - request.created_at).total_seconds()
            
            return {
                "request_id": request_id,
                "status": request.status.value,
                "priority": request.priority.name,
                "total_time": total_time,
                "completed_at": request.completed_at.isoformat(),
                "result": request.result,
                "error_message": request.error_message
            }
        
        # íì—ì„œ ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ í™•ì¸
        queue_position = await self._get_queue_position(request_id)
        if queue_position is not None:
            return {
                "request_id": request_id,
                "status": RequestStatus.QUEUED.value,
                "queue_position": queue_position,
                "estimated_wait_time": self._estimate_wait_time(queue_position)
            }
        
        raise HTTPException(status_code=404, detail="Request not found")
    
    async def cancel_request(self, request_id: str) -> bool:
        """ìš”ì²­ ì·¨ì†Œ"""
        # Futureê°€ ìˆìœ¼ë©´ ì·¨ì†Œ
        if request_id in self.request_futures:
            future = self.request_futures[request_id]
            if not future.done():
                future.cancel()
                del self.request_futures[request_id]
                return True
        
        # ì²˜ë¦¬ ì¤‘ì¸ ìš”ì²­ì€ ì·¨ì†Œí•  ìˆ˜ ì—†ìŒ
        if request_id in self.processing_requests:
            return False
        
        return False
    
    async def get_queue_stats(self) -> QueueStats:
        """í í†µê³„ ë°˜í™˜"""
        async with self.processing_lock:
            # ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸
            self.stats.queued_requests = self.priority_queue.qsize()
            self.stats.processing_requests = len(self.processing_requests)
            
            # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
            now = datetime.now()
            time_diff = (now - self.last_throughput_calc).total_seconds()
            if time_diff >= 10:  # 10ì´ˆë§ˆë‹¤ ì²˜ë¦¬ëŸ‰ ì—…ë°ì´íŠ¸
                recent_count = len([t for t in self.recent_completions if (now - t).total_seconds() <= 60])
                self.stats.current_throughput = recent_count / 60.0  # ë¶„ë‹¹ ì²˜ë¦¬ëŸ‰
                self.last_throughput_calc = now
            
            return self.stats
    
    async def process_requests(self, stt_service):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìš”ì²­ ì²˜ë¦¬"""
        while self._running:
            try:
                # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì œí•œ
                async with self.semaphore:
                    # íì—ì„œ ìš”ì²­ ê°€ì ¸ì˜¤ê¸°
                    try:
                        _, request = await asyncio.wait_for(self.priority_queue.get(), timeout=1.0)
                    except asyncio.TimeoutError:
                        continue
                    
                    # íƒ€ì„ì•„ì›ƒ í™•ì¸
                    if datetime.now() > request.timeout_at:
                        request.status = RequestStatus.TIMEOUT
                        await self._complete_request(request, None, "Request timed out")
                        continue
                    
                    # ì²˜ë¦¬ ì‹œì‘
                    request.status = RequestStatus.PROCESSING
                    request.processing_started_at = datetime.now()
                    
                    async with self.processing_lock:
                        self.processing_requests[request.request_id] = request
                        self.stats.queued_requests = max(0, self.stats.queued_requests - 1)
                        self.stats.processing_requests += 1
                    
                    logger.info(f"ğŸ”„ ìš”ì²­ ì²˜ë¦¬ ì‹œì‘ - ID: {request.request_id}")
                    
                    # ì‹¤ì œ STT ì²˜ë¦¬
                    try:
                        result = await self._process_stt_request(request, stt_service)
                        await self._complete_request(request, result, None)
                        
                    except Exception as e:
                        error_msg = str(e)
                        logger.error(f"âŒ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨ - ID: {request.request_id}, ì˜¤ë¥˜: {error_msg}")
                        await self._complete_request(request, None, error_msg)
            
            except Exception as e:
                logger.error(f"âŒ í ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def _process_stt_request(self, request: QueuedRequest, stt_service) -> Dict[str, Any]:
        """ì‹¤ì œ STT ì²˜ë¦¬"""
        start_time = time.time()
        
        # Base64 ë””ì½”ë”©
        audio_bytes = base64.b64decode(request.audio_data)
        
        # STT ì²˜ë¦¬ - ì˜¬ë°”ë¥¸ ë©”ì„œë“œëª… ì‚¬ìš©
        result = await stt_service.transcribe_file_bytes(
            audio_bytes, 
            language=request.language
        )
        
        processing_time = time.time() - start_time
        
        # STTResult ê°ì²´ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        return {
            "text": result.text,
            "language": result.language,
            "rtf": result.rtf,
            "processing_time": processing_time,
            "audio_duration": result.audio_duration,
            "gpu_optimized": True,
            "queue_processed": True,
            "request_id": request.request_id,
            "client_id": request.client_id,
            "segments": result.segments
        }
    
    async def _complete_request(self, request: QueuedRequest, result: Optional[Dict[str, Any]], error_message: Optional[str]):
        """ìš”ì²­ ì™„ë£Œ ì²˜ë¦¬"""
        request.completed_at = datetime.now()
        request.result = result
        request.error_message = error_message
        request.status = RequestStatus.COMPLETED if result else RequestStatus.FAILED
        
        # ì²˜ë¦¬ ì¤‘ ëª©ë¡ì—ì„œ ì œê±°
        async with self.processing_lock:
            if request.request_id in self.processing_requests:
                del self.processing_requests[request.request_id]
                self.stats.processing_requests = max(0, self.stats.processing_requests - 1)
        
        # ì™„ë£Œ ëª©ë¡ì— ì¶”ê°€
        self.completed_requests[request.request_id] = request
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        if result:
            self.stats.completed_requests += 1
            self.recent_completions.append(datetime.now())
        else:
            self.stats.failed_requests += 1
        
        # Future ì™„ë£Œ
        if request.request_id in self.request_futures:
            future = self.request_futures[request.request_id]
            if not future.done():
                if result:
                    future.set_result(result)
                else:
                    future.set_exception(Exception(error_message or "Unknown error"))
        
        logger.info(f"âœ… ìš”ì²­ ì™„ë£Œ - ID: {request.request_id}, ìƒíƒœ: {request.status.value}")
    
    async def _cleanup_expired_requests(self):
        """ë§Œë£Œëœ ìš”ì²­ ì •ë¦¬"""
        while self._running:
            try:
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì •ë¦¬
                
                now = datetime.now()
                expired_ids = []
                
                # ì™„ë£Œëœ ìš”ì²­ ì¤‘ 1ì‹œê°„ ì´ìƒ ëœ ê²ƒ ì œê±°
                for request_id, request in list(self.completed_requests.items()):
                    if (now - request.completed_at).total_seconds() > 3600:  # 1ì‹œê°„
                        expired_ids.append(request_id)
                
                for request_id in expired_ids:
                    del self.completed_requests[request_id]
                
                # ìµœê·¼ ì™„ë£Œ ê¸°ë¡ ì •ë¦¬ (1ì‹œê°„ ì´ìƒ)
                self.recent_completions = [t for t in self.recent_completions if (now - t).total_seconds() <= 3600]
                
                if expired_ids:
                    logger.info(f"ğŸ§¹ ë§Œë£Œëœ ìš”ì²­ {len(expired_ids)}ê°œ ì •ë¦¬ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ ìš”ì²­ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _monitor_performance(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        while self._running:
            try:
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§
                
                stats = await self.get_queue_stats()
                logger.info(f"ğŸ“Š í ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§:")
                logger.info(f"   ëŒ€ê¸° ì¤‘: {stats.queued_requests}ê°œ")
                logger.info(f"   ì²˜ë¦¬ ì¤‘: {stats.processing_requests}ê°œ")
                logger.info(f"   ì™„ë£Œ: {stats.completed_requests}ê°œ")
                logger.info(f"   ì‹¤íŒ¨: {stats.failed_requests}ê°œ")
                logger.info(f"   ì²˜ë¦¬ëŸ‰: {stats.current_throughput:.2f} ìš”ì²­/ë¶„")
                
            except Exception as e:
                logger.error(f"âŒ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _get_queue_position(self, request_id: str) -> Optional[int]:
        """íì—ì„œì˜ ìœ„ì¹˜ í™•ì¸"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í ë‚´ìš©ì„ í™•ì¸í•´ì•¼ í•˜ì§€ë§Œ,
        # PriorityQueueëŠ” ë‚´ìš©ì„ ì§ì ‘ í™•ì¸í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì¶”ì •
        return self.priority_queue.qsize()
    
    def _estimate_wait_time(self, queue_position: int) -> float:
        """ëŒ€ê¸° ì‹œê°„ ì¶”ì •"""
        if self.stats.current_throughput > 0:
            return queue_position / (self.stats.current_throughput / 60.0)  # ì´ˆ ë‹¨ìœ„
        return queue_position * 30  # ê¸°ë³¸ 30ì´ˆ/ìš”ì²­ìœ¼ë¡œ ì¶”ì •

# ì „ì—­ í ì¸ìŠ¤í„´ìŠ¤
stt_queue: Optional[IntelligentSTTQueue] = None

# Pydantic ëª¨ë¸
class TranscriptionRequest(BaseModel):
    audio_data: str  # base64 ì¸ì½”ë”©ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
    language: Optional[str] = "ko"
    audio_format: Optional[str] = "pcm_16khz"

class WordSegment(BaseModel):
    word: str
    start: float
    end: float
    confidence: Optional[float] = None

class SegmentInfo(BaseModel):
    id: int
    text: str
    start: float
    end: float
    confidence: Optional[float] = None
    words: Optional[List[WordSegment]] = None

class TranscriptionResponse(BaseModel):
    text: str
    language: str
    rtf: float
    processing_time: float
    audio_duration: float
    gpu_optimized: bool
    model_load_time: Optional[float] = None
    segments: Optional[List[SegmentInfo]] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    model_loaded: bool
    cudnn_enabled: bool
    gpu_name: Optional[str] = None
    gpu_info: Optional[Dict[str, Any]] = None
    optimization_status: Optional[Dict[str, Any]] = None

# íì‰ ì‹œìŠ¤í…œ ê´€ë ¨ Pydantic ëª¨ë¸
class QueuedTranscriptionRequest(BaseModel):
    audio_data: str
    language: Optional[str] = "ko"
    audio_format: Optional[str] = "pcm_16khz"
    client_id: Optional[str] = None
    priority: Optional[str] = "medium"  # high, medium, low
    timeout: Optional[int] = None

class QueuedTranscriptionResponse(BaseModel):
    request_id: str
    status: str
    message: str
    estimated_wait_time: Optional[float] = None
    queue_position: Optional[int] = None

class QueueStatusResponse(BaseModel):
    request_id: str
    status: str
    priority: Optional[str] = None
    wait_time: Optional[float] = None
    processing_time: Optional[float] = None
    total_time: Optional[float] = None
    queue_position: Optional[int] = None
    estimated_wait_time: Optional[float] = None
    estimated_remaining_time: Optional[float] = None
    completed_at: Optional[str] = None
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None

class QueueStatsResponse(BaseModel):
    total_requests: int
    queued_requests: int
    processing_requests: int
    completed_requests: int
    failed_requests: int
    timeout_requests: int
    average_wait_time: float
    average_processing_time: float
    current_throughput: float
    queue_capacity: int
    max_concurrent: int

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Large-v3 ê·¹í•œ ìµœì í™” STT API",
    description="Large-v3 ëª¨ë¸ ì „ìš© ê·¹í•œ GPU ìµœì í™” ìŒì„± ì¸ì‹ API ì„œë²„ - float16, TF32, 95% GPU ë©”ëª¨ë¦¬ í™œìš©",
    version="3.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
stt_service: Optional[FasterWhisperSTTService] = None
post_processing_corrector = None

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ STT ì„œë¹„ìŠ¤, í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œ ë° íì‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global stt_service, post_processing_corrector, stt_queue
    try:
        logger.info("ğŸš€ Large-v3 ì „ìš© ê·¹í•œ ìµœì í™” STT Server ì‹œì‘ ì¤‘...")
        logger.info(f"cuDNN í™œì„±í™” ìƒíƒœ: {torch.backends.cudnn.enabled}")
        logger.info(f"cuDNN ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ: {torch.backends.cudnn.benchmark}")
        logger.info(f"TF32 í™œì„±í™” ìƒíƒœ: {torch.backends.cuda.matmul.allow_tf32}")
        
        if torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            logger.info(f"ğŸ¯ GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"ğŸ¯ GPU ë©”ëª¨ë¦¬: {gpu_props.total_memory / 1024**3:.1f}GB")
            logger.info(f"ğŸ¯ CUDA ë²„ì „: {torch.version.cuda}")
            logger.info(f"ğŸ¯ PyTorch ë²„ì „: {torch.__version__}")
            logger.info(f"ğŸ¯ GPU ì•„í‚¤í…ì²˜: {gpu_props.major}.{gpu_props.minor}")
            logger.info(f"ğŸ¯ ë©€í‹°í”„ë¡œì„¸ì„œ ìˆ˜: {gpu_props.multi_processor_count}")
        
        # STT ì„œë¹„ìŠ¤ ìƒì„± ë° ì¦‰ì‹œ ì´ˆê¸°í™” (Large-v3 ì „ìš© ìµœì í™”)
        logger.info("ğŸ“¦ Large-v3 ëª¨ë¸ ë¡œë”© ì¤‘ (float16 ê·¹í•œ ìµœì í™”)...")
        stt_service = FasterWhisperSTTService(
            model_size="large-v3",
            device="cuda",
            compute_type="float16"
        )
        
        # ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ì²« ë²ˆì§¸ ìš”ì²­ ì§€ì—° ì œê±°
        start_time = time.time()
        await stt_service.initialize()
        load_time = time.time() - start_time
        
        logger.info(f"âœ… Large-v3 STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸ ë¡œë”© ì‹œê°„: {load_time:.2f}ì´ˆ")
        
        # ëª¨ë¸ ì›œì—… ìˆ˜í–‰ (ì²« ìš”ì²­ ì§€ì—° ìµœì†Œí™”)
        await warmup_large_model(stt_service)
        
        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
        gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
        gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3
        logger.info(f"ğŸ¯ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ - í• ë‹¹: {gpu_memory_allocated:.2f}GB, ì˜ˆì•½: {gpu_memory_reserved:.2f}GB")
        
        # í›„ì²˜ë¦¬ í‚¤ì›Œë“œ êµì • ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logger.info("ğŸ”§ í›„ì²˜ë¦¬ í‚¤ì›Œë“œ êµì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        try:
            post_processing_corrector = get_post_processing_corrector()
            logger.info("âœ… í›„ì²˜ë¦¬ í‚¤ì›Œë“œ êµì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info("   - ì •í™• ë§¤ì¹­ êµì • í™œì„±í™”")
            logger.info("   - í¼ì§€ ë§¤ì¹­ êµì • í™œì„±í™”")
            logger.info("   - í•œêµ­ì–´ íŠ¹í™” êµì • í™œì„±í™”")
        except Exception as e:
            logger.error(f"âŒ í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            post_processing_corrector = None
        
        # ğŸ”„ ì§€ëŠ¥í˜• íì‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logger.info("ğŸ”„ ì§€ëŠ¥í˜• STT íì‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        try:
            stt_queue = IntelligentSTTQueue(
                max_concurrent=8,  # ë™ì‹œ ì²˜ë¦¬ ìµœëŒ€ 8ê°œ (í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜)
                max_queue_size=50,  # ìµœëŒ€ í í¬ê¸° 50ê°œ
                default_timeout=60,  # ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ 60ì´ˆ
                priority_timeout=30  # ìš°ì„ ìˆœìœ„ ìš”ì²­ íƒ€ì„ì•„ì›ƒ 30ì´ˆ
            )
            
            await stt_queue.start()
            
            # ë°±ê·¸ë¼ìš´ë“œ ìš”ì²­ ì²˜ë¦¬ ì‹œì‘
            asyncio.create_task(stt_queue.process_requests(stt_service))
            
            logger.info("âœ… ì§€ëŠ¥í˜• íì‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"   ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬: {stt_queue.max_concurrent}ê°œ")
            logger.info(f"   ìµœëŒ€ í í¬ê¸°: {stt_queue.max_queue_size}ê°œ")
            
        except Exception as e:
            logger.error(f"âŒ íì‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            stt_queue = None
        
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ íì‰ ì‹œìŠ¤í…œ ì •ë¦¬"""
    global stt_queue
    try:
        if stt_queue:
            logger.info("ğŸ›‘ íì‰ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
            await stt_queue.stop()
            logger.info("âœ… íì‰ ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ íì‰ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - Large-v3 ì „ìš© ê·¹í•œ ìµœì í™” ì„œë²„"""
    gpu_features = {}
    if torch.cuda.is_available():
        gpu_props = torch.cuda.get_device_properties(0)
        gpu_features = {
            "gpu_name": torch.cuda.get_device_name(0),
            "total_memory_gb": f"{gpu_props.total_memory / 1024**3:.1f}",
            "architecture": f"{gpu_props.major}.{gpu_props.minor}"
        }
    
    return {
        "message": "Large-v3 ì „ìš© ê·¹í•œ ìµœì í™” STT API Server", 
        "model": "large-v3",
        "optimization": "extreme",
        "status": "running",
        "features": {
            "model_type": "large-v3",
            "compute_type": "float16", 
            "memory_fraction": 0.95,
            "cudnn_enabled": torch.backends.cudnn.enabled,
            "cudnn_benchmark": torch.backends.cudnn.benchmark,
            "tf32_enabled": torch.backends.cuda.matmul.allow_tf32,
            "gpu_available": torch.cuda.is_available(),
            **gpu_features
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ (Large-v3 ìµœì í™” ì •ë³´ í¬í•¨)"""
    model_loaded = False
    gpu_info = {}
    optimization_status = {}
    
    if stt_service is not None:
        model_loaded = hasattr(stt_service, 'model') and stt_service.model is not None
    
    if torch.cuda.is_available():
        gpu_props = torch.cuda.get_device_properties(0)
        gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
        gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3
        
        gpu_info = {
            "device_name": torch.cuda.get_device_name(0),
            "total_memory_gb": f"{gpu_props.total_memory / 1024**3:.1f}",
            "allocated_memory_gb": f"{gpu_memory_allocated:.2f}",
            "reserved_memory_gb": f"{gpu_memory_reserved:.2f}",
            "memory_utilization": f"{gpu_memory_allocated / (gpu_props.total_memory / 1024**3) * 100:.1f}%",
            "architecture": f"{gpu_props.major}.{gpu_props.minor}",
            "multiprocessor_count": gpu_props.multi_processor_count
        }
        
        optimization_status = {
            "model_type": "large-v3",
            "compute_type": "float16",
            "memory_fraction": "0.95",
            "cudnn_benchmark": torch.backends.cudnn.benchmark,
            "tf32_enabled": torch.backends.cuda.matmul.allow_tf32,
            "flash_attention": "enabled"
        }
        
    return HealthResponse(
        status="healthy" if model_loaded else "loading",
        gpu_available=torch.cuda.is_available(),
        model_loaded=model_loaded,
        cudnn_enabled=torch.backends.cudnn.enabled,
        gpu_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
        gpu_info=gpu_info,
        optimization_status=optimization_status
    )

@app.post("/infer/utterance", response_model=TranscriptionResponse) 
async def transcribe_utterance(request: TranscriptionRequest):
    """
    ì‹ ë¢°ë„ ì •ë³´ í¬í•¨í•œ ìƒì„¸ ì „ì‚¬ ì—”ë“œí¬ì¸íŠ¸
    
    Large-v3 ì „ìš© ê·¹í•œ ìµœì í™”ì™€ í•¨ê»˜ ì‹ ë¢°ë„ ì ìˆ˜, ì„¸ê·¸ë¨¼íŠ¸, ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    ìë™ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…ì´ ì ìš©ë©ë‹ˆë‹¤.
    """
    if stt_service is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        request_id = str(uuid.uuid4())
        
        logger.info(f"ğŸ¯ Large-v3 ì‹ ë¢°ë„ ì „ì‚¬ ìš”ì²­ {request_id} ì‹œì‘ (ì–¸ì–´: {request.language})")
        
        # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì²´í¬ ë° ì ìš©
        boosting_applied = False
        boosted_text = None
        keyword_stats = {}
        
        try:
            # ì „ì²´ í‚¤ì›Œë“œ í†µê³„ í™•ì¸
            logger.info("ğŸ” í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ìƒíƒœ í™•ì¸ ì¤‘...")
            global_stats = await keyword_service.get_global_statistics()
            total_keywords = global_stats.get('total_keywords', 0)
            active_keywords = global_stats.get('active_keywords', 0)
            
            logger.info(f"ğŸ”‘ í‚¤ì›Œë“œ ìƒíƒœ: ì´ {total_keywords}ê°œ, í™œì„± {active_keywords}ê°œ")
            
            if active_keywords > 0:
                logger.info("ğŸš€ í™œì„± í‚¤ì›Œë“œ ë°œê²¬! í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ì‹œë„...")
                
                # ëª¨ë“  í™œì„± í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
                all_keywords = await keyword_service.get_all_active_keywords()
                if all_keywords:
                    keywords_list = list(all_keywords.keys())
                    logger.info(f"ğŸ¯ ë°œê²¬ëœ í‚¤ì›Œë“œ: {keywords_list}")
                    
                    # ì„ì‹œ call_id ìƒì„±
                    temp_call_id = f"auto_boost_{request_id}"
                    
                    # í‚¤ì›Œë“œë¥¼ ì„ì‹œë¡œ ë“±ë¡ (ê¸°ì¡´ í‚¤ì›Œë“œê°€ ìë™ìœ¼ë¡œ ì ìš©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
                    try:
                        from src.api.keyword_models import KeywordEntry, KeywordRegistrationRequest
                        
                        temp_keywords = []
                        for keyword, details in all_keywords.items():
                            temp_keywords.append(KeywordEntry(
                                keyword=keyword,
                                boost_factor=details.get('boost_factor', 2.0),
                                category=details.get('category', 'custom'),
                                confidence_threshold=details.get('confidence_threshold', 0.3),
                                aliases=details.get('aliases', []),
                                enabled=True
                            ))
                        
                        temp_request = KeywordRegistrationRequest(
                            call_id=temp_call_id,
                            keywords=temp_keywords,
                            global_boost_factor=2.0,
                            replace_existing=True
                        )
                        
                        # ì„ì‹œ í‚¤ì›Œë“œ ë“±ë¡
                        await keyword_service.register_keywords(temp_request)
                        logger.info(f"âœ… ì„ì‹œ í‚¤ì›Œë“œ ë“±ë¡ ì™„ë£Œ: {temp_call_id}")
                        
                        # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ì‹œë„ (initial_prompt ì‚¬ìš©)
                        logger.info("ğŸ™ï¸ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ìƒí™©ì—ì„œ ì „ì‚¬ ì‹¤í–‰...")
                        
                        # í‚¤ì›Œë“œë¥¼ initial_promptë¡œ ë³€í™˜
                        keywords_prompt = ", ".join(keywords_list)
                        initial_prompt = f"ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤: {keywords_prompt}"
                        logger.info(f"ğŸ¯ Initial prompt: '{initial_prompt}'")
                        
                        # Base64 ë””ì½”ë”©í•˜ì—¬ NumPy ë°°ì—´ë¡œ ë³€í™˜
                        try:
                            audio_bytes = base64.b64decode(request.audio_data)
                            audio_array = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
                            
                            # FasterWhisper ëª¨ë¸ì— ì§ì ‘ ì ‘ê·¼í•˜ì—¬ initial_prompt ì‚¬ìš©
                            segments, info = stt_service.model.transcribe(
                                audio_array,
                                beam_size=5,
                                best_of=5,
                                temperature=0.0,
                                vad_filter=False,
                                language=request.language,
                                word_timestamps=True,
                                initial_prompt=initial_prompt  # í‚¤ì›Œë“œ íŒíŠ¸ ì œê³µ
                            )
                            
                            segments_list = list(segments)
                            if segments_list:
                                boosted_text = " ".join([segment.text.strip() for segment in segments_list if segment.text.strip()])
                                
                                # STTResult í˜•íƒœë¡œ ë³€í™˜
                                class MockResult:
                                    def __init__(self, text, language, segments, audio_duration):
                                        self.text = text
                                        self.language = language
                                        self.segments = segments
                                        self.audio_duration = len(audio_array) / 16000.0
                                        self.rtf = 0.05  # ì„ì‹œê°’
                                
                                result = MockResult(boosted_text, request.language, segments_list, len(audio_array) / 16000.0)
                                logger.info(f"âœ… í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ê²°ê³¼: '{boosted_text}'")
                            
                        except Exception as e:
                            logger.error(f"âŒ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            # fallback to normal transcription
                            result = await stt_service.transcribe_audio(
                                audio_data=request.audio_data,
                                audio_format=request.audio_format,
                                language=request.language
                            )
                            boosting_applied = False
                        
                        if result and result.text:
                            boosted_text = result.text
                            boosting_applied = True
                            keyword_stats = {
                                'registered_keywords': len(all_keywords),
                                'keyword_list': keywords_list,
                                'boosting_applied': True
                            }
                            logger.info(f"âœ… í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ì„±ê³µ: '{boosted_text}'")
                            
                            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                            matches = []
                            for keyword in keywords_list:
                                if keyword in boosted_text:
                                    matches.append(keyword)
                            
                            if matches:
                                keyword_stats['detected_keywords'] = matches
                                logger.info(f"ğŸ¯ ê°ì§€ëœ í‚¤ì›Œë“œ: {matches}")
                            
                        # ì„ì‹œ í‚¤ì›Œë“œ ì •ë¦¬
                        await keyword_service.delete_keywords(temp_call_id)
                        logger.info(f"ğŸ§¹ ì„ì‹œ í‚¤ì›Œë“œ ì •ë¦¬ ì™„ë£Œ: {temp_call_id}")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì „ì‚¬ ì‹¤íŒ¨: {e}")
                        # ì„ì‹œ í‚¤ì›Œë“œ ì •ë¦¬ ì‹œë„
                        try:
                            await keyword_service.delete_keywords(temp_call_id)
                        except:
                            pass
                else:
                    logger.info("â„¹ï¸ í™œì„± í‚¤ì›Œë“œê°€ ì‹¤ì œë¡œëŠ” ë¹„ì–´ìˆìŒ")
            else:
                logger.info("â„¹ï¸ í™œì„± í‚¤ì›Œë“œ ì—†ìŒ, ì¼ë°˜ ì „ì‚¬ ì§„í–‰")
                
        except Exception as e:
            logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… í™•ì¸ ì‹¤íŒ¨: {e}")
            boosting_applied = False
        
        # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…ì´ ì„±ê³µí•˜ì§€ ì•Šì€ ê²½ìš° ì¼ë°˜ ì „ì‚¬ ìˆ˜í–‰
        if not boosting_applied:
            logger.info("ğŸ™ï¸ ì¼ë°˜ FasterWhisper ì „ì‚¬ ì‹¤í–‰")
            # ì˜¤ë””ì˜¤ ì „ì‚¬ ì‹¤í–‰ (Large-v3 ìµœì í™” íŒŒë¼ë¯¸í„°)
            result = await stt_service.transcribe_audio(
                audio_data=request.audio_data,
                audio_format=request.audio_format,
                language=request.language
            )
        else:
            # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…ëœ ê²°ê³¼ ì‚¬ìš©
            pass
        
        processing_time = time.time() - start_time
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ ê²°ì •
        final_text = boosted_text if boosting_applied else result.text
        
        # ê¸°ë³¸ ì‘ë‹µ ìƒì„± (ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ í¬í•¨)
        response = TranscriptionResponse(
            text=final_text,
            language=result.language,
            rtf=result.rtf,
            processing_time=processing_time,
            audio_duration=result.audio_duration,
            gpu_optimized=True,
            model_load_time=None
        )
        
        # ì„¸ê·¸ë¨¼íŠ¸ì™€ ì‹ ë¢°ë„ ì •ë³´ ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
        segments_list = []
        if hasattr(result, 'segments') and result.segments:
            for i, segment in enumerate(result.segments):
                segment_confidence = None
                words_list = []
                
                # ì‹ ë¢°ë„ ê³„ì‚° (avg_logprobì´ ìˆëŠ” ê²½ìš°)
                if hasattr(segment, 'avg_logprob') or 'avg_logprob' in segment:
                    avg_logprob = getattr(segment, 'avg_logprob', segment.get('avg_logprob', -1.0))
                    segment_confidence = logprob_to_confidence(avg_logprob)
                
                # ë‹¨ì–´ ë ˆë²¨ ì •ë³´ (ìˆëŠ” ê²½ìš°)
                if hasattr(segment, 'words') or 'words' in segment:
                    segment_words = getattr(segment, 'words', segment.get('words', []))
                    if segment_words:
                        for word in segment_words:
                            word_confidence = None
                            if hasattr(word, 'probability') or 'probability' in word:
                                prob = getattr(word, 'probability', word.get('probability'))
                                if prob is not None:
                                    word_confidence = normalize_word_probability(prob)
                            
                            words_list.append(WordSegment(
                                word=getattr(word, 'word', word.get('word', '')),
                                start=getattr(word, 'start', word.get('start', 0.0)),
                                end=getattr(word, 'end', word.get('end', 0.0)),
                                confidence=word_confidence
                            ))
                
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ì¶”ê°€
                segment_info = SegmentInfo(
                    id=i,
                    text=getattr(segment, 'text', segment.get('text', '')).strip(),
                    start=getattr(segment, 'start', segment.get('start', 0.0)),
                    end=getattr(segment, 'end', segment.get('end', result.audio_duration)),
                    confidence=segment_confidence,
                    words=words_list if words_list else None
                )
                
                segments_list.append(segment_info)
        
        response.segments = segments_list if segments_list else None
        
        # ë¡œê·¸ ì¶œë ¥
        boost_status = "í‚¤ì›Œë“œë¶€ìŠ¤íŒ…ì ìš©" if boosting_applied else "ì¼ë°˜ì „ì‚¬"
        logger.info(f"âœ… {boost_status} ì™„ë£Œ {request_id}: RTF={result.rtf:.3f}x, "
                   f"ì²˜ë¦¬ì‹œê°„={processing_time:.3f}ì´ˆ, í…ìŠ¤íŠ¸='{final_text}'")
        
        if boosting_applied and keyword_stats:
            logger.info(f"ğŸ¯ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… í†µê³„: {keyword_stats}")
        
        return response
        
    except Exception as e:
        logger.error(f"âŒ ì‹ ë¢°ë„ ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì „ì‚¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(request: TranscriptionRequest):
    """ì˜¤ë””ì˜¤ ì „ì‚¬ ì—”ë“œí¬ì¸íŠ¸ (JSON)"""
    if stt_service is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        
        # ì˜¤ë””ì˜¤ ì „ì‚¬ ì‹¤í–‰
        result = await stt_service.transcribe_audio(
            audio_data=request.audio_data,
            audio_format=request.audio_format,
            language=request.language
        )
        
        processing_time = time.time() - start_time
        
        return TranscriptionResponse(
            text=result.text,
            language=result.language,
            rtf=result.rtf,
            processing_time=processing_time,
            audio_duration=result.audio_duration,
            gpu_optimized=torch.cuda.is_available() and torch.backends.cudnn.enabled
        )
        
    except Exception as e:
        logger.error(f"ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì „ì‚¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/transcribe/file", response_model=TranscriptionResponse)
async def transcribe_file(
    audio: UploadFile = File(...),
    language: str = Form("ko"),
    vad_filter: bool = Form(False)
):
    """ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬ ì—”ë“œí¬ì¸íŠ¸"""
    if stt_service is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        
        # íŒŒì¼ ì½ê¸°
        audio_bytes = await audio.read()
        
        # ì „ì‚¬ ì‹¤í–‰
        result = await stt_service.transcribe_file_bytes(
            audio_bytes=audio_bytes,
            language=language,
            vad_filter=vad_filter
        )
        
        processing_time = time.time() - start_time
        
        return TranscriptionResponse(
            text=result.text,
            language=result.language,
            rtf=result.rtf,
            processing_time=processing_time,
            audio_duration=result.audio_duration,
            gpu_optimized=torch.cuda.is_available() and torch.backends.cudnn.enabled
        )
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì „ì‚¬ ì‹¤íŒ¨: {str(e)}")

# ============================================================================
# íì‰ ì‹œìŠ¤í…œ API ì—”ë“œí¬ì¸íŠ¸
# ============================================================================

@app.post("/queue/transcribe", response_model=QueuedTranscriptionResponse)
async def queue_transcribe_audio(request: QueuedTranscriptionRequest):
    """ìŒì„± ì¸ì‹ ìš”ì²­ì„ íì— ì œì¶œ"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    try:
        # ìš°ì„ ìˆœìœ„ ë³€í™˜
        priority_map = {
            "high": RequestPriority.HIGH,
            "medium": RequestPriority.MEDIUM,
            "low": RequestPriority.LOW
        }
        priority = priority_map.get(request.priority.lower(), RequestPriority.MEDIUM)
        
        # íì— ìš”ì²­ ì œì¶œ
        request_id = await stt_queue.submit_request(
            audio_data=request.audio_data,
            language=request.language,
            audio_format=request.audio_format,
            client_id=request.client_id,
            priority=priority,
            timeout=request.timeout
        )
        
        # í í†µê³„ ê°€ì ¸ì˜¤ê¸°
        stats = await stt_queue.get_queue_stats()
        estimated_wait_time = (stats.queued_requests / max(1, stats.current_throughput)) * 60 if stats.current_throughput > 0 else stats.queued_requests * 30
        
        return QueuedTranscriptionResponse(
            request_id=request_id,
            status="queued",
            message="Request successfully queued for processing",
            estimated_wait_time=estimated_wait_time,
            queue_position=stats.queued_requests
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ í ìš”ì²­ ì œì¶œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to queue request: {str(e)}")

@app.get("/queue/result/{request_id}")
async def get_queue_result(request_id: str):
    """í ì²˜ë¦¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ëŒ€ê¸°/ì™„ë£Œê¹Œì§€)"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    try:
        result = await stt_queue.get_request_result(request_id)
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ í ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get result: {str(e)}")

@app.get("/queue/status/{request_id}", response_model=QueueStatusResponse)
async def get_queue_status(request_id: str):
    """ìš”ì²­ ìƒíƒœ ì¡°íšŒ"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    try:
        status_info = await stt_queue.get_request_status(request_id)
        return QueueStatusResponse(**status_info)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ í ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")

@app.delete("/queue/cancel/{request_id}")
async def cancel_queue_request(request_id: str):
    """í ìš”ì²­ ì·¨ì†Œ"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    try:
        cancelled = await stt_queue.cancel_request(request_id)
        
        if cancelled:
            return {"message": f"Request {request_id} cancelled successfully", "cancelled": True}
        else:
            return {"message": f"Request {request_id} could not be cancelled (may be processing or completed)", "cancelled": False}
        
    except Exception as e:
        logger.error(f"âŒ í ìš”ì²­ ì·¨ì†Œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to cancel request: {str(e)}")

@app.get("/queue/stats", response_model=QueueStatsResponse)
async def get_queue_stats():
    """í í†µê³„ ì¡°íšŒ"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    try:
        stats = await stt_queue.get_queue_stats()
        
        return QueueStatsResponse(
            total_requests=stats.total_requests,
            queued_requests=stats.queued_requests,
            processing_requests=stats.processing_requests,
            completed_requests=stats.completed_requests,
            failed_requests=stats.failed_requests,
            timeout_requests=stats.timeout_requests,
            average_wait_time=stats.average_wait_time,
            average_processing_time=stats.average_processing_time,
            current_throughput=stats.current_throughput,
            queue_capacity=stt_queue.max_queue_size,
            max_concurrent=stt_queue.max_concurrent
        )
        
    except Exception as e:
        logger.error(f"âŒ í í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get queue stats: {str(e)}")

@app.post("/queue/priority/transcribe", response_model=QueuedTranscriptionResponse)
async def priority_transcribe_audio(request: QueuedTranscriptionRequest):
    """ìš°ì„ ìˆœìœ„ ìŒì„± ì¸ì‹ ìš”ì²­ (ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ íì— ì œì¶œ)"""
    if stt_queue is None:
        raise HTTPException(status_code=503, detail="Queue system not initialized")
    
    # ìš”ì²­ì„ ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ ì„¤ì •
    request.priority = "high"
    
    return await queue_transcribe_audio(request)


# í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/keywords/register")
async def register_keywords(request: KeywordRegistrationRequest):
    """í‚¤ì›Œë“œ ë“±ë¡ API (í›„ì²˜ë¦¬ êµì •ìš©)"""
    if post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        success = await post_processing_corrector.register_keywords(
            call_id=request.call_id,
            keywords=request.keywords
        )
        
        if not success:
            raise HTTPException(status_code=500, detail="í‚¤ì›Œë“œ ë“±ë¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        return {
            "message": "í‚¤ì›Œë“œê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤",
            "call_id": request.call_id,
            "keyword_count": len(request.keywords),
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ë“±ë¡ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ë“±ë¡ ì‹¤íŒ¨: {str(e)}")


@app.post("/keywords/correct", response_model=KeywordCorrectionResponse)
async def correct_keywords(request: KeywordCorrectionRequest):
    """í‚¤ì›Œë“œ êµì • API"""
    if post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        result = await post_processing_corrector.apply_correction(
            call_id=request.call_id,
            text=request.text,
            enable_fuzzy_matching=request.enable_fuzzy_matching,
            min_similarity=request.min_similarity
        )
        
        return KeywordCorrectionResponse(
            original_text=result.original_text,
            corrected_text=result.corrected_text,
            corrections=result.corrections,
            keywords_detected=result.keywords_detected,
            confidence_score=result.confidence_score,
            processing_time=result.processing_time
        )
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ êµì • ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ êµì • ì‹¤íŒ¨: {str(e)}")


@app.get("/keywords/{call_id}")
async def get_keywords(call_id: str):
    """í‚¤ì›Œë“œ ì¡°íšŒ API"""
    if post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        keywords = await post_processing_corrector.get_keywords(call_id)
        if keywords is None:
            raise HTTPException(status_code=404, detail=f"í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {call_id}")
        
        # KeywordEntry ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        keywords_dict = {}
        for keyword, entry in keywords.items():
            keywords_dict[keyword] = {
                "keyword": entry.keyword,
                "aliases": entry.aliases,
                "confidence_threshold": entry.confidence_threshold,
                "category": entry.category,
                "enabled": entry.enabled,
                "created_at": entry.created_at.isoformat()
            }
        
        return {
            "call_id": call_id,
            "keywords": keywords_dict,
            "keyword_count": len(keywords_dict)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.delete("/keywords/{call_id}")
async def delete_keywords(call_id: str):
    """í‚¤ì›Œë“œ ì‚­ì œ API"""
    if post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        success = await post_processing_corrector.delete_keywords(call_id)
        if not success:
            raise HTTPException(status_code=404, detail=f"í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {call_id}")
        
        return {"message": f"í‚¤ì›Œë“œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {call_id}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.get("/keywords/stats", response_model=KeywordStatsResponse)
async def get_keyword_statistics():
    """í‚¤ì›Œë“œ êµì • í†µê³„ API"""
    if post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="í›„ì²˜ë¦¬ êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        stats = post_processing_corrector.get_stats()
        
        return KeywordStatsResponse(
            total_keywords=len(post_processing_corrector.keyword_cache),
            total_corrections=stats.get('total_corrections', 0),
            successful_corrections=stats.get('successful_corrections', 0),
            success_rate=stats.get('success_rate', 0.0),
            avg_processing_time=stats.get('avg_processing_time', 0.0),
            categories={}  # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ëŠ” í•„ìš”ì‹œ ì¶”ê°€
        )
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.post("/transcribe/with-correction/{call_id}", response_model=TranscriptionWithCorrection)
async def transcribe_with_keyword_correction(
    call_id: str,
    audio: UploadFile = File(...),
    language: str = Form("ko"),
    vad_filter: bool = Form(False),
    enable_fuzzy_matching: bool = Form(True),
    min_similarity: float = Form(0.8)
):
    """í‚¤ì›Œë“œ êµì •ì´ ì ìš©ëœ íŒŒì¼ ì „ì‚¬ API"""
    if stt_service is None or post_processing_corrector is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ ë˜ëŠ” êµì • ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        
        # íŒŒì¼ ì½ê¸°
        audio_bytes = await audio.read()
        
        # ê¸°ë³¸ ì „ì‚¬ ì‹¤í–‰
        stt_result = await stt_service.transcribe_file_bytes(
            audio_bytes=audio_bytes,
            language=language,
            vad_filter=vad_filter
        )
        
        # í‚¤ì›Œë“œ êµì • ì ìš©
        correction_result = await post_processing_corrector.apply_correction(
            call_id=call_id,
            text=stt_result.text,
            enable_fuzzy_matching=enable_fuzzy_matching,
            min_similarity=min_similarity
        )
        
        processing_time = time.time() - start_time
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë³€í™˜
        segments = []
        for i, segment in enumerate(stt_result.segments):
            segment_dict = segment if isinstance(segment, dict) else asdict(segment)
            segments.append(SegmentInfo(
                id=segment_dict.get('id', i),
                text=segment_dict.get('text', ''),
                start=segment_dict.get('start', 0.0),
                end=segment_dict.get('end', 0.0),
                confidence=segment_dict.get('confidence')
            ))
        
        return TranscriptionWithCorrection(
            text=stt_result.text,
            corrected_text=correction_result.corrected_text,
            language=stt_result.language,
            segments=segments,
            keyword_correction=KeywordCorrectionResponse(
                original_text=correction_result.original_text,
                corrected_text=correction_result.corrected_text,
                corrections=correction_result.corrections,
                keywords_detected=correction_result.keywords_detected,
                confidence_score=correction_result.confidence_score,
                processing_time=correction_result.processing_time
            ),
            metrics=ProcessingMetrics(
                total_duration=processing_time,
                audio_duration=stt_result.audio_duration,
                rtf=stt_result.rtf,
                inference_time=stt_result.processing_time
            )
        )
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ êµì • ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ êµì • ì „ì‚¬ ì‹¤íŒ¨: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    import argparse
    
    parser = argparse.ArgumentParser(description="GPU Optimized STT Server")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind to")
    parser.add_argument("--workers", type=int, default=1, help="Number of workers")
    
    args = parser.parse_args()
    
    uvicorn.run(app, host=args.host, port=args.port, workers=args.workers)



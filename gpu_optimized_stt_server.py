#!/usr/bin/env python3
"""
GPU Optimized STT Server
cuDNNì„ ì™„ì „íˆ í™œì„±í™”í•˜ê³  RTX 4090 ìµœì í™”ë¥¼ ì ìš©í•œ STT API ì„œë²„
"""

import sys
import os
sys.path.append('/home/jonghooy/haiv_stt_decoder_v2')

import asyncio
import time
import torch
import torchaudio
import numpy as np
import logging
import base64
import uvicorn
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional

# GPU ìµœì í™” ì„¤ì • - cuDNN ì™„ì „ í™œì„±í™”
torch.backends.cudnn.enabled = True
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

from src.api.stt_service import FasterWhisperSTTService

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Pydantic ëª¨ë¸
class TranscriptionRequest(BaseModel):
    audio_data: str  # base64 ì¸ì½”ë”©ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
    language: Optional[str] = "ko"
    audio_format: Optional[str] = "pcm_16khz"

class TranscriptionResponse(BaseModel):
    text: str
    language: str
    rtf: float
    processing_time: float
    audio_duration: float
    gpu_optimized: bool
    model_load_time: Optional[float] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    model_loaded: bool
    cudnn_enabled: bool
    gpu_name: Optional[str] = None

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="GPU Optimized STT API",
    description="cuDNN í™œì„±í™”ëœ ê³ ì„±ëŠ¥ ìŒì„± ì¸ì‹ API ì„œë²„",
    version="2.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ STT ì„œë¹„ìŠ¤
stt_service: Optional[FasterWhisperSTTService] = None

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
    global stt_service
    try:
        logger.info("ğŸš€ GPU Optimized STT Server ì‹œì‘ ì¤‘...")
        logger.info(f"cuDNN í™œì„±í™” ìƒíƒœ: {torch.backends.cudnn.enabled}")
        logger.info(f"cuDNN ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ: {torch.backends.cudnn.benchmark}")
        
        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"CUDA ë²„ì „: {torch.version.cuda}")
            logger.info(f"PyTorch ë²„ì „: {torch.__version__}")
        
        # STT ì„œë¹„ìŠ¤ ìƒì„± ë° ì¦‰ì‹œ ì´ˆê¸°í™”
        stt_service = FasterWhisperSTTService()
        logger.info("ğŸ“¦ STT ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ì²« ë²ˆì§¸ ìš”ì²­ ì§€ì—° ì œê±°
        start_time = time.time()
        await stt_service.initialize()
        load_time = time.time() - start_time
        
        logger.info(f"âœ… STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸ ë¡œë”© ì‹œê°„: {load_time:.2f}ì´ˆ")
        logger.info(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "GPU Optimized STT API Server", 
        "status": "running",
        "features": {
            "cudnn_enabled": torch.backends.cudnn.enabled,
            "gpu_available": torch.cuda.is_available(),
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    model_loaded = False
    if stt_service is not None:
        model_loaded = hasattr(stt_service, 'model') and stt_service.model is not None
        
    return HealthResponse(
        status="healthy",
        gpu_available=torch.cuda.is_available(),
        model_loaded=model_loaded,
        cudnn_enabled=torch.backends.cudnn.enabled,
        gpu_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
    )

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(request: TranscriptionRequest):
    """ì˜¤ë””ì˜¤ ì „ì‚¬ ì—”ë“œí¬ì¸íŠ¸ (JSON)"""
    if stt_service is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        
        # ì˜¤ë””ì˜¤ ì „ì‚¬ ì‹¤í–‰
        result = await stt_service.transcribe_audio(
            audio_data=request.audio_data,
            audio_format=request.audio_format,
            language=request.language
        )
        
        processing_time = time.time() - start_time
        
        return TranscriptionResponse(
            text=result.text,
            language=result.language,
            rtf=result.rtf,
            processing_time=processing_time,
            audio_duration=result.audio_duration,
            gpu_optimized=torch.cuda.is_available() and torch.backends.cudnn.enabled
        )
        
    except Exception as e:
        logger.error(f"ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì „ì‚¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/transcribe/file", response_model=TranscriptionResponse)
async def transcribe_file(
    audio: UploadFile = File(...),
    language: str = Form("ko"),
    vad_filter: bool = Form(False)
):
    """ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬ ì—”ë“œí¬ì¸íŠ¸"""
    if stt_service is None:
        raise HTTPException(status_code=500, detail="STT ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        start_time = time.time()
        
        # íŒŒì¼ ì½ê¸°
        audio_bytes = await audio.read()
        
        # ì „ì‚¬ ì‹¤í–‰
        result = await stt_service.transcribe_file_bytes(
            audio_bytes=audio_bytes,
            language=language,
            vad_filter=vad_filter
        )
        
        processing_time = time.time() - start_time
        
        return TranscriptionResponse(
            text=result.text,
            language=result.language,
            rtf=result.rtf,
            processing_time=processing_time,
            audio_duration=result.audio_duration,
            gpu_optimized=torch.cuda.is_available() and torch.backends.cudnn.enabled
        )
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì „ì‚¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì „ì‚¬ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "gpu_optimized_stt_server:app",
        host="0.0.0.0",
        port=8001,  # ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
        log_level="info",
        reload=False
    ) 